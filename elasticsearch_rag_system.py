import os
import json
from typing import List, Dict, Any, Optional
import streamlit as st
from datetime import datetime

# LlamaIndex æ ¸å¿ƒ
from llama_index.core import VectorStoreIndex, Document, Settings
from llama_index.core.storage.storage_context import StorageContext
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.postprocessor import SimilarityPostprocessor

# Elasticsearch integration
try:
    from elasticsearch import Elasticsearch
    from llama_index.vector_stores.elasticsearch import ElasticsearchStore
    ELASTICSEARCH_AVAILABLE = True
except ImportError:
    ELASTICSEARCH_AVAILABLE = False
    st.warning("âš ï¸ Elasticsearch dependencies not installed. Install with: pip install elasticsearch llama-index-vector-stores-elasticsearch")

# ç¹¼æ‰¿å¢å¼·ç‰ˆç³»çµ±
from enhanced_rag_system import EnhancedRAGSystem
from config import (
    GROQ_API_KEY, EMBEDDING_MODEL, LLM_MODEL, 
    ELASTICSEARCH_HOST, ELASTICSEARCH_PORT, ELASTICSEARCH_SCHEME,
    ELASTICSEARCH_INDEX_NAME, ELASTICSEARCH_USERNAME, ELASTICSEARCH_PASSWORD,
    ELASTICSEARCH_TIMEOUT, ELASTICSEARCH_MAX_RETRIES, ELASTICSEARCH_VERIFY_CERTS,
    ELASTICSEARCH_SHARDS, ELASTICSEARCH_REPLICAS, ELASTICSEARCH_VECTOR_DIMENSION,
    ELASTICSEARCH_SIMILARITY
)

class ElasticsearchRAGSystem(EnhancedRAGSystem):
    """Elasticsearch RAG ç³»çµ± - é«˜æ•ˆèƒ½ã€å¯æ“´å±•çš„å‘é‡æª¢ç´¢"""
    
    def __init__(self, elasticsearch_config: Optional[Dict] = None):
        """åˆå§‹åŒ– Elasticsearch RAG ç³»çµ±"""
        super().__init__(use_elasticsearch=False, use_chroma=False)  # ES RAGç³»çµ±è‡ªå·±ç®¡ç†Elasticsearch
        
        self.elasticsearch_config = elasticsearch_config or self._get_default_config()
        self.elasticsearch_client = None
        self.elasticsearch_store = None
        self.index_name = self.elasticsearch_config.get('index_name', 'rag_documents')
        
        # è¨˜æ†¶é«”ä½¿ç”¨ç›£æ§
        self.memory_stats = {
            'documents_processed': 0,
            'vectors_stored': 0,
            'peak_memory_mb': 0
        }
        
        # æ¨¡å‹å¯¦ä¾‹
        self.embedding_model = None
        self.llm_model = None
    
    def _ensure_models_initialized(self):
        """ç¢ºä¿æ¨¡å‹å·²åˆå§‹åŒ–ä¸¦å­˜å„²ç‚ºå¯¦ä¾‹å±¬æ€§"""
        if not self.models_initialized or self.embedding_model is None:
            super()._ensure_models_initialized()
            
            # å¾ Settings ç²å–æ¨¡å‹ä¸¦å­˜å„²ç‚ºå¯¦ä¾‹å±¬æ€§
            from llama_index.core import Settings
            self.embedding_model = Settings.embed_model
            self.llm_model = Settings.llm
            
            st.info("âœ… Elasticsearch RAG ç³»çµ±æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def _get_default_config(self) -> Dict:
        """ç²å–é è¨­ Elasticsearch é…ç½®"""
        return {
            'host': ELASTICSEARCH_HOST or 'localhost',
            'port': ELASTICSEARCH_PORT or 9200,
            'scheme': ELASTICSEARCH_SCHEME or 'http',
            'username': ELASTICSEARCH_USERNAME,
            'password': ELASTICSEARCH_PASSWORD,
            'timeout': ELASTICSEARCH_TIMEOUT or 30,
            'max_retries': ELASTICSEARCH_MAX_RETRIES or 3,
            'verify_certs': ELASTICSEARCH_VERIFY_CERTS,
            'index_name': ELASTICSEARCH_INDEX_NAME or 'rag_intelligent_assistant',
            'shards': ELASTICSEARCH_SHARDS or 1,
            'replicas': ELASTICSEARCH_REPLICAS or 0,
            'dimension': ELASTICSEARCH_VECTOR_DIMENSION or 1024,
            'similarity': ELASTICSEARCH_SIMILARITY or 'cosine',
            'text_field': 'content',
            'vector_field': 'embedding',
            'metadata_fields': ['source', 'page', 'chunk_id', 'timestamp', 'file_type', 'file_size']
        }
    
    def _setup_elasticsearch_client(self) -> bool:
        """è¨­ç½® Elasticsearch å®¢æˆ¶ç«¯"""
        if not ELASTICSEARCH_AVAILABLE:
            st.error("âŒ Elasticsearch ä¾è³´æœªå®‰è£")
            return False
        
        try:
            config = self.elasticsearch_config
            
            # é€£æ¥é…ç½®
            es_config = {
                'hosts': [f"{config['scheme']}://{config['host']}:{config['port']}"],
                'timeout': config['timeout'],
                'max_retries': config['max_retries'],
                'retry_on_timeout': True,
                'verify_certs': config['verify_certs']
            }
            
            # èªè­‰é…ç½®
            if config.get('username') and config.get('password'):
                es_config['basic_auth'] = (config['username'], config['password'])
            
            self.elasticsearch_client = Elasticsearch(**es_config)
            
            # æ¸¬è©¦é€£æ¥
            if self.elasticsearch_client.ping():
                st.success(f"âœ… æˆåŠŸé€£æ¥åˆ° Elasticsearch: {config['host']}:{config['port']}")
                return True
            else:
                st.error(f"âŒ ç„¡æ³•é€£æ¥åˆ° Elasticsearch: {config['host']}:{config['port']}")
                return False
                
        except Exception as e:
            st.error(f"âŒ Elasticsearch é€£æ¥å¤±æ•—: {str(e)}")
            return False
    
    def _create_elasticsearch_index(self) -> bool:
        """å‰µå»º Elasticsearch ç´¢å¼•"""
        try:
            config = self.elasticsearch_config
            
            # ç´¢å¼•æ˜ å°„è¨­å®š (ä½¿ç”¨ä¸­æ–‡åˆ†æå™¨)
            index_mapping = {
                "settings": {
                    "number_of_shards": config['shards'],
                    "number_of_replicas": config['replicas'],
                    "analysis": {
                        "analyzer": {
                            "chinese_analyzer": {
                                "type": "custom",
                                "tokenizer": "standard",
                                "filter": ["lowercase", "cjk_width", "cjk_bigram"]
                            }
                        }
                    }
                },
                "mappings": {
                    "properties": {
                        config['text_field']: {
                            "type": "text",
                            "analyzer": "chinese_analyzer",
                            "search_analyzer": "chinese_analyzer"
                        },
                        config['vector_field']: {
                            "type": "dense_vector",
                            "dims": config['dimension'],
                            "index": True,
                            "similarity": config['similarity']
                        },
                        "metadata": {
                            "type": "object",
                            "properties": {
                                "source": {"type": "keyword"},
                                "page": {"type": "integer"},
                                "chunk_id": {"type": "keyword"},
                                "timestamp": {"type": "date"},
                                "file_type": {"type": "keyword"},
                                "file_size": {"type": "integer"}
                            }
                        }
                    }
                }
            }
            
            # æª¢æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            if self.elasticsearch_client.indices.exists(index=self.index_name):
                st.info(f"ğŸ“š ç´¢å¼• '{self.index_name}' å·²å­˜åœ¨")
                return True
            
            # å‰µå»ºç´¢å¼• - ä¿®å¾© async/await å…¼å®¹æ€§å•é¡Œ
            try:
                response = self.elasticsearch_client.indices.create(
                    index=self.index_name,
                    body=index_mapping,
                    ignore=400  # å¿½ç•¥å·²å­˜åœ¨çš„éŒ¯èª¤
                )
                
                if response.get('acknowledged', False):
                    st.success(f"âœ… æˆåŠŸå‰µå»ºç´¢å¼•: {self.index_name}")
                    return True
                else:
                    st.error(f"âŒ ç´¢å¼•å‰µå»ºå¤±æ•—: {response}")
                    return False
                    
            except Exception as create_error:
                # å¦‚æœæ˜¯ HeadApiResponse async éŒ¯èª¤ï¼Œå˜—è©¦åŒæ­¥æ–¹å¼
                error_msg = str(create_error)
                if "HeadApiResponse" in error_msg or "await" in error_msg:
                    st.warning(f"âš ï¸ æª¢æ¸¬åˆ°asyncå…¼å®¹æ€§å•é¡Œï¼Œå˜—è©¦åŒæ­¥æ–¹å¼å‰µå»ºç´¢å¼•...")
                    try:
                        # ä½¿ç”¨åŒæ­¥ Elasticsearch å®¢æˆ¶ç«¯é‡æ–°åˆå§‹åŒ–
                        from elasticsearch import Elasticsearch
                        sync_client = Elasticsearch(
                            [{'host': self.elasticsearch_config['host'], 'port': self.elasticsearch_config['port']}],
                            timeout=30,
                            request_timeout=30
                        )
                        
                        # æ¸¬è©¦é€£æ¥
                        if sync_client.ping():
                            # ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯å‰µå»ºç´¢å¼•
                            response = sync_client.indices.create(
                                index=self.index_name,
                                body=index_mapping,
                                ignore=400
                            )
                            # æ›´æ–°å®¢æˆ¶ç«¯ç‚ºåŒæ­¥ç‰ˆæœ¬
                            self.elasticsearch_client = sync_client
                            st.success(f"âœ… ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯æˆåŠŸå‰µå»ºç´¢å¼•: {self.index_name}")
                            return True
                        else:
                            st.error("âŒ åŒæ­¥å®¢æˆ¶ç«¯ç„¡æ³•é€£æ¥åˆ° Elasticsearch")
                            return False
                    except Exception as sync_error:
                        st.error(f"âŒ åŒæ­¥å‰µå»ºç´¢å¼•ä¹Ÿå¤±æ•—: {str(sync_error)}")
                        return False
                else:
                    st.error(f"âŒ å‰µå»ºç´¢å¼•å¤±æ•—: {error_msg}")
                    return False
                
        except Exception as e:
            st.error(f"âŒ å‰µå»ºç´¢å¼•éç¨‹å‡ºç¾ç•°å¸¸: {str(e)}")
            return False
    
    def _setup_elasticsearch_store(self) -> bool:
        """è¨­ç½® Elasticsearch å‘é‡å­˜å„²"""
        try:
            # ç¢ºä¿ä½¿ç”¨æ­£ç¢ºçš„åŒæ­¥å®¢æˆ¶ç«¯
            if not hasattr(self, 'elasticsearch_client') or not self.elasticsearch_client:
                st.error("âŒ Elasticsearch å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–")
                return False
            
            self.elasticsearch_store = ElasticsearchStore(
                es_client=self.elasticsearch_client,
                index_name=self.index_name,
                vector_field=self.elasticsearch_config['vector_field'],
                text_field=self.elasticsearch_config['text_field'],
                metadata_field='metadata'
            )
            
            st.success("âœ… Elasticsearch å‘é‡å­˜å„²è¨­ç½®å®Œæˆ (ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯)")
            return True
            
        except Exception as e:
            st.error(f"âŒ Elasticsearch å‘é‡å­˜å„²è¨­ç½®å¤±æ•—: {str(e)}")
            import traceback
            st.error(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
            return False
    
    def create_index(self, documents: List[Document]):
        """å‰µå»º Elasticsearch ç´¢å¼•"""
        with st.spinner("æ­£åœ¨å»ºç«‹ Elasticsearch ç´¢å¼•..."):
            try:
                # 1. ç¢ºä¿æ‰€æœ‰åŸºç¤è¨­å®šéƒ½å·²å°±ç·’
                if not all([
                    self._setup_elasticsearch_client(),
                    self._create_elasticsearch_index(),
                    self._setup_elasticsearch_store()
                ]):
                    st.error("âŒ Elasticsearch åŸºç¤è¨­å®šå¤±æ•—ï¼Œç„¡æ³•å»ºç«‹ç´¢å¼•ã€‚")
                    return None

                # 2. ç¢ºä¿æ¨¡å‹å·²åˆå§‹åŒ–
                self._ensure_models_initialized()

                # 3. ç¢ºä¿æ‰€æœ‰æ–‡ä»¶éƒ½æœ‰å”¯ä¸€çš„ ID å’Œæ™‚é–“æˆ³
                import uuid
                for doc in documents:
                    if not hasattr(doc, 'id_') or not doc.id_:
                        doc.id_ = str(uuid.uuid4())
                    if 'timestamp' not in doc.metadata:
                        doc.metadata['timestamp'] = datetime.now().isoformat()

                # 4. å»ºç«‹å­˜å„²ä¸Šä¸‹æ–‡
                storage_context = StorageContext.from_defaults(vector_store=self.elasticsearch_store)

                # 5. ä½¿ç”¨ LlamaIndex çš„æ¨™æº–æ–¹æ³•å»ºç«‹ç´¢å¼•
                try:
                    self.index = VectorStoreIndex.from_documents(
                        documents,
                        storage_context=storage_context,
                        embed_model=self.embedding_model,
                        show_progress=True
                    )

                    if self.index:
                        st.success(f"âœ… Elasticsearch ç´¢å¼•å»ºç«‹å®Œæˆï¼è™•ç†äº† {len(documents)} å€‹æ–‡æª”ã€‚")
                        
                        # é©—è­‰æ–‡æª”æ˜¯å¦çœŸçš„è¢«æ’å…¥
                        try:
                            stats = self.elasticsearch_client.indices.stats(index=self.index_name)
                            doc_count = stats['indices'][self.index_name]['total']['docs']['count']
                            st.info(f"ğŸ“Š Elasticsearch å¯¦éš›æ–‡æª”æ•¸: {doc_count}")
                            
                            if doc_count == 0:
                                st.warning("âš ï¸ ç´¢å¼•å‰µå»ºæˆåŠŸä½†æ–‡æª”æ•¸ç‚º0ï¼Œå¯èƒ½å­˜åœ¨æ’å…¥å•é¡Œ")
                        except Exception as stat_e:
                            st.warning(f"ç„¡æ³•ç²å–ç´¢å¼•çµ±è¨ˆ: {stat_e}")
                        
                        return self.index
                    else:
                        st.error("âŒ ç´¢å¼•å°è±¡ç‚º None")
                        return None
                        
                except Exception as index_e:
                    st.error(f"âŒ ç´¢å¼•å‰µå»ºéç¨‹å¤±æ•—: {str(index_e)}")
                    import traceback
                    st.error(f"ç´¢å¼•å‰µå»ºè©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
                    return None

            except Exception as e:
                st.error(f"âŒ Elasticsearch ç´¢å¼•å»ºç«‹å¤±æ•—: {str(e)}")
                import traceback
                st.error(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
                return None
    
    def setup_query_engine(self):
        """è¨­ç½® Elasticsearch æŸ¥è©¢å¼•æ“"""
        if self.index is None:
            st.warning("è«‹å…ˆå»ºç«‹ç´¢å¼•")
            return
        
        try:
            # å‰µå»ºæª¢ç´¢å™¨
            retriever = VectorIndexRetriever(
                index=self.index,
                similarity_top_k=10,  # å¢åŠ æª¢ç´¢æ•¸é‡
                vector_store_query_mode="default"
            )
            
            # å‰µå»ºå¾Œè™•ç†å™¨
            postprocessor = SimilarityPostprocessor(
                similarity_cutoff=0.7  # ç›¸ä¼¼åº¦éæ¿¾
            )
            
            # å‰µå»ºæŸ¥è©¢å¼•æ“
            self.query_engine = RetrieverQueryEngine.from_args(
                retriever=retriever,
                node_postprocessors=[postprocessor],
                response_mode="compact",  # ç·Šæ¹Šæ¨¡å¼ç¯€çœè¨˜æ†¶é«”
                streaming=False
            )
            
            st.success("âœ… Elasticsearch æŸ¥è©¢å¼•æ“è¨­ç½®å®Œæˆ")
            
        except Exception as e:
            st.error(f"âŒ æŸ¥è©¢å¼•æ“è¨­ç½®å¤±æ•—: {str(e)}")
    
    def get_elasticsearch_statistics(self) -> Dict[str, Any]:
        """ç²å– Elasticsearch çµ±è¨ˆè³‡è¨Š"""
        stats = super().get_enhanced_statistics()
        
        if self.elasticsearch_client:
            try:
                # ç´¢å¼•çµ±è¨ˆ
                index_stats = self.elasticsearch_client.indices.stats(
                    index=self.index_name
                )
                
                # é›†ç¾¤å¥åº·ç‹€æ…‹
                cluster_health = self.elasticsearch_client.cluster.health()
                
                # æœç´¢çµ±è¨ˆ
                search_stats = index_stats.get('indices', {}).get(self.index_name, {})
                
                elasticsearch_stats = {
                    'cluster_status': cluster_health.get('status', 'unknown'),
                    'total_documents': search_stats.get('total', {}).get('docs', {}).get('count', 0),
                    'index_size_bytes': search_stats.get('total', {}).get('store', {}).get('size_in_bytes', 0),
                    'search_queries': search_stats.get('total', {}).get('search', {}).get('query_total', 0),
                    'memory_stats': self.memory_stats,
                    'index_name': self.index_name,
                    'elasticsearch_config': self.elasticsearch_config
                }
                
                stats['elasticsearch_stats'] = elasticsearch_stats
                
            except Exception as e:
                st.warning(f"ç„¡æ³•ç²å– Elasticsearch çµ±è¨ˆ: {str(e)}")
        
        return stats
    
    def search_documents(self, query: str, size: int = 10) -> List[Dict]:
        """ç›´æ¥æœç´¢ Elasticsearch æ–‡æª”"""
        if not self.elasticsearch_client:
            return []
        
        try:
            # æ··åˆæœç´¢ï¼šå‘é‡æœç´¢ + æ–‡æœ¬æœç´¢
            search_query = {
                "size": size,
                "query": {
                    "bool": {
                        "should": [
                            # æ–‡æœ¬æœç´¢
                            {
                                "match": {
                                    self.elasticsearch_config['text_field']: {
                                        "query": query,
                                        "analyzer": "chinese_analyzer",
                                        "boost": 1.0
                                    }
                                }
                            },
                            # çŸ­èªæœç´¢
                            {
                                "match_phrase": {
                                    self.elasticsearch_config['text_field']: {
                                        "query": query,
                                        "boost": 1.5
                                    }
                                }
                            }
                        ],
                        "minimum_should_match": 1
                    }
                },
                "highlight": {
                    "fields": {
                        self.elasticsearch_config['text_field']: {}
                    }
                },
                "_source": ["*"]
            }
            
            response = self.elasticsearch_client.search(
                index=self.index_name,
                body=search_query
            )
            
            results = []
            for hit in response['hits']['hits']:
                result = {
                    'score': hit['_score'],
                    'content': hit['_source'].get(self.elasticsearch_config['text_field'], ''),
                    'metadata': hit['_source'].get('metadata', {}),
                    'highlights': hit.get('highlight', {})
                }
                results.append(result)
            
            return results
            
        except Exception as e:
            st.error(f"æœç´¢å¤±æ•—: {str(e)}")
            return []
    
    def cleanup_memory(self):
        """æ¸…ç†è¨˜æ†¶é«”"""
        try:
            import gc
            import psutil
            import os
            
            # ç²å–ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨é‡
            process = psutil.Process(os.getpid())
            memory_before = process.memory_info().rss / 1024 / 1024  # MB
            
            # æ¸…ç†æ“ä½œ
            gc.collect()
            
            # è¨˜éŒ„å³°å€¼è¨˜æ†¶é«”
            memory_after = process.memory_info().rss / 1024 / 1024  # MB
            self.memory_stats['peak_memory_mb'] = max(
                self.memory_stats['peak_memory_mb'], 
                memory_before
            )
            
            st.info(f"ğŸ§¹ è¨˜æ†¶é«”æ¸…ç†ï¼š{memory_before:.1f}MB â†’ {memory_after:.1f}MB")
            
        except Exception as e:
            st.warning(f"è¨˜æ†¶é«”æ¸…ç†å¤±æ•—: {str(e)}")
    
    def load_existing_index(self) -> bool:
        """è¼‰å…¥ç¾æœ‰çš„ Elasticsearch ç´¢å¼•"""
        try:
            # è¨­ç½® Elasticsearch å®¢æˆ¶ç«¯
            if not self._setup_elasticsearch_client():
                st.error("âŒ ç„¡æ³•é€£æ¥åˆ° Elasticsearch")
                return False
            
            # æª¢æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            if self.elasticsearch_client.indices.exists(index=self.index_name):
                # ç²å–ç´¢å¼•çµ±è¨ˆè³‡è¨Š
                stats = self.elasticsearch_client.indices.stats(index=self.index_name)
                doc_count = stats['indices'][self.index_name]['total']['docs']['count']
                
                if doc_count > 0:
                    st.success(f"âœ… ç™¼ç¾ç¾æœ‰çš„ Elasticsearch ç´¢å¼•ï¼š{self.index_name} ({doc_count} æ–‡æª”)")
                    
                    # è¨­ç½®å‘é‡å­˜å„²
                    if self._setup_elasticsearch_store():
                        # é‡æ–°å‰µå»ºç´¢å¼•å°è±¡
                        from llama_index.core import VectorStoreIndex
                        from llama_index.core.storage.storage_context import StorageContext
                        
                        self._ensure_models_initialized()
                        storage_context = StorageContext.from_defaults(vector_store=self.elasticsearch_store)
                        self.index = VectorStoreIndex.from_vector_store(
                            vector_store=self.elasticsearch_store,
                            storage_context=storage_context,
                            embed_model=self.embedding_model
                        )
                        
                        # è¨­ç½®æŸ¥è©¢å¼•æ“
                        self.setup_query_engine()
                        return True
                    else:
                        st.error("âŒ Elasticsearch å‘é‡å­˜å„²è¨­ç½®å¤±æ•—")
                        return False
                else:
                    st.info(f"ğŸ“š ç´¢å¼• '{self.index_name}' å­˜åœ¨ä½†ç‚ºç©º")
                    return False
            else:
                st.info("ğŸ’¡ æ²’æœ‰ç™¼ç¾ç¾æœ‰çš„ Elasticsearch ç´¢å¼•ï¼Œè«‹ä¸Šå‚³æ–‡æª”ä¾†å»ºç«‹æ–°ç´¢å¼•")
                return False
                
        except Exception as e:
            st.error(f"âŒ è¼‰å…¥ Elasticsearch ç´¢å¼•å¤±æ•—: {str(e)}")
            return False
    
    def get_enhanced_statistics(self) -> Dict[str, Any]:
        """ç²å– Elasticsearch RAG ç³»çµ±çš„çµ±è¨ˆè³‡è¨Š"""
        try:
            stats = {
                "system_type": "elasticsearch",
                "base_statistics": self.memory_stats.copy(),
                "elasticsearch_stats": {}
            }
            
            if self.elasticsearch_client and self.index_name:
                try:
                    # ç²å–ç´¢å¼•çµ±è¨ˆ
                    index_stats = self.elasticsearch_client.indices.stats(index=self.index_name)
                    total_stats = index_stats['indices'][self.index_name]['total']
                    
                    stats["elasticsearch_stats"] = {
                        "index_name": self.index_name,
                        "document_count": total_stats['docs']['count'],
                        "index_size_bytes": total_stats['store']['size_in_bytes'],
                        "index_size_mb": round(total_stats['store']['size_in_bytes'] / 1024 / 1024, 2)
                    }
                    
                    # æ›´æ–°åŸºç¤çµ±è¨ˆ
                    stats["base_statistics"]["total_documents"] = total_stats['docs']['count']
                    stats["base_statistics"]["total_nodes"] = total_stats['docs']['count']
                    
                except Exception as e:
                    st.warning(f"ç„¡æ³•ç²å– Elasticsearch çµ±è¨ˆ: {e}")
                    
            return stats
            
        except Exception as e:
            st.error(f"çµ±è¨ˆè³‡è¨Šç²å–å¤±æ•—: {e}")
            return {
                "system_type": "elasticsearch", 
                "base_statistics": self.memory_stats.copy(),
                "elasticsearch_stats": {}
            }
    
    def get_elasticsearch_statistics(self) -> Dict[str, Any]:
        """ç²å–è©³ç´°çš„ Elasticsearch çµ±è¨ˆè³‡è¨Š"""
        return self.get_enhanced_statistics()
    
    def delete_documents_by_source(self, source_filename: str) -> bool:
        """æ ¹æ“šä¾†æºæ–‡ä»¶ååˆªé™¤æ–‡æª”"""
        if not self.elasticsearch_client:
            st.error("âŒ Elasticsearch å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–")
            return False
        
        try:
            # æ§‹å»ºæŸ¥è©¢ä»¥æŸ¥æ‰¾æŒ‡å®šä¾†æºçš„æ–‡æª”
            query = {
                "query": {
                    "term": {
                        "metadata.source.keyword": source_filename
                    }
                }
            }
            
            # åˆªé™¤åŒ¹é…çš„æ–‡æª”
            response = self.elasticsearch_client.delete_by_query(
                index=self.index_name,
                body=query
            )
            
            deleted_count = response.get('deleted', 0)
            if deleted_count > 0:
                st.success(f"âœ… å¾ Elasticsearch ä¸­åˆªé™¤äº† {deleted_count} å€‹æ–‡æª”å¡Šï¼ˆä¾†æºï¼š{source_filename}ï¼‰")
                return True
            else:
                st.info(f"ğŸ“ åœ¨ Elasticsearch ä¸­æ²’æœ‰æ‰¾åˆ°ä¾†æºç‚º '{source_filename}' çš„æ–‡æª”")
                return False
                
        except Exception as e:
            st.error(f"âŒ å¾ Elasticsearch åˆªé™¤æ–‡æª”å¤±æ•—: {str(e)}")
            return False
    
    def refresh_index_after_deletion(self):
        """åˆªé™¤æ–‡æª”å¾Œåˆ·æ–°ç´¢å¼•"""
        try:
            if self.elasticsearch_client:
                self.elasticsearch_client.indices.refresh(index=self.index_name)
                st.info("ğŸ”„ Elasticsearch ç´¢å¼•å·²åˆ·æ–°")
        except Exception as e:
            st.warning(f"ç´¢å¼•åˆ·æ–°è­¦å‘Š: {str(e)}")

    def __del__(self):
        """ææ§‹å‡½æ•¸ï¼šæ¸…ç†è³‡æº"""
        try:
            if hasattr(self, 'elasticsearch_client') and self.elasticsearch_client:
                self.elasticsearch_client.close()
        except:
            pass