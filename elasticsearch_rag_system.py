import os
import json
from typing import List, Dict, Any, Optional
import streamlit as st
from datetime import datetime

# LlamaIndex æ ¸å¿ƒ
from llama_index.core import VectorStoreIndex, Document, Settings
from llama_index.core.storage.storage_context import StorageContext
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.postprocessor import SimilarityPostprocessor

# Elasticsearch integration
try:
    from elasticsearch import Elasticsearch
    from custom_elasticsearch_store import CustomElasticsearchStore
    ELASTICSEARCH_AVAILABLE = True
except ImportError:
    ELASTICSEARCH_AVAILABLE = False
    st.warning("âš ï¸ Elasticsearch dependencies not installed. Install with: pip install elasticsearch llama-index-vector-stores-elasticsearch")

# ç¹¼æ‰¿å¢å¼·ç‰ˆç³»çµ±
from enhanced_rag_system import EnhancedRAGSystem
from config import (
    GROQ_API_KEY, EMBEDDING_MODEL, LLM_MODEL, 
    ELASTICSEARCH_HOST, ELASTICSEARCH_INDEX_NAME,
    ELASTICSEARCH_USERNAME, ELASTICSEARCH_PASSWORD
)

class ElasticsearchRAGSystem(EnhancedRAGSystem):
    """Elasticsearch RAG ç³»çµ± - é«˜æ•ˆèƒ½ã€å¯æ“´å±•çš„å‘é‡æª¢ç´¢"""
    
    def __init__(self, elasticsearch_config: Optional[Dict] = None):
        """åˆå§‹åŒ– Elasticsearch RAG ç³»çµ±"""
        super().__init__(use_chroma=False)  # ä¸ä½¿ç”¨ ChromaDB
        
        self.elasticsearch_config = elasticsearch_config or self._get_default_config()
        self.elasticsearch_client = None
        self.elasticsearch_store = None
        self.index_name = self.elasticsearch_config.get('index_name', 'rag_documents')
        
        # è¨˜æ†¶é«”ä½¿ç”¨ç›£æ§
        self.memory_stats = {
            'documents_processed': 0,
            'vectors_stored': 0,
            'peak_memory_mb': 0
        }
        
        # æ¨¡å‹å¯¦ä¾‹
        self.embedding_model = None
        self.llm_model = None
    
    def _ensure_models_initialized(self):
        """ç¢ºä¿æ¨¡å‹å·²åˆå§‹åŒ–ä¸¦å­˜å„²ç‚ºå¯¦ä¾‹å±¬æ€§"""
        if not self.models_initialized or self.embedding_model is None:
            super()._ensure_models_initialized()
            
            # å¾ Settings ç²å–æ¨¡å‹ä¸¦å­˜å„²ç‚ºå¯¦ä¾‹å±¬æ€§
            from llama_index.core import Settings
            self.embedding_model = Settings.embed_model
            self.llm_model = Settings.llm
            
            st.info("âœ… Elasticsearch RAG ç³»çµ±æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def _get_default_config(self) -> Dict:
        """ç²å–é è¨­ Elasticsearch é…ç½®"""
        return {
            'host': ELASTICSEARCH_HOST or 'localhost',
            'port': 9200,
            'scheme': 'http',
            'username': ELASTICSEARCH_USERNAME,
            'password': ELASTICSEARCH_PASSWORD,
            'index_name': ELASTICSEARCH_INDEX_NAME or 'rag_intelligent_assistant',
            'dimension': 384,  # all-MiniLM-L6-v2 embedding dimension
            'similarity': 'cosine',
            'text_field': 'content',
            'vector_field': 'embedding',
            'metadata_fields': ['source', 'page', 'chunk_id', 'timestamp']
        }
    
    def _setup_elasticsearch_client(self) -> bool:
        """è¨­ç½® Elasticsearch å®¢æˆ¶ç«¯"""
        if not ELASTICSEARCH_AVAILABLE:
            st.error("âŒ Elasticsearch ä¾è³´æœªå®‰è£")
            return False
        
        try:
            config = self.elasticsearch_config
            
            # é€£æ¥é…ç½®
            es_config = {
                'hosts': [f"{config['scheme']}://{config['host']}:{config['port']}"],
                'timeout': 30,
                'max_retries': 3,
                'retry_on_timeout': True
            }
            
            # èªè­‰é…ç½®
            if config.get('username') and config.get('password'):
                es_config['basic_auth'] = (config['username'], config['password'])
            
            self.elasticsearch_client = Elasticsearch(**es_config)
            
            # æ¸¬è©¦é€£æ¥
            if self.elasticsearch_client.ping():
                st.success(f"âœ… æˆåŠŸé€£æ¥åˆ° Elasticsearch: {config['host']}:{config['port']}")
                return True
            else:
                st.error(f"âŒ ç„¡æ³•é€£æ¥åˆ° Elasticsearch: {config['host']}:{config['port']}")
                return False
                
        except Exception as e:
            st.error(f"âŒ Elasticsearch é€£æ¥å¤±æ•—: {str(e)}")
            return False
    
    def _create_elasticsearch_index(self) -> bool:
        """å‰µå»º Elasticsearch ç´¢å¼•"""
        try:
            config = self.elasticsearch_config
            
            # ç´¢å¼•æ˜ å°„è¨­å®š (ä½¿ç”¨æ¨™æº–åˆ†æå™¨ï¼Œå…¼å®¹æ€§æ›´å¥½)
            index_mapping = {
                "settings": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    "analysis": {
                        "analyzer": {
                            "text_analyzer": {
                                "type": "custom",
                                "tokenizer": "standard",
                                "filter": ["lowercase", "stop", "cjk_width"]
                            }
                        }
                    }
                },
                "mappings": {
                    "properties": {
                        config['text_field']: {
                            "type": "text",
                            "analyzer": "text_analyzer",
                            "search_analyzer": "text_analyzer"
                        },
                        config['vector_field']: {
                            "type": "dense_vector",
                            "dims": config['dimension'],
                            "index": True,
                            "similarity": config['similarity']
                        },
                        "metadata": {
                            "type": "object",
                            "properties": {
                                "source": {"type": "keyword"},
                                "page": {"type": "integer"},
                                "chunk_id": {"type": "keyword"},
                                "timestamp": {"type": "date"},
                                "file_type": {"type": "keyword"},
                                "file_size": {"type": "integer"}
                            }
                        }
                    }
                }
            }
            
            # æª¢æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            if self.elasticsearch_client.indices.exists(index=self.index_name):
                st.info(f"ğŸ“š ç´¢å¼• '{self.index_name}' å·²å­˜åœ¨")
                return True
            
            # å‰µå»ºç´¢å¼•
            response = self.elasticsearch_client.indices.create(
                index=self.index_name,
                body=index_mapping,
                ignore=400  # å¿½ç•¥å·²å­˜åœ¨çš„éŒ¯èª¤
            )
            
            if response.get('acknowledged', False):
                st.success(f"âœ… æˆåŠŸå‰µå»ºç´¢å¼•: {self.index_name}")
                return True
            else:
                st.error(f"âŒ ç´¢å¼•å‰µå»ºå¤±æ•—: {response}")
                return False
                
        except Exception as e:
            st.error(f"âŒ å‰µå»ºç´¢å¼•å¤±æ•—: {str(e)}")
            return False
    
    def _setup_elasticsearch_store(self) -> bool:
        """è¨­ç½® Elasticsearch å‘é‡å­˜å„²"""
        try:
            self.elasticsearch_store = CustomElasticsearchStore(
                es_client=self.elasticsearch_client,
                index_name=self.index_name,
                text_field=self.elasticsearch_config['text_field'],
                vector_field=self.elasticsearch_config['vector_field'],
                metadata_field='metadata'
            )
            
            st.success("âœ… Elasticsearch å‘é‡å­˜å„²è¨­ç½®å®Œæˆ (ä½¿ç”¨è‡ªå®šç¾©å¯¦ç¾)")
            return True
            
        except Exception as e:
            st.error(f"âŒ Elasticsearch å‘é‡å­˜å„²è¨­ç½®å¤±æ•—: {str(e)}")
            return False
    
    def create_index(self, documents: List[Document]):
        """å‰µå»º Elasticsearch ç´¢å¼•"""
        with st.spinner("æ­£åœ¨å»ºç«‹ Elasticsearch ç´¢å¼•..."):
            try:
                # è¨­ç½® Elasticsearch
                if not self._setup_elasticsearch_client():
                    st.error("âŒ Elasticsearch å®¢æˆ¶ç«¯è¨­ç½®å¤±æ•—")
                    return None
                
                if not self._create_elasticsearch_index():
                    st.error("âŒ Elasticsearch ç´¢å¼•å‰µå»ºå¤±æ•—")  
                    return None
                
                if not self._setup_elasticsearch_store():
                    st.error("âŒ Elasticsearch å‘é‡å­˜å„²è¨­ç½®å¤±æ•—")
                    return None
                
                # åˆå§‹åŒ–æ¨¡å‹ç¢ºä¿åµŒå…¥æ¨¡å‹å¯ç”¨
                self._ensure_models_initialized()
                
                # å»ºç«‹å­˜å„²ä¸Šä¸‹æ–‡
                storage_context = StorageContext.from_defaults(
                    vector_store=self.elasticsearch_store
                )
                
                # é‡ç½®è¨˜æ†¶é«”çµ±è¨ˆ
                self.memory_stats['documents_processed'] = 0
                self.memory_stats['vectors_stored'] = 0
                
                st.info(f"ğŸ“Š æº–å‚™è™•ç† {len(documents)} å€‹æ–‡æª”")
                
                # ç‚ºæ–‡æª”æ·»åŠ æ™‚é–“æˆ³
                for i, doc in enumerate(documents):
                    if 'timestamp' not in doc.metadata:
                        doc.metadata['timestamp'] = datetime.now().isoformat()
                    if not hasattr(doc, 'id_') or not doc.id_:
                        doc.id_ = f"doc_{i}_{datetime.now().timestamp()}"
                
                # ä½¿ç”¨ç¶“éé©—è­‰çš„æ–¹æ³•å‰µå»ºç´¢å¼•
                st.info("ğŸ”„ ä½¿ç”¨ VectorStoreIndex.from_documents å‰µå»ºç´¢å¼•...")
                
                try:
                    self.index = VectorStoreIndex.from_documents(
                        documents,
                        storage_context=storage_context,
                        embed_model=self.embedding_model,
                        show_progress=True
                    )
                    
                    # æ›´æ–°çµ±è¨ˆ
                    self.memory_stats['documents_processed'] = len(documents)
                    self.memory_stats['vectors_stored'] = len(documents)
                    
                    st.success(f"âœ… Elasticsearch ç´¢å¼•å»ºç«‹å®Œæˆï¼è™•ç†äº† {len(documents)} å€‹æ–‡æª”")
                    return self.index
                    
                except Exception as index_error:
                    st.warning(f"âš ï¸ from_documents æ–¹æ³•å¤±æ•—: {index_error}")
                    st.info("ğŸ”„ å˜—è©¦æ›¿ä»£æ–¹æ³•...")
                    
                    # æ›¿ä»£æ–¹æ³•ï¼šå‰µå»ºç©ºç´¢å¼•ç„¶å¾Œé€å€‹æ·»åŠ æ–‡æª”
                    try:
                        self.index = VectorStoreIndex(
                            nodes=[],
                            storage_context=storage_context,
                            embed_model=self.embedding_model
                        )
                        
                        progress_bar = st.progress(0)
                        
                        for i, doc in enumerate(documents):
                            try:
                                self.index.insert(doc)
                                self.memory_stats['documents_processed'] += 1
                                self.memory_stats['vectors_stored'] += 1
                                
                                # æ›´æ–°é€²åº¦æ¢
                                progress = (i + 1) / len(documents)
                                progress_bar.progress(progress)
                                
                            except Exception as doc_error:
                                st.warning(f"âš ï¸ æ–‡æª” {i+1} æ’å…¥å¤±æ•—: {doc_error}")
                        
                        progress_bar.progress(1.0)
                        
                        processed_count = self.memory_stats['documents_processed']
                        if processed_count > 0:
                            st.success(f"âœ… æ›¿ä»£æ–¹æ³•æˆåŠŸï¼è™•ç†äº† {processed_count} å€‹æ–‡æª”")
                            return self.index
                        else:
                            st.error("âŒ æ²’æœ‰æ–‡æª”æˆåŠŸç´¢å¼•")
                            return None
                            
                    except Exception as alt_error:
                        st.error(f"âŒ æ›¿ä»£æ–¹æ³•ä¹Ÿå¤±æ•—: {alt_error}")
                        return None
                
            except Exception as e:
                st.error(f"âŒ Elasticsearch ç´¢å¼•å»ºç«‹å¤±æ•—: {str(e)}")
                import traceback
                st.error(f"éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}")
                return None
    
    def setup_query_engine(self):
        """è¨­ç½® Elasticsearch æŸ¥è©¢å¼•æ“"""
        if self.index is None:
            st.warning("è«‹å…ˆå»ºç«‹ç´¢å¼•")
            return
        
        try:
            # å‰µå»ºæª¢ç´¢å™¨
            retriever = VectorIndexRetriever(
                index=self.index,
                similarity_top_k=10,  # å¢åŠ æª¢ç´¢æ•¸é‡
                vector_store_query_mode="default"
            )
            
            # å‰µå»ºå¾Œè™•ç†å™¨
            postprocessor = SimilarityPostprocessor(
                similarity_cutoff=0.7  # ç›¸ä¼¼åº¦éæ¿¾
            )
            
            # å‰µå»ºæŸ¥è©¢å¼•æ“
            self.query_engine = RetrieverQueryEngine.from_args(
                retriever=retriever,
                node_postprocessors=[postprocessor],
                response_mode="compact",  # ç·Šæ¹Šæ¨¡å¼ç¯€çœè¨˜æ†¶é«”
                streaming=False
            )
            
            st.success("âœ… Elasticsearch æŸ¥è©¢å¼•æ“è¨­ç½®å®Œæˆ")
            
        except Exception as e:
            st.error(f"âŒ æŸ¥è©¢å¼•æ“è¨­ç½®å¤±æ•—: {str(e)}")
    
    def get_elasticsearch_statistics(self) -> Dict[str, Any]:
        """ç²å– Elasticsearch çµ±è¨ˆè³‡è¨Š"""
        stats = super().get_enhanced_statistics()
        
        if self.elasticsearch_client:
            try:
                # ç´¢å¼•çµ±è¨ˆ
                index_stats = self.elasticsearch_client.indices.stats(
                    index=self.index_name
                )
                
                # é›†ç¾¤å¥åº·ç‹€æ…‹
                cluster_health = self.elasticsearch_client.cluster.health()
                
                # æœç´¢çµ±è¨ˆ
                search_stats = index_stats.get('indices', {}).get(self.index_name, {})
                
                elasticsearch_stats = {
                    'cluster_status': cluster_health.get('status', 'unknown'),
                    'total_documents': search_stats.get('total', {}).get('docs', {}).get('count', 0),
                    'index_size_bytes': search_stats.get('total', {}).get('store', {}).get('size_in_bytes', 0),
                    'search_queries': search_stats.get('total', {}).get('search', {}).get('query_total', 0),
                    'memory_stats': self.memory_stats,
                    'index_name': self.index_name,
                    'elasticsearch_config': self.elasticsearch_config
                }
                
                stats['elasticsearch_stats'] = elasticsearch_stats
                
            except Exception as e:
                st.warning(f"ç„¡æ³•ç²å– Elasticsearch çµ±è¨ˆ: {str(e)}")
        
        return stats
    
    def search_documents(self, query: str, size: int = 10) -> List[Dict]:
        """ç›´æ¥æœç´¢ Elasticsearch æ–‡æª”"""
        if not self.elasticsearch_client:
            return []
        
        try:
            # æ··åˆæœç´¢ï¼šå‘é‡æœç´¢ + æ–‡æœ¬æœç´¢
            search_query = {
                "size": size,
                "query": {
                    "bool": {
                        "should": [
                            # æ–‡æœ¬æœç´¢
                            {
                                "match": {
                                    self.elasticsearch_config['text_field']: {
                                        "query": query,
                                        "analyzer": "chinese_analyzer",
                                        "boost": 1.0
                                    }
                                }
                            },
                            # çŸ­èªæœç´¢
                            {
                                "match_phrase": {
                                    self.elasticsearch_config['text_field']: {
                                        "query": query,
                                        "boost": 1.5
                                    }
                                }
                            }
                        ],
                        "minimum_should_match": 1
                    }
                },
                "highlight": {
                    "fields": {
                        self.elasticsearch_config['text_field']: {}
                    }
                },
                "_source": ["*"]
            }
            
            response = self.elasticsearch_client.search(
                index=self.index_name,
                body=search_query
            )
            
            results = []
            for hit in response['hits']['hits']:
                result = {
                    'score': hit['_score'],
                    'content': hit['_source'].get(self.elasticsearch_config['text_field'], ''),
                    'metadata': hit['_source'].get('metadata', {}),
                    'highlights': hit.get('highlight', {})
                }
                results.append(result)
            
            return results
            
        except Exception as e:
            st.error(f"æœç´¢å¤±æ•—: {str(e)}")
            return []
    
    def cleanup_memory(self):
        """æ¸…ç†è¨˜æ†¶é«”"""
        try:
            import gc
            import psutil
            import os
            
            # ç²å–ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨é‡
            process = psutil.Process(os.getpid())
            memory_before = process.memory_info().rss / 1024 / 1024  # MB
            
            # æ¸…ç†æ“ä½œ
            gc.collect()
            
            # è¨˜éŒ„å³°å€¼è¨˜æ†¶é«”
            memory_after = process.memory_info().rss / 1024 / 1024  # MB
            self.memory_stats['peak_memory_mb'] = max(
                self.memory_stats['peak_memory_mb'], 
                memory_before
            )
            
            st.info(f"ğŸ§¹ è¨˜æ†¶é«”æ¸…ç†ï¼š{memory_before:.1f}MB â†’ {memory_after:.1f}MB")
            
        except Exception as e:
            st.warning(f"è¨˜æ†¶é«”æ¸…ç†å¤±æ•—: {str(e)}")
    
    def load_existing_index(self) -> bool:
        """è¼‰å…¥ç¾æœ‰çš„ Elasticsearch ç´¢å¼•"""
        try:
            # è¨­ç½® Elasticsearch å®¢æˆ¶ç«¯
            if not self._setup_elasticsearch_client():
                st.error("âŒ ç„¡æ³•é€£æ¥åˆ° Elasticsearch")
                return False
            
            # æª¢æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            if self.elasticsearch_client.indices.exists(index=self.index_name):
                # ç²å–ç´¢å¼•çµ±è¨ˆè³‡è¨Š
                stats = self.elasticsearch_client.indices.stats(index=self.index_name)
                doc_count = stats['indices'][self.index_name]['total']['docs']['count']
                
                if doc_count > 0:
                    st.success(f"âœ… ç™¼ç¾ç¾æœ‰çš„ Elasticsearch ç´¢å¼•ï¼š{self.index_name} ({doc_count} æ–‡æª”)")
                    
                    # è¨­ç½®å‘é‡å­˜å„²
                    if self._setup_elasticsearch_store():
                        # é‡æ–°å‰µå»ºç´¢å¼•å°è±¡
                        from llama_index.core import VectorStoreIndex
                        from llama_index.core.storage.storage_context import StorageContext
                        
                        self._ensure_models_initialized()
                        storage_context = StorageContext.from_defaults(vector_store=self.elasticsearch_store)
                        self.index = VectorStoreIndex.from_vector_store(
                            vector_store=self.elasticsearch_store,
                            storage_context=storage_context,
                            embed_model=self.embedding_model
                        )
                        
                        # è¨­ç½®æŸ¥è©¢å¼•æ“
                        self.setup_query_engine()
                        return True
                    else:
                        st.error("âŒ Elasticsearch å‘é‡å­˜å„²è¨­ç½®å¤±æ•—")
                        return False
                else:
                    st.info(f"ğŸ“š ç´¢å¼• '{self.index_name}' å­˜åœ¨ä½†ç‚ºç©º")
                    return False
            else:
                st.info("ğŸ’¡ æ²’æœ‰ç™¼ç¾ç¾æœ‰çš„ Elasticsearch ç´¢å¼•ï¼Œè«‹ä¸Šå‚³æ–‡æª”ä¾†å»ºç«‹æ–°ç´¢å¼•")
                return False
                
        except Exception as e:
            st.error(f"âŒ è¼‰å…¥ Elasticsearch ç´¢å¼•å¤±æ•—: {str(e)}")
            return False
    
    def get_enhanced_statistics(self) -> Dict[str, Any]:
        """ç²å– Elasticsearch RAG ç³»çµ±çš„çµ±è¨ˆè³‡è¨Š"""
        try:
            stats = {
                "system_type": "elasticsearch",
                "base_statistics": self.memory_stats.copy(),
                "elasticsearch_stats": {}
            }
            
            if self.elasticsearch_client and self.index_name:
                try:
                    # ç²å–ç´¢å¼•çµ±è¨ˆ
                    index_stats = self.elasticsearch_client.indices.stats(index=self.index_name)
                    total_stats = index_stats['indices'][self.index_name]['total']
                    
                    stats["elasticsearch_stats"] = {
                        "index_name": self.index_name,
                        "document_count": total_stats['docs']['count'],
                        "index_size_bytes": total_stats['store']['size_in_bytes'],
                        "index_size_mb": round(total_stats['store']['size_in_bytes'] / 1024 / 1024, 2)
                    }
                    
                    # æ›´æ–°åŸºç¤çµ±è¨ˆ
                    stats["base_statistics"]["total_documents"] = total_stats['docs']['count']
                    stats["base_statistics"]["total_nodes"] = total_stats['docs']['count']
                    
                except Exception as e:
                    st.warning(f"ç„¡æ³•ç²å– Elasticsearch çµ±è¨ˆ: {e}")
                    
            return stats
            
        except Exception as e:
            st.error(f"çµ±è¨ˆè³‡è¨Šç²å–å¤±æ•—: {e}")
            return {
                "system_type": "elasticsearch", 
                "base_statistics": self.memory_stats.copy(),
                "elasticsearch_stats": {}
            }
    
    def get_elasticsearch_statistics(self) -> Dict[str, Any]:
        """ç²å–è©³ç´°çš„ Elasticsearch çµ±è¨ˆè³‡è¨Š"""
        return self.get_enhanced_statistics()
    
    def __del__(self):
        """ææ§‹å‡½æ•¸ï¼šæ¸…ç†è³‡æº"""
        try:
            if hasattr(self, 'elasticsearch_client') and self.elasticsearch_client:
                self.elasticsearch_client.close()
        except:
            pass