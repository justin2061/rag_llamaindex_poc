import os
import asyncio
import nest_asyncio
from typing import List, Optional, Dict, Any, Callable, Union
import streamlit as st
import networkx as nx
from pyvis.network import Network
import tempfile

# LlamaIndex æ ¸å¿ƒ
from llama_index.core import VectorStoreIndex, Document, PropertyGraphIndex
from llama_index.core.storage.storage_context import StorageContext
from llama_index.core.graph_stores.simple import SimpleGraphStore
from llama_index.core.retrievers import KnowledgeGraphRAGRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.schema import TransformComponent, BaseNode
from llama_index.core.indices.property_graph.utils import default_parse_triplets_fn
from llama_index.core.graph_stores.types import EntityNode, KG_NODES_KEY, KG_RELATIONS_KEY, Relation
from llama_index.core.prompts import PromptTemplate
from llama_index.core.prompts.default_prompts import DEFAULT_KG_TRIPLET_EXTRACT_PROMPT
from llama_index.core.async_utils import run_jobs
from llama_index.core.llms.llm import LLM

# ç¹¼æ‰¿åŸæœ‰ç³»çµ±
from enhanced_rag_system import EnhancedRAGSystem
from config import (
    GROQ_API_KEY, EMBEDDING_MODEL, LLM_MODEL, INDEX_DIR,
    ENABLE_GRAPH_RAG, MAX_TRIPLETS_PER_CHUNK, GRAPH_COMMUNITY_SIZE
)

# æ‡‰ç”¨ nest_asyncio ä»¥æ”¯æ´åœ¨ Streamlit ä¸­ä½¿ç”¨ asyncio
nest_asyncio.apply()

class GraphRAGExtractor(TransformComponent):
    """Graph RAG çŸ¥è­˜åœ–è­œæå–å™¨"""
    
    def __init__(
        self,
        llm: Optional[LLM] = None,
        extract_prompt: Optional[Union[str, PromptTemplate]] = None,
        parse_fn: Callable = default_parse_triplets_fn,
        max_paths_per_chunk: int = 10,
        num_workers: int = 4,
    ) -> None:
        """åˆå§‹åŒ–æå–å™¨"""
        from llama_index.core import Settings
        
        if isinstance(extract_prompt, str):
            extract_prompt = PromptTemplate(extract_prompt)
            
        super().__init__(
            llm=llm or Settings.llm,
            extract_prompt=extract_prompt or DEFAULT_KG_TRIPLET_EXTRACT_PROMPT,
            parse_fn=parse_fn,
            num_workers=num_workers,
            max_paths_per_chunk=max_paths_per_chunk,
        )
    
    @classmethod
    def class_name(cls) -> str:
        return "GraphRAGExtractor"
    
    def __call__(
        self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any
    ) -> List[BaseNode]:
        """åŒæ­¥æå–æ¥å£ - å…¼å®¹ Streamlit ç’°å¢ƒ"""
        if not nodes:
            return nodes
            
        try:
            return self._safe_async_call(nodes, show_progress, **kwargs)
        except Exception as e:
            st.warning(f"åœ–è­œæå–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            st.info("å°‡ä½¿ç”¨ç°¡åŒ–çš„è™•ç†æ–¹å¼ç¹¼çºŒ...")
            return self._fallback_sync_processing(nodes)
    
    def _safe_async_call(self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any) -> List[BaseNode]:
        """å®‰å…¨çš„ç•°æ­¥èª¿ç”¨è™•ç†"""
        try:
            # æ–¹æ³• 1: æª¢æŸ¥ç•¶å‰äº‹ä»¶å¾ªç’°ç‹€æ…‹
            loop = asyncio.get_running_loop()
            
            # åœ¨å·²æœ‰äº‹ä»¶å¾ªç’°ä¸­ï¼Œä½¿ç”¨ç·šç¨‹æ± åŸ·è¡Œ
            st.info("ğŸ”„ æª¢æ¸¬åˆ° Streamlit ç’°å¢ƒï¼Œä½¿ç”¨ç·šç¨‹æ± è™•ç†ç•°æ­¥æ“ä½œ...")
            
            import concurrent.futures
            import threading
            
            def run_in_thread():
                # åœ¨æ–°ç·šç¨‹ä¸­å‰µå»ºç¨ç«‹çš„äº‹ä»¶å¾ªç’°
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    result = new_loop.run_until_complete(
                        self.acall(nodes, show_progress=show_progress, **kwargs)
                    )
                    return result
                finally:
                    new_loop.close()
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(run_in_thread)
                return future.result(timeout=300)  # 5åˆ†é˜è¶…æ™‚
                
        except RuntimeError:
            # æ–¹æ³• 2: æ²’æœ‰é‹è¡Œçš„äº‹ä»¶å¾ªç’°ï¼Œç›´æ¥ä½¿ç”¨ asyncio.run
            st.info("ğŸ”„ ä½¿ç”¨æ¨™æº–ç•°æ­¥è™•ç†...")
            return asyncio.run(
                self.acall(nodes, show_progress=show_progress, **kwargs)
            )
        except Exception as e:
            st.warning(f"ç•°æ­¥è™•ç†å¤±æ•—: {str(e)}")
            raise
    
    def _fallback_sync_processing(self, nodes: List[BaseNode]) -> List[BaseNode]:
        """å‚™ç”¨çš„åŒæ­¥è™•ç†æ–¹å¼"""
        st.info("ğŸ”„ ä½¿ç”¨åŒæ­¥å‚™ç”¨æ–¹æ¡ˆè™•ç†çŸ¥è­˜åœ–è­œ...")
        
        processed_nodes = []
        for i, node in enumerate(nodes):
            try:
                # é¡¯ç¤ºé€²åº¦
                progress = (i + 1) / len(nodes)
                st.progress(progress, text=f"è™•ç†ç¯€é» {i+1}/{len(nodes)}")
                
                # åŒæ­¥è™•ç†å–®å€‹ç¯€é»
                processed_node = self._sync_extract_single_node(node)
                processed_nodes.append(processed_node)
                
            except Exception as e:
                st.warning(f"è™•ç†ç¯€é» {i+1} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                processed_nodes.append(node)  # ä¿ç•™åŸç¯€é»
        
        return processed_nodes
    
    def _sync_extract_single_node(self, node: BaseNode) -> BaseNode:
        """åŒæ­¥æå–å–®å€‹ç¯€é»çš„çŸ¥è­˜åœ–è­œ"""
        try:
            text = node.get_content(metadata_mode="llm")
            
            # åŒæ­¥èª¿ç”¨ LLM
            llm_response = self.llm.predict(
                self.extract_prompt,
                text=text,
                max_knowledge_triplets=self.max_paths_per_chunk,
            )
            
            # è§£æçµæœ
            entities, entities_relationship = self.parse_fn(llm_response)
            
            # è™•ç†å¯¦é«”å’Œé—œä¿‚ï¼ˆèˆ‡ç•°æ­¥ç‰ˆæœ¬ç›¸åŒçš„é‚è¼¯ï¼‰
            existing_nodes = node.metadata.pop(KG_NODES_KEY, [])
            existing_relations = node.metadata.pop(KG_RELATIONS_KEY, [])
            
            # è™•ç†å¯¦é«”
            metadata = node.metadata.copy()
            for entity, entity_type, description in entities:
                metadata["entity_description"] = description
                entity_node = EntityNode(
                    name=entity, label=entity_type, properties=metadata
                )
                existing_nodes.append(entity_node)
            
            # è™•ç†é—œä¿‚
            metadata = node.metadata.copy()
            for triple in entities_relationship:
                subj, rel, obj, description = triple
                subj_node = EntityNode(name=subj, properties=metadata)
                obj_node = EntityNode(name=obj, properties=metadata)
                metadata["relationship_description"] = description
                rel_node = Relation(
                    label=rel,
                    source_id=subj_node.id,
                    target_id=obj_node.id,
                    properties=metadata,
                )
                
                existing_nodes.extend([subj_node, obj_node])
                existing_relations.append(rel_node)
            
            node.metadata[KG_NODES_KEY] = existing_nodes
            node.metadata[KG_RELATIONS_KEY] = existing_relations
            return node
            
        except Exception as e:
            st.warning(f"åŒæ­¥è™•ç†ç¯€é»å¤±æ•—: {str(e)}")
            return node
    
    async def _aextract(self, node: BaseNode) -> BaseNode:
        """ç•°æ­¥æå–å–®å€‹ç¯€é»çš„ä¸‰å…ƒçµ„"""
        assert hasattr(node, "text")
        
        text = node.get_content(metadata_mode="llm")
        try:
            llm_response = await self.llm.apredict(
                self.extract_prompt,
                text=text,
                max_knowledge_triplets=self.max_paths_per_chunk,
            )
            
            entities, entities_relationship = self.parse_fn(llm_response)
        except ValueError:
            entities = []
            entities_relationship = []
        
        existing_nodes = node.metadata.pop(KG_NODES_KEY, [])
        existing_relations = node.metadata.pop(KG_RELATIONS_KEY, [])
        
        # è™•ç†å¯¦é«”
        metadata = node.metadata.copy()
        for entity, entity_type, description in entities:
            metadata["entity_description"] = description
            entity_node = EntityNode(
                name=entity, label=entity_type, properties=metadata
            )
            existing_nodes.append(entity_node)
        
        # è™•ç†é—œä¿‚
        metadata = node.metadata.copy()
        for triple in entities_relationship:
            subj, rel, obj, description = triple
            subj_node = EntityNode(name=subj, properties=metadata)
            obj_node = EntityNode(name=obj, properties=metadata)
            metadata["relationship_description"] = description
            rel_node = Relation(
                label=rel,
                source_id=subj_node.id,
                target_id=obj_node.id,
                properties=metadata,
            )
            
            existing_nodes.extend([subj_node, obj_node])
            existing_relations.append(rel_node)
        
        node.metadata[KG_NODES_KEY] = existing_nodes
        node.metadata[KG_RELATIONS_KEY] = existing_relations
        return node
    
    async def acall(
        self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any
    ) -> List[BaseNode]:
        """ç•°æ­¥æå–å¤šå€‹ç¯€é»çš„ä¸‰å…ƒçµ„"""
        jobs = []
        for node in nodes:
            jobs.append(self._aextract(node))
        
        return await run_jobs(
            jobs,
            workers=self.num_workers,
            show_progress=show_progress,
            desc="æ­£åœ¨æå–çŸ¥è­˜åœ–è­œ...",
        )


class GraphRAGSystem(EnhancedRAGSystem):
    """Graph RAG ç³»çµ± - åŸºæ–¼å±¬æ€§åœ–çš„æª¢ç´¢å¢å¼·ç”Ÿæˆ"""
    
    def __init__(self, use_chroma: bool = True):
        super().__init__(use_chroma)
        self.property_graph_index = None
        self.graph_store = SimpleGraphStore()
        self.kg_extractor = None
        self.graph_rag_retriever = None
        self.communities = {}
        self.community_summaries = {}
        
    def _ensure_models_initialized(self):
        """ç¢ºä¿æ¨¡å‹å·²åˆå§‹åŒ–"""
        if not self.models_initialized:
            self._setup_models()
            self.models_initialized = True
            
        # åˆå§‹åŒ–çŸ¥è­˜åœ–è­œæå–å™¨
        if self.kg_extractor is None:
            self.kg_extractor = GraphRAGExtractor(
                llm=self._get_llm(),
                max_paths_per_chunk=MAX_TRIPLETS_PER_CHUNK,
                num_workers=4
            )
    
    def _get_llm(self):
        """ç²å– LLM å¯¦ä¾‹"""
        from llama_index.core import Settings
        return Settings.llm
    
    def create_property_graph_index(self, documents: List[Document]) -> PropertyGraphIndex:
        """å»ºç«‹å±¬æ€§åœ–ç´¢å¼•"""
        with st.spinner("æ­£åœ¨å»ºç«‹çŸ¥è­˜åœ–è­œç´¢å¼•..."):
            self._ensure_models_initialized()
            
            try:
                # å»ºç«‹å„²å­˜ä¸Šä¸‹æ–‡
                storage_context = StorageContext.from_defaults(
                    graph_store=self.graph_store
                )
                
                # å»ºç«‹å±¬æ€§åœ–ç´¢å¼•
                self.property_graph_index = PropertyGraphIndex.from_documents(
                    documents,
                    kg_extractors=[self.kg_extractor],
                    storage_context=storage_context,
                    show_progress=True,
                    embed_kg_nodes=True,  # å•Ÿç”¨ç¯€é»åµŒå…¥ä»¥æ”¯æ´æ··åˆæª¢ç´¢
                )
                
                # æŒä¹…åŒ–
                self.property_graph_index.storage_context.persist(persist_dir=INDEX_DIR)
                
                st.success("âœ… çŸ¥è­˜åœ–è­œç´¢å¼•å»ºç«‹æˆåŠŸ")
                return self.property_graph_index
                
            except Exception as e:
                st.error(f"çŸ¥è­˜åœ–è­œç´¢å¼•å»ºç«‹å¤±æ•—: {str(e)}")
                # å›é€€åˆ°å‚³çµ±ç´¢å¼•
                st.warning("æ­£åœ¨å›é€€åˆ°å‚³çµ± RAG æ¨¡å¼...")
                return super().create_index(documents)
    
    def setup_graph_rag_retriever(self):
        """è¨­ç½® Graph RAG æª¢ç´¢å™¨"""
        if self.property_graph_index:
            try:
                self.graph_rag_retriever = KnowledgeGraphRAGRetriever(
                    storage_context=self.property_graph_index.storage_context,
                    verbose=True,
                    with_nl2graphquery=True,  # å•Ÿç”¨è‡ªç„¶èªè¨€è½‰åœ–æŸ¥è©¢
                )
                
                self.query_engine = RetrieverQueryEngine.from_args(
                    self.graph_rag_retriever,
                )
                
                st.success("âœ… Graph RAG æª¢ç´¢å™¨è¨­ç½®æˆåŠŸ")
                
            except Exception as e:
                st.warning(f"Graph RAG æª¢ç´¢å™¨è¨­ç½®å¤±æ•—: {str(e)}")
                # å›é€€åˆ°å‚³çµ±æŸ¥è©¢å¼•æ“
                super().setup_query_engine()
    
    def create_index(self, documents: List[Document]):
        """è¦†å¯«ç´¢å¼•å»ºç«‹æ–¹æ³•ä»¥æ”¯æ´ Graph RAG"""
        if ENABLE_GRAPH_RAG:
            index = self.create_property_graph_index(documents)
            self.setup_graph_rag_retriever()
            
            # å»ºç«‹ç¤¾ç¾¤
            if self.property_graph_index:
                self._build_communities()
            
            return index
        else:
            # ä½¿ç”¨å‚³çµ± RAG
            return super().create_index(documents)
    
    def query_with_graph_context(self, question: str) -> str:
        """åŸºæ–¼åœ–çš„ä¸Šä¸‹æ–‡æŸ¥è©¢"""
        if not self.query_engine:
            return "ç³»çµ±å°šæœªåˆå§‹åŒ–ï¼Œè«‹å…ˆè¼‰å…¥æ–‡ä»¶ã€‚"
        
        try:
            # å»ºæ§‹åŒ…å«æ­·å²å°è©±å’Œåœ–ä¸Šä¸‹æ–‡çš„æŸ¥è©¢
            context_prompt = self.memory.get_context_prompt()
            
            if context_prompt and self.memory.is_enabled():
                enhanced_question = f"""
{context_prompt}

ç•¶å‰å•é¡Œ: {question}

è«‹åŸºæ–¼çŸ¥è­˜åœ–è­œã€å°è©±æ­·å²å’Œæ–‡æª”å…§å®¹å›ç­”ç•¶å‰å•é¡Œã€‚
å¦‚æœèƒ½å¾çŸ¥è­˜åœ–è­œä¸­æ‰¾åˆ°ç›¸é—œçš„å¯¦é«”å’Œé—œä¿‚ï¼Œè«‹å„ªå…ˆä½¿ç”¨é€™äº›ä¿¡æ¯ã€‚
"""
            else:
                enhanced_question = question
            
            with st.spinner("æ­£åœ¨åˆ†æçŸ¥è­˜åœ–è­œ..."):
                response = self.query_engine.query(enhanced_question)
                response_str = str(response)
                
                # å°‡é€™è¼ªå°è©±åŠ å…¥è¨˜æ†¶
                self.memory.add_exchange(question, response_str)
                
                return response_str
                
        except Exception as e:
            st.error(f"Graph RAG æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            # å›é€€åˆ°å‚³çµ±æŸ¥è©¢
            return super().query_with_context(question)
    
    def _build_communities(self):
        """å»ºç«‹åœ–ç¤¾ç¾¤"""
        try:
            if not self.property_graph_index:
                return
            
            with st.spinner("æ­£åœ¨åˆ†æçŸ¥è­˜åœ–è­œç¤¾ç¾¤..."):
                # è½‰æ›ç‚º NetworkX åœ–
                nx_graph = self._create_networkx_graph()
                
                if len(nx_graph.nodes()) == 0:
                    st.warning("çŸ¥è­˜åœ–è­œä¸­æ²’æœ‰è¶³å¤ çš„ç¯€é»å»ºç«‹ç¤¾ç¾¤")
                    return
                
                # ä½¿ç”¨ NetworkX çš„ç¤¾ç¾¤æª¢æ¸¬
                try:
                    import community as community_louvain
                    partition = community_louvain.best_partition(nx_graph.to_undirected())
                    
                    # çµ„ç¹”ç¤¾ç¾¤è³‡è¨Š
                    communities = {}
                    for node, comm_id in partition.items():
                        if comm_id not in communities:
                            communities[comm_id] = []
                        communities[comm_id].append(node)
                    
                    self.communities = communities
                    
                    # ç”Ÿæˆç¤¾ç¾¤æ‘˜è¦
                    self._generate_community_summaries(nx_graph)
                    
                    st.success(f"âœ… è­˜åˆ¥å‡º {len(communities)} å€‹çŸ¥è­˜ç¤¾ç¾¤")
                    
                except ImportError:
                    # å¦‚æœæ²’æœ‰ python-louvainï¼Œä½¿ç”¨ç°¡å–®çš„é€£é€šåˆ†é‡
                    components = list(nx.connected_components(nx_graph.to_undirected()))
                    communities = {i: list(comp) for i, comp in enumerate(components)}
                    self.communities = communities
                    st.info(f"è­˜åˆ¥å‡º {len(communities)} å€‹çŸ¥è­˜ç¾¤çµ„")
                    
        except Exception as e:
            st.warning(f"ç¤¾ç¾¤å»ºç«‹å¤±æ•—: {str(e)}")
    
    def _create_networkx_graph(self) -> nx.Graph:
        """å¾å±¬æ€§åœ–å»ºç«‹ NetworkX åœ–"""
        nx_graph = nx.Graph()
        
        try:
            # è¨ºæ–·æª¢æŸ¥
            if not self.property_graph_index:
                st.warning("ğŸš¨ property_graph_index æœªåˆå§‹åŒ–")
                return nx_graph
            
            # æª¢æŸ¥åœ–å­˜å„²
            if not hasattr(self.property_graph_index, 'property_graph_store'):
                st.warning("ğŸš¨ æ‰¾ä¸åˆ° property_graph_store")
                
                # æª¢æŸ¥æ˜¯å¦æœ‰å…¶ä»–æ–¹å¼ç²å–åœ–æ•¸æ“š
                if hasattr(self.property_graph_index, '_graph_store'):
                    st.info("æ‰¾åˆ° _graph_store å±¬æ€§ï¼Œå˜—è©¦ä½¿ç”¨...")
                    graph_store = self.property_graph_index._graph_store
                elif hasattr(self.property_graph_index, 'storage_context') and hasattr(self.property_graph_index.storage_context, 'graph_store'):
                    st.info("å¾ storage_context ç²å– graph_store...")
                    graph_store = self.property_graph_index.storage_context.graph_store
                else:
                    st.info("å˜—è©¦æ›¿ä»£æ–¹æ³• - å¾ç´¢å¼•ä¸­ç²å–ç¯€é»æ•¸æ“š")
                    graph_store = None
                
                # å¦‚æœæ‰¾åˆ°äº†æ›¿ä»£çš„ graph_storeï¼Œè·³éå‚™ç”¨æ–¹æ³•
                if graph_store:
                    # ä½¿ç”¨æ‰¾åˆ°çš„ graph_store ç¹¼çºŒæ­£å¸¸æµç¨‹
                    st.info(f"âœ… ä½¿ç”¨æ›¿ä»£ graph_store: {type(graph_store).__name__}")
                    # è·³è½‰åˆ°çµ±ä¸€çš„åœ–å­˜å„²è™•ç†é‚è¼¯
                    return self._process_graph_store(graph_store, nx_graph)
                else:
                    # ä½¿ç”¨å‚™ç”¨æ–¹æ³•å¾ docstore æå–æ•¸æ“š
                    try:
                        # å˜—è©¦å¾å‘é‡ç´¢å¼•ç²å–æ–‡æª”ç¯€é»
                        if hasattr(self.property_graph_index, 'docstore'):
                            docs = self.property_graph_index.docstore.docs
                            st.info(f"ğŸ“„ å¾ docstore æ‰¾åˆ° {len(docs)} å€‹æ–‡æª”")
                            
                            # å¾æ–‡æª”å…ƒæ•¸æ“šä¸­æå–å¯¦é«”å’Œé—œä¿‚
                            total_nodes = 0
                            total_relations = 0
                            
                            for doc_id, doc in docs.items():
                                # æª¢æŸ¥ç¯€é»æ˜¯å¦æœ‰çŸ¥è­˜åœ–è­œæ•¸æ“š
                                if hasattr(doc, 'metadata'):
                                    nodes = doc.metadata.get(KG_NODES_KEY, [])
                                    relations = doc.metadata.get(KG_RELATIONS_KEY, [])
                                    
                                    st.info(f"æ–‡æª” {doc_id}: æ‰¾åˆ° {len(nodes)} å€‹ç¯€é», {len(relations)} å€‹é—œä¿‚")
                                    total_nodes += len(nodes)
                                    total_relations += len(relations)
                                    
                                    # æ·»åŠ å¯¦é«”ç¯€é»
                                    for node in nodes:
                                        if hasattr(node, 'name'):
                                            nx_graph.add_node(node.name, 
                                                            label=getattr(node, 'label', 'Entity'),
                                                            **getattr(node, 'properties', {}))
                                        else:
                                            st.warning(f"ç¯€é»ç¼ºå°‘åç¨±å±¬æ€§: {type(node)}")
                                    
                                    # æ·»åŠ é—œä¿‚é‚Š
                                    for rel in relations:
                                        if hasattr(rel, 'source_id') and hasattr(rel, 'target_id'):
                                            nx_graph.add_edge(
                                                rel.source_id,
                                                rel.target_id,
                                                relationship=getattr(rel, 'label', 'related'),
                                                **getattr(rel, 'properties', {})
                                            )
                                        else:
                                            st.warning(f"é—œä¿‚ç¼ºå°‘ source_id æˆ– target_id: {type(rel)}")
                            
                            st.info(f"ğŸ“Š ç¸½è¨ˆç™¼ç¾: {total_nodes} å€‹ç¯€é», {total_relations} å€‹é—œä¿‚")
                            st.info(f"ğŸ“Š å¯¦éš›æ·»åŠ : {len(nx_graph.nodes())} å€‹ç¯€é»ï¼Œ{len(nx_graph.edges())} å€‹é—œä¿‚")
                        
                    except Exception as fallback_e:
                        st.warning(f"æ›¿ä»£æ–¹æ³•ä¹Ÿå¤±æ•—: {str(fallback_e)}")
                    
                    return nx_graph
            else:
                # æ­£å¸¸è·¯å¾‘ï¼šæœ‰ property_graph_store
                graph_store = self.property_graph_index.property_graph_store
                st.info(f"âœ… ä½¿ç”¨æ¨™æº– property_graph_store: {type(graph_store).__name__}")
                # ä½¿ç”¨çµ±ä¸€çš„åœ–å­˜å„²è™•ç†é‚è¼¯
                return self._process_graph_store(graph_store, nx_graph)
                
        except Exception as e:
            st.error(f"NetworkX åœ–å»ºç«‹å¤±æ•—: {str(e)}")
            import traceback
            st.error(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
        
        return nx_graph
    
    def _process_graph_store(self, graph_store, nx_graph: nx.Graph) -> nx.Graph:
        """çµ±ä¸€è™•ç†åœ–å­˜å„²çš„ç¯€é»å’Œé‚Š"""
        try:
            # æ·»åŠ ç¯€é»
            nodes_added = 0
            if hasattr(graph_store, 'get_all_nodes'):
                try:
                    all_nodes = graph_store.get_all_nodes()
                    for node in all_nodes:
                        nx_graph.add_node(node.name, **node.properties)
                        nodes_added += 1
                    st.info(f"âœ… æˆåŠŸæ·»åŠ  {nodes_added} å€‹ç¯€é»")
                except Exception as nodes_e:
                    st.warning(f"æ·»åŠ ç¯€é»å¤±æ•—: {str(nodes_e)}")
            else:
                st.warning("ğŸš¨ graph_store æ²’æœ‰ get_all_nodes æ–¹æ³•")
            
            # æ·»åŠ é‚Š
            edges_added = 0
            if hasattr(graph_store, 'get_all_relationships'):
                try:
                    all_relationships = graph_store.get_all_relationships()
                    for rel in all_relationships:
                        nx_graph.add_edge(
                            rel.source_id, 
                            rel.target_id,
                            relationship=rel.label,
                            **rel.properties
                        )
                        edges_added += 1
                    st.info(f"âœ… æˆåŠŸæ·»åŠ  {edges_added} å€‹é—œä¿‚")
                except Exception as edges_e:
                    st.warning(f"æ·»åŠ é‚Šå¤±æ•—: {str(edges_e)}")
            else:
                st.warning("ğŸš¨ graph_store æ²’æœ‰ get_all_relationships æ–¹æ³•")
                        
        except Exception as e:
            st.error(f"åœ–å­˜å„²è™•ç†å¤±æ•—: {str(e)}")
            import traceback
            st.error(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
        
        return nx_graph
    
    def _generate_community_summaries(self, nx_graph: nx.Graph):
        """ç”Ÿæˆç¤¾ç¾¤æ‘˜è¦"""
        try:
            llm = self._get_llm()
            
            for comm_id, nodes in self.communities.items():
                if len(nodes) < 2:
                    continue
                
                # æ”¶é›†ç¤¾ç¾¤å…§çš„é—œä¿‚è³‡è¨Š
                relationships = []
                for node in nodes:
                    for neighbor in nx_graph.neighbors(node):
                        if neighbor in nodes:
                            edge_data = nx_graph.get_edge_data(node, neighbor)
                            if edge_data:
                                relationship = edge_data.get('relationship', 'ç›¸é—œ')
                                relationships.append(f"{node} -> {neighbor} -> {relationship}")
                
                if relationships:
                    relationships_text = "\n".join(relationships[:10])  # é™åˆ¶é•·åº¦
                    
                    # ç”Ÿæˆæ‘˜è¦æç¤º
                    summary_prompt = f"""
è«‹ç‚ºä»¥ä¸‹çŸ¥è­˜åœ–è­œç¤¾ç¾¤ç”Ÿæˆä¸€å€‹ç°¡æ½”çš„æ‘˜è¦ï¼š

é—œä¿‚è³‡è¨Šï¼š
{relationships_text}

è«‹ç¸½çµé€™å€‹ç¤¾ç¾¤çš„ä¸»è¦ä¸»é¡Œå’Œæ ¸å¿ƒæ¦‚å¿µï¼Œä¸è¶…é100å­—ã€‚
"""
                    try:
                        summary = llm.complete(summary_prompt).text.strip()
                        self.community_summaries[comm_id] = summary
                    except Exception:
                        self.community_summaries[comm_id] = f"åŒ…å« {len(nodes)} å€‹ç›¸é—œæ¦‚å¿µçš„çŸ¥è­˜ç¾¤çµ„"
                        
        except Exception as e:
            st.warning(f"ç¤¾ç¾¤æ‘˜è¦ç”Ÿæˆå¤±æ•—: {str(e)}")
    
    def visualize_knowledge_graph(self, max_nodes: int = 100) -> str:
        """å¯è¦–åŒ–çŸ¥è­˜åœ–è­œ"""
        try:
            # è¨ºæ–·æª¢æŸ¥
            if not self.property_graph_index:
                st.error("âŒ çŸ¥è­˜åœ–è­œç´¢å¼•æœªåˆå§‹åŒ–ã€‚è«‹å…ˆä¸Šå‚³æ–‡æª”ä¸¦å»ºç«‹åœ–è­œã€‚")
                return None
            
            st.info("ğŸ” æ­£åœ¨æª¢æŸ¥çŸ¥è­˜åœ–è­œæ•¸æ“š...")
            nx_graph = self._create_networkx_graph()
            
            # è©³ç´°çš„è¨ºæ–·ä¿¡æ¯
            nodes_count = len(nx_graph.nodes())
            edges_count = len(nx_graph.edges())
            
            st.info(f"ğŸ“Š ç™¼ç¾ {nodes_count} å€‹ç¯€é»ï¼Œ{edges_count} å€‹é—œä¿‚")
            
            if nodes_count == 0:
                st.warning("âš ï¸ çŸ¥è­˜åœ–è­œä¸­æ²’æœ‰ç¯€é»æ•¸æ“šã€‚å¯èƒ½çš„åŸå› ï¼š")
                st.markdown("""
                - æ–‡æª”æ²’æœ‰æˆåŠŸæå–å¯¦é«”
                - Graph RAG è™•ç†éç¨‹ä¸­å‡ºç¾éŒ¯èª¤
                - éœ€è¦é‡æ–°è™•ç†æ–‡æª”
                """)
                return None
            
            # é™åˆ¶ç¯€é»æ•¸é‡ä»¥é¿å…éæ–¼è¤‡é›œ
            if len(nx_graph.nodes()) > max_nodes:
                # é¸æ“‡åº¦æ•¸æœ€é«˜çš„ç¯€é»
                degree_dict = dict(nx_graph.degree())
                top_nodes = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)[:max_nodes]
                selected_nodes = [node for node, _ in top_nodes]
                nx_graph = nx_graph.subgraph(selected_nodes)
            
            # å‰µå»º Pyvis ç¶²çµ¡
            net = Network(
                height="600px", 
                width="100%", 
                bgcolor="#ffffff",
                font_color="#333333"
            )
            
            # é…ç½®ç‰©ç†æ•ˆæœ
            net.set_options("""
            var options = {
              "physics": {
                "enabled": true,
                "stabilization": {"iterations": 100}
              },
              "nodes": {
                "borderWidth": 2,
                "borderWidthSelected": 3,
                "font": {"size": 14}
              },
              "edges": {
                "font": {"size": 12},
                "smooth": {"type": "continuous"}
              }
            }
            """)
            
            # ç¤¾ç¾¤è‘—è‰²
            colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', 
                     '#DDA0DD', '#98D8C8', '#F7DC6F', '#BB8FCE', '#85C1E9']
            
            # æ·»åŠ ç¯€é»
            for node in nx_graph.nodes():
                # ç¢ºå®šç¯€é»æ‰€å±¬ç¤¾ç¾¤
                node_color = '#667eea'  # é»˜èªé¡è‰²
                for comm_id, comm_nodes in self.communities.items():
                    if node in comm_nodes:
                        node_color = colors[comm_id % len(colors)]
                        break
                
                # ç¯€é»å¤§å°åŸºæ–¼åº¦æ•¸
                degree = nx_graph.degree(node)
                size = min(max(degree * 5, 10), 30)
                
                net.add_node(
                    node, 
                    label=str(node)[:20],  # é™åˆ¶æ¨™ç±¤é•·åº¦
                    color=node_color,
                    size=size,
                    title=f"ç¯€é»: {node}\nåº¦æ•¸: {degree}"
                )
            
            # æ·»åŠ é‚Š
            for edge in nx_graph.edges():
                source, target = edge
                edge_data = nx_graph.get_edge_data(source, target)
                relationship = edge_data.get('relationship', 'ç›¸é—œ') if edge_data else 'ç›¸é—œ'
                
                net.add_edge(
                    source, 
                    target,
                    label=relationship[:10],  # é™åˆ¶æ¨™ç±¤é•·åº¦
                    title=f"é—œä¿‚: {relationship}",
                    color={'color': '#848484'}
                )
            
            # ç”Ÿæˆ HTML
            with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False) as tmp_file:
                net.save_graph(tmp_file.name)
                return tmp_file.name
                
        except Exception as e:
            st.error(f"çŸ¥è­˜åœ–è­œå¯è¦–åŒ–å¤±æ•—: {str(e)}")
            return None
    
    def get_graph_statistics(self) -> Dict[str, Any]:
        """ç²å–åœ–çµ±è¨ˆè³‡è¨Š"""
        stats = super().get_document_statistics()
        
        if self.property_graph_index:
            nx_graph = self._create_networkx_graph()
            
            graph_stats = {
                "nodes_count": len(nx_graph.nodes()),
                "edges_count": len(nx_graph.edges()),
                "communities_count": len(self.communities),
                "average_degree": sum(dict(nx_graph.degree()).values()) / len(nx_graph.nodes()) if nx_graph.nodes() else 0,
                "density": nx.density(nx_graph),
                "communities": self.communities,
                "community_summaries": self.community_summaries
            }
            
            stats["graph_stats"] = graph_stats
        
        return stats
    
    def get_related_entities(self, entity_name: str, max_results: int = 10) -> List[Dict]:
        """ç²å–èˆ‡æŒ‡å®šå¯¦é«”ç›¸é—œçš„å¯¦é«”"""
        related = []
        
        try:
            nx_graph = self._create_networkx_graph()
            
            if entity_name in nx_graph.nodes():
                # ç²å–ç›´æ¥ç›¸é€£çš„é„°å±…
                neighbors = list(nx_graph.neighbors(entity_name))
                
                for neighbor in neighbors[:max_results]:
                    edge_data = nx_graph.get_edge_data(entity_name, neighbor)
                    relationship = edge_data.get('relationship', 'ç›¸é—œ') if edge_data else 'ç›¸é—œ'
                    
                    related.append({
                        'entity': neighbor,
                        'relationship': relationship,
                        'type': 'direct'
                    })
                
                # å¦‚æœç›´æ¥é„°å±…ä¸è¶³ï¼Œæ·»åŠ äºŒåº¦é„°å±…
                if len(related) < max_results:
                    for neighbor in neighbors:
                        second_neighbors = list(nx_graph.neighbors(neighbor))
                        for second_neighbor in second_neighbors:
                            if second_neighbor != entity_name and second_neighbor not in [r['entity'] for r in related]:
                                if len(related) >= max_results:
                                    break
                                    
                                related.append({
                                    'entity': second_neighbor,
                                    'relationship': f"é€é {neighbor}",
                                    'type': 'indirect'
                                })
                        
                        if len(related) >= max_results:
                            break
                            
        except Exception as e:
            st.warning(f"ç²å–ç›¸é—œå¯¦é«”å¤±æ•—: {str(e)}")
        
        return related
