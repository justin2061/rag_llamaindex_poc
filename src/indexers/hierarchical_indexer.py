"""
Hierarchical Indexer System
éšå±¤å¼ç´¢å¼•ç³»çµ±ï¼Œæ”¯æ´å¤šç²’åº¦æ–‡æª”ç´¢å¼•
"""

import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime
from elasticsearch import Elasticsearch
from llama_index.core import Document
from src.processors.enhanced_document_processor import EnhancedDocumentProcessor

# é…ç½®logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class IndexingStrategy:
    """ç´¢å¼•ç­–ç•¥é…ç½®"""
    strategy_name: str
    chunk_size: int
    chunk_overlap: int
    embedding_models: List[str]
    enable_bm25: bool = True
    enable_structural_indexing: bool = True

class HierarchicalIndexer:
    """éšå±¤å¼ç´¢å¼•å™¨"""
    
    def __init__(
        self,
        elasticsearch_client: Elasticsearch,
        index_name: str,
        embedding_models: Dict[str, Any] = None,
        processor: EnhancedDocumentProcessor = None
    ):
        self.es_client = elasticsearch_client
        self.index_name = index_name
        self.embedding_models = embedding_models or {}
        self.processor = processor or EnhancedDocumentProcessor()
        
        # é è¨­ç´¢å¼•ç­–ç•¥
        self.indexing_strategies = [
            IndexingStrategy(
                strategy_name="precise",
                chunk_size=512,
                chunk_overlap=100,
                embedding_models=["general", "sentence"],
                enable_bm25=True,
                enable_structural_indexing=True
            ),
            IndexingStrategy(
                strategy_name="contextual",
                chunk_size=1024,
                chunk_overlap=200,
                embedding_models=["general", "domain"],
                enable_bm25=True,
                enable_structural_indexing=True
            ),
            IndexingStrategy(
                strategy_name="comprehensive",
                chunk_size=2048,
                chunk_overlap=400,
                embedding_models=["general"],
                enable_bm25=True,
                enable_structural_indexing=False  # é•·æ–‡æœ¬ä¸éœ€è¦çµæ§‹ç´¢å¼•
            )
        ]
        
        logger.info(f"ğŸ—ï¸ HierarchicalIndexer åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   - ç´¢å¼•åç¨±: {index_name}")
        logger.info(f"   - ç´¢å¼•ç­–ç•¥: {len(self.indexing_strategies)} ç¨®")
    
    def create_hierarchical_index(self, documents: List[Document]) -> Dict[str, Any]:
        """
        å‰µå»ºéšå±¤å¼ç´¢å¼•
        """
        logger.info(f"ğŸš€ é–‹å§‹å‰µå»ºéšå±¤å¼ç´¢å¼•ï¼Œæ–‡æª”æ•¸é‡: {len(documents)}")
        
        indexing_stats = {
            "total_documents": len(documents),
            "indexed_chunks": 0,
            "indexing_strategies": {},
            "start_time": datetime.now()
        }
        
        for document in documents:
            doc_stats = self._index_single_document(document)
            indexing_stats["indexed_chunks"] += doc_stats["chunks_created"]
            
            # æ›´æ–°ç­–ç•¥çµ±è¨ˆ
            for strategy, count in doc_stats["strategy_stats"].items():
                if strategy not in indexing_stats["indexing_strategies"]:
                    indexing_stats["indexing_strategies"][strategy] = 0
                indexing_stats["indexing_strategies"][strategy] += count
        
        indexing_stats["end_time"] = datetime.now()
        indexing_stats["duration"] = (indexing_stats["end_time"] - indexing_stats["start_time"]).total_seconds()
        
        logger.info(f"âœ… éšå±¤å¼ç´¢å¼•å‰µå»ºå®Œæˆ")
        logger.info(f"   - ç¸½æ–‡æª”æ•¸: {indexing_stats['total_documents']}")
        logger.info(f"   - ç¸½chunksæ•¸: {indexing_stats['indexed_chunks']}")
        logger.info(f"   - è€—æ™‚: {indexing_stats['duration']:.2f}ç§’")
        
        return indexing_stats
    
    def _index_single_document(self, document: Document) -> Dict[str, Any]:
        """ç´¢å¼•å–®å€‹æ–‡æª”"""
        doc_source = document.metadata.get('source', 'unknown')
        logger.info(f"ğŸ“„ é–‹å§‹ç´¢å¼•æ–‡æª”: {doc_source}")
        
        doc_stats = {
            "chunks_created": 0,
            "strategy_stats": {}
        }
        
        try:
            # 1. ä½¿ç”¨å¢å¼·è™•ç†å™¨è™•ç†æ–‡æª”
            processed_documents = self.processor.process_document(document)
            
            # 2. æŒ‰ä¸åŒç­–ç•¥å‰µå»ºç´¢å¼•
            for strategy in self.indexing_strategies:
                strategy_chunks = self._filter_chunks_by_strategy(processed_documents, strategy)
                
                for chunk_doc in strategy_chunks:
                    # ç”Ÿæˆå¤šç¨®embeddings
                    embeddings = self._generate_multiple_embeddings(chunk_doc.text, strategy)
                    
                    # å‰µå»ºç´¢å¼•æ–‡æª”
                    index_doc = self._create_index_document(chunk_doc, embeddings, strategy)
                    
                    # ç´¢å¼•åˆ°Elasticsearch
                    self._index_to_elasticsearch(index_doc)
                    
                    doc_stats["chunks_created"] += 1
                
                strategy_name = strategy.strategy_name
                doc_stats["strategy_stats"][strategy_name] = len(strategy_chunks)
                logger.info(f"   - {strategy_name} ç­–ç•¥: {len(strategy_chunks)} chunks")
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æª”ç´¢å¼•å¤±æ•— {doc_source}: {e}")
        
        return doc_stats
    
    def _filter_chunks_by_strategy(self, documents: List[Document], strategy: IndexingStrategy) -> List[Document]:
        """æ ¹æ“šç­–ç•¥éæ¿¾chunks"""
        filtered = []
        
        for doc in documents:
            chunking_info = doc.metadata.get('chunking_strategy', {})
            chunk_size = chunking_info.get('chunk_size', 0)
            
            # æ ¹æ“šç­–ç•¥çš„chunk_sizeç¯„åœéæ¿¾
            size_range = self._get_size_range_for_strategy(strategy)
            if size_range[0] <= chunk_size <= size_range[1]:
                filtered.append(doc)
        
        return filtered
    
    def _get_size_range_for_strategy(self, strategy: IndexingStrategy) -> tuple:
        """ç²å–ç­–ç•¥çš„å¤§å°ç¯„åœ"""
        base_size = strategy.chunk_size
        return (base_size - 100, base_size + 100)  # å…è¨±Â±100çš„ç¯„åœ
    
    def _generate_multiple_embeddings(self, text: str, strategy: IndexingStrategy) -> Dict[str, List[float]]:
        """ç”Ÿæˆå¤šç¨®embedding"""
        embeddings = {}
        
        try:
            from llama_index.core import Settings
            
            # ä½¿ç”¨é è¨­embeddingæ¨¡å‹
            if "general" in strategy.embedding_models:
                general_embedding = Settings.embed_model.get_text_embedding(text)
                embeddings["general"] = general_embedding
            
            # å¦‚æœé…ç½®äº†å…¶ä»–embeddingæ¨¡å‹ï¼Œé€™è£¡å¯ä»¥èª¿ç”¨
            # ç‚ºäº†ç°¡åŒ–ï¼Œæš«æ™‚éƒ½ä½¿ç”¨åŒä¸€å€‹æ¨¡å‹
            for model_name in strategy.embedding_models:
                if model_name != "general" and model_name not in embeddings:
                    # é€™è£¡å¯ä»¥æ ¹æ“šmodel_nameèª¿ç”¨ä¸åŒçš„embeddingæ¨¡å‹
                    embeddings[model_name] = Settings.embed_model.get_text_embedding(text)
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç”Ÿæˆembeddingå¤±æ•—: {e}")
            # æä¾›å¾Œå‚™æ–¹æ¡ˆ
            embeddings["general"] = [0.0] * 512  # é›¶å‘é‡ä½œç‚ºå¾Œå‚™
        
        return embeddings
    
    def _create_index_document(self, chunk_doc: Document, embeddings: Dict[str, List[float]], strategy: IndexingStrategy) -> Dict[str, Any]:
        """å‰µå»ºç´¢å¼•æ–‡æª”"""
        
        # åŸºæœ¬å…§å®¹
        index_doc = {
            "content": chunk_doc.text,
            "metadata": chunk_doc.metadata,
        }
        
        # æ·»åŠ ä¸»è¦embeddingï¼ˆç”¨æ–¼å‘é‡æœç´¢ï¼‰
        if "general" in embeddings:
            index_doc["embedding"] = embeddings["general"]
        
        # æ·»åŠ å¤šç¨®embeddingsï¼ˆç”¨æ–¼å¤šæ¨¡å‹æœç´¢ï¼‰
        if len(embeddings) > 1:
            index_doc["embeddings"] = embeddings
        
        # æ·»åŠ BM25å…§å®¹ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if strategy.enable_bm25:
            index_doc["bm25_content"] = chunk_doc.text
        
        # æ·»åŠ æ¨™é¡Œå’Œæ‘˜è¦ï¼ˆå¦‚æœå¯ä»¥æå–ï¼‰
        title, summary = self._extract_title_and_summary(chunk_doc)
        if title:
            index_doc["title"] = title
        if summary:
            index_doc["summary"] = summary
        
        # æ·»åŠ ç­–ç•¥ä¿¡æ¯
        index_doc["indexing_strategy"] = {
            "strategy_name": strategy.strategy_name,
            "chunk_size": strategy.chunk_size,
            "chunk_overlap": strategy.chunk_overlap,
            "embedding_models": strategy.embedding_models,
            "indexed_at": datetime.now().isoformat()
        }
        
        # æ·»åŠ æœç´¢å…ƒæ•¸æ“š
        index_doc["search_metadata"] = {
            "content_length": len(chunk_doc.text),
            "word_count": len(chunk_doc.text.split()),
            "has_title": bool(title),
            "has_summary": bool(summary),
            "structure_type": chunk_doc.metadata.get('document_structure', {}).get('content_type', 'unknown')
        }
        
        return index_doc
    
    def _extract_title_and_summary(self, chunk_doc: Document) -> tuple:
        """æå–æ¨™é¡Œå’Œæ‘˜è¦"""
        text = chunk_doc.text
        title = None
        summary = None
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºæ¨™é¡Œé¡å‹
        structure = chunk_doc.metadata.get('document_structure', {})
        if structure.get('content_type') == 'title':
            title = text[:100]  # æ¨™é¡Œé™åˆ¶100å­—ç¬¦
        
        # å¦‚æœæ–‡æœ¬è¼ƒé•·ï¼Œç”Ÿæˆæ‘˜è¦ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        if len(text) > 300:
            sentences = text.split('ã€‚')
            if len(sentences) > 2:
                summary = 'ã€‚'.join(sentences[:2]) + 'ã€‚'  # å–å‰å…©å¥ä½œç‚ºæ‘˜è¦
        
        return title, summary
    
    def _index_to_elasticsearch(self, index_doc: Dict[str, Any]):
        """ç´¢å¼•åˆ°Elasticsearch"""
        try:
            # ç”Ÿæˆæ–‡æª”ID
            import hashlib
            content_hash = hashlib.md5(index_doc["content"].encode()).hexdigest()
            doc_id = f"{content_hash}_{index_doc['indexing_strategy']['strategy_name']}"
            
            # ç´¢å¼•æ–‡æª”
            response = self.es_client.index(
                index=self.index_name,
                id=doc_id,
                body=index_doc
            )
            
            if response.get('result') in ['created', 'updated']:
                logger.debug(f"âœ… æ–‡æª”å·²ç´¢å¼•: {doc_id}")
            else:
                logger.warning(f"âš ï¸ æ–‡æª”ç´¢å¼•ç•°å¸¸: {response}")
                
        except Exception as e:
            logger.error(f"âŒ Elasticsearchç´¢å¼•å¤±æ•—: {e}")
    
    def update_index_mapping(self):
        """æ›´æ–°ç´¢å¼•æ˜ å°„"""
        try:
            from config.enhanced_elasticsearch_mapping import get_hybrid_search_mapping
            
            # æª¢æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            if not self.es_client.indices.exists(index=self.index_name):
                logger.info(f"ğŸ“‹ å‰µå»ºæ–°ç´¢å¼•: {self.index_name}")
                mapping = get_hybrid_search_mapping()
                self.es_client.indices.create(index=self.index_name, body=mapping)
            else:
                logger.info(f"ğŸ“‹ ç´¢å¼•å·²å­˜åœ¨: {self.index_name}")
                # å¯ä»¥åœ¨é€™è£¡æ·»åŠ æ˜ å°„æ›´æ–°é‚è¼¯
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ç´¢å¼•æ˜ å°„å¤±æ•—: {e}")
    
    def get_index_statistics(self) -> Dict[str, Any]:
        """ç²å–ç´¢å¼•çµ±è¨ˆä¿¡æ¯"""
        try:
            # ç²å–åŸºæœ¬çµ±è¨ˆ
            stats = self.es_client.indices.stats(index=self.index_name)
            
            # ç²å–ç­–ç•¥åˆ†å¸ƒçµ±è¨ˆ
            strategy_agg = {
                "aggs": {
                    "strategy_distribution": {
                        "terms": {
                            "field": "indexing_strategy.strategy_name.keyword",
                            "size": 10
                        }
                    },
                    "content_type_distribution": {
                        "terms": {
                            "field": "metadata.document_structure.content_type.keyword",
                            "size": 10
                        }
                    }
                },
                "size": 0
            }
            
            agg_result = self.es_client.search(index=self.index_name, body=strategy_agg)
            
            return {
                "total_documents": stats['indices'][self.index_name]['total']['docs']['count'],
                "index_size": stats['indices'][self.index_name]['total']['store']['size_in_bytes'],
                "strategy_distribution": agg_result['aggregations']['strategy_distribution']['buckets'],
                "content_type_distribution": agg_result['aggregations']['content_type_distribution']['buckets']
            }
            
        except Exception as e:
            logger.error(f"âŒ ç²å–ç´¢å¼•çµ±è¨ˆå¤±æ•—: {e}")
            return {}
    
    def optimize_index(self):
        """å„ªåŒ–ç´¢å¼•"""
        try:
            logger.info(f"ğŸ”§ é–‹å§‹å„ªåŒ–ç´¢å¼•: {self.index_name}")
            
            # å¼·åˆ¶åˆä½µæ®µ
            self.es_client.indices.forcemerge(
                index=self.index_name,
                max_num_segments=1
            )
            
            # åˆ·æ–°ç´¢å¼•
            self.es_client.indices.refresh(index=self.index_name)
            
            logger.info(f"âœ… ç´¢å¼•å„ªåŒ–å®Œæˆ: {self.index_name}")
            
        except Exception as e:
            logger.error(f"âŒ ç´¢å¼•å„ªåŒ–å¤±æ•—: {e}")