import os
import json
import time
from typing import List, Dict, Any, Optional
import streamlit as st
from datetime import datetime
import traceback

# LlamaIndex æ ¸å¿ƒ
from llama_index.core import VectorStoreIndex, Document, Settings
from llama_index.core.storage.storage_context import StorageContext
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.postprocessor import SimilarityPostprocessor

# æ¢ä»¶æ€§æŠ€è¡“è¨Šæ¯é¡¯ç¤º
def _show_technical_message(func, message, *args, **kwargs):
    """æ¢ä»¶æ€§é¡¯ç¤ºæŠ€è¡“è¨Šæ¯ï¼Œæ ¹æ“šé…ç½®æ±ºå®šæ˜¯å¦é¡¯ç¤º"""
    try:
        from config.config import SHOW_TECHNICAL_MESSAGES, DEBUG_MODE
        show_tech = (DEBUG_MODE or SHOW_TECHNICAL_MESSAGES or 
                    st.session_state.get('show_tech_messages', False))
        if show_tech:
            return func(message, *args, **kwargs)
        else:
            # åªè¨˜éŒ„åˆ°æ—¥èªŒï¼Œä¸é¡¯ç¤ºåœ¨UIä¸Š
            print(f"[TECH] {message}")
            return None
    except Exception:
        # å¦‚æžœç„¡æ³•ç²å–é…ç½®ï¼Œé è¨­ä¸é¡¯ç¤º
        print(f"[TECH] {message}")
        return None

# ä¾¿åˆ©å‡½æ•¸
def _tech_info(message): return _show_technical_message(st.info, message)
def _tech_success(message): return _show_technical_message(st.success, message)
def _tech_warning(message): return _show_technical_message(st.warning, message)
def _tech_error(message): return _show_technical_message(st.error, message)

# å°è©±è¨˜éŒ„ç®¡ç†
from src.storage.conversation_history import ConversationHistoryManager

# æ€§èƒ½è¿½è¹¤
from src.utils.performance_tracker import get_performance_tracker, track_rag_stage, RAGStages

# Elasticsearch integration
try:
    from elasticsearch import Elasticsearch
    ELASTICSEARCH_AVAILABLE = True
except ImportError:
    ELASTICSEARCH_AVAILABLE = False
    st.warning("âš ï¸ Elasticsearch dependencies not installed. Install with: pip install elasticsearch")

# ç¹¼æ‰¿å¢žå¼·ç‰ˆç³»çµ±
from .enhanced_rag_system import EnhancedRAGSystem
from config.config import (
    GROQ_API_KEY, EMBEDDING_MODEL, LLM_MODEL, 
    ELASTICSEARCH_HOST, ELASTICSEARCH_PORT, ELASTICSEARCH_SCHEME,
    ELASTICSEARCH_INDEX_NAME, ELASTICSEARCH_USERNAME, ELASTICSEARCH_PASSWORD,
    ELASTICSEARCH_TIMEOUT, ELASTICSEARCH_MAX_RETRIES, ELASTICSEARCH_VERIFY_CERTS,
    ELASTICSEARCH_SHARDS, ELASTICSEARCH_REPLICAS, ELASTICSEARCH_VECTOR_DIMENSION,
    ELASTICSEARCH_SIMILARITY, SHOW_TECHNICAL_MESSAGES, DEBUG_MODE
)

class ElasticsearchRAGSystem(EnhancedRAGSystem):
    """Elasticsearch RAG ç³»çµ± - é«˜æ•ˆèƒ½ã€å¯æ“´å±•çš„å‘é‡æª¢ç´¢"""
    
    def __init__(self, elasticsearch_config: Optional[Dict] = None):
        """åˆå§‹åŒ– Elasticsearch RAG ç³»çµ±"""
        # é¦–å…ˆè¨­ç½® elasticsearch_configï¼Œé¿å…åœ¨çˆ¶é¡žåˆå§‹åŒ–æ™‚å¼•ç”¨éŒ¯èª¤
        self.elasticsearch_config = elasticsearch_config or self._get_default_config()
        self.elasticsearch_client = None
        self.elasticsearch_store = None
        
        # ç³»çµ±ç‹€æ…‹å„²å­˜ï¼ˆç”¨æ–¼ Dashboard é¡¯ç¤ºï¼‰
        self.system_status = {}
        
        # è¨˜æ†¶é«”ä½¿ç”¨ç›£æŽ§
        self.memory_stats = {
            'documents_processed': 0,
            'vectors_stored': 0,
            'peak_memory_mb': 0
        }
        
        # æ¨¡åž‹å¯¦ä¾‹
        self.embedding_model = None
        self.llm_model = None
        
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ç´¢å¼•åç¨±
        from config.config import ELASTICSEARCH_INDEX_NAME
        self.index_name = ELASTICSEARCH_INDEX_NAME
        
        # åˆå§‹åŒ–å°è©±è¨˜éŒ„ç®¡ç†å™¨
        self.conversation_manager = ConversationHistoryManager(self.elasticsearch_config)
        
        # èª¿ç”¨çˆ¶é¡žåˆå§‹åŒ–ï¼Œä½†ç¦ç”¨å…¶ Elasticsearch è‡ªå‹•åˆå§‹åŒ–
        super().__init__(use_elasticsearch=False, use_chroma=False)  # å…ˆè¨­ç½®ç‚º False
        
        # ç„¶å¾Œæ‰‹å‹•è¨­ç½®æ¨™èªŒä¸¦åˆå§‹åŒ– Elasticsearch 
        self.use_elasticsearch = True
        if self._initialize_elasticsearch():
            # å¦‚æžœåˆå§‹åŒ–æˆåŠŸï¼Œå˜—è©¦è¼‰å…¥ç¾æœ‰ç´¢å¼•
            self.load_existing_index()
    
    def _store_system_status(self, key: str, value: Any):
        """å„²å­˜ç³»çµ±ç‹€æ…‹ä¿¡æ¯"""
        self.system_status[key] = value
    
    def get_system_status(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±ç‹€æ…‹ä¿¡æ¯"""
        return self.system_status.copy()
    
    def _initialize_elasticsearch(self):
        """åˆå§‹åŒ– Elasticsearch é€£æŽ¥å’Œå‘é‡å­˜å„²"""
        try:
            if self._setup_elasticsearch_client():
                if self._create_elasticsearch_index():
                    if self._setup_elasticsearch_store():
                        if SHOW_TECHNICAL_MESSAGES:
                            st.success("âœ… Elasticsearch RAG ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
                        self._store_system_status('system_initialized', True)
                        # ç¢ºä¿ use_elasticsearch æ¨™èªŒæ­£ç¢ºè¨­ç½®
                        self.use_elasticsearch = True
                        return True
                    else:
                        st.error("âŒ Elasticsearch å‘é‡å­˜å„²è¨­ç½®å¤±æ•—")
                        self.use_elasticsearch = False
                else:
                    st.error("âŒ Elasticsearch ç´¢å¼•å‰µå»ºå¤±æ•—")
                    self.use_elasticsearch = False
            else:
                st.error("âŒ Elasticsearch å®¢æˆ¶ç«¯é€£æŽ¥å¤±æ•—")
                self.use_elasticsearch = False
            return False
        except Exception as e:
            st.error(f"âŒ Elasticsearch åˆå§‹åŒ–å¤±æ•—: {str(e)}")
            self.use_elasticsearch = False
            return False
    
    def _ensure_models_initialized(self):
        """ç¢ºä¿æ¨¡åž‹å·²åˆå§‹åŒ–ä¸¦å­˜å„²ç‚ºå¯¦ä¾‹å±¬æ€§"""
        if not self.models_initialized or self.embedding_model is None:
            super()._ensure_models_initialized()
            
            # å¾ž Settings ç²å–æ¨¡åž‹ä¸¦å­˜å„²ç‚ºå¯¦ä¾‹å±¬æ€§
            from llama_index.core import Settings
            self.embedding_model = Settings.embed_model
            self.llm_model = Settings.llm
            
            if SHOW_TECHNICAL_MESSAGES:
                st.info("âœ… Elasticsearch RAG ç³»çµ±æ¨¡åž‹åˆå§‹åŒ–å®Œæˆ")
            self._store_system_status('model_initialized', True)
    
    def _get_default_config(self) -> Dict:
        """ç²å–é è¨­ Elasticsearch é…ç½®"""
        return {
            'host': ELASTICSEARCH_HOST or 'localhost',
            'port': ELASTICSEARCH_PORT or 9200,
            'scheme': ELASTICSEARCH_SCHEME or 'http',
            'username': ELASTICSEARCH_USERNAME,
            'password': ELASTICSEARCH_PASSWORD,
            'timeout': ELASTICSEARCH_TIMEOUT or 30,
            'max_retries': ELASTICSEARCH_MAX_RETRIES or 3,
            'verify_certs': ELASTICSEARCH_VERIFY_CERTS,
            'index_name': ELASTICSEARCH_INDEX_NAME or 'rag_intelligent_assistant',
            'shards': ELASTICSEARCH_SHARDS or 1,
            'replicas': ELASTICSEARCH_REPLICAS or 0,
            'dimension': ELASTICSEARCH_VECTOR_DIMENSION or 384,
            'similarity': ELASTICSEARCH_SIMILARITY or 'cosine',
            'text_field': 'content',
            'vector_field': 'embedding',
            'metadata_fields': ['source', 'page', 'chunk_id', 'timestamp', 'file_type', 'file_size']
        }
    
    def _setup_elasticsearch_client(self) -> bool:
        """è¨­ç½® Elasticsearch å®¢æˆ¶ç«¯ï¼ˆçµ±ä¸€ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯ï¼‰"""
        if not ELASTICSEARCH_AVAILABLE:
            st.error("âŒ Elasticsearch ä¾è³´æœªå®‰è£")
            return False
            
        try:
            config = self.elasticsearch_config
            
            # çµ±ä¸€ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯ - LlamaIndex ElasticsearchStore éœ€è¦åŒæ­¥å®¢æˆ¶ç«¯
            from elasticsearch import Elasticsearch
            
            # å»ºç«‹é€£æŽ¥é…ç½® - ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯
            es_config = {
                'hosts': [{
                    'host': config['host'],
                    'port': config['port'],
                    'scheme': config['scheme']
                }],
                'request_timeout': config['timeout'],
                'max_retries': 3,
                'retry_on_timeout': True
            }
            
            # æ·»åŠ é©—è­‰ä¿¡æ¯ï¼ˆå¦‚æžœé…ç½®äº†ï¼‰
            if config.get('username') and config.get('password'):
                es_config['basic_auth'] = (config['username'], config['password'])
            
            # å‰µå»ºåŒæ­¥å®¢æˆ¶ç«¯ï¼ˆElasticsearchStore è¦æ±‚ï¼‰
            sync_client = Elasticsearch(**es_config)
            
            # æ¸¬è©¦é€£æŽ¥
            if sync_client.ping():
                # è¨˜éŒ„é€£æŽ¥æˆåŠŸä¿¡æ¯ï¼ˆåƒ…åœ¨æŠ€è¡“æ¨¡å¼ä¸‹é¡¯ç¤ºï¼‰
                if SHOW_TECHNICAL_MESSAGES:
                    st.success(f"âœ… æˆåŠŸé€£æŽ¥åˆ° Elasticsearch: {config['host']}:{config['port']}")
                    
                    # é¡¯ç¤ºé›†ç¾¤ä¿¡æ¯
                    try:
                        cluster_info = sync_client.info()
                        st.info(f"ðŸ“Š ES é›†ç¾¤ç‰ˆæœ¬: {cluster_info.get('version', {}).get('number', 'unknown')}")
                    except:
                        pass
                
                # å„²å­˜ç³»çµ±ç‹€æ…‹ä¿¡æ¯ä¾› Dashboard ä½¿ç”¨
                self._store_system_status('elasticsearch_connected', True)
                try:
                    cluster_info = sync_client.info()
                    self._store_system_status('elasticsearch_version', cluster_info.get('version', {}).get('number', 'unknown'))
                except:
                    self._store_system_status('elasticsearch_version', 'unknown')
                
                # çµ±ä¸€ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯
                self.elasticsearch_client = sync_client
                self.sync_elasticsearch_client = sync_client
                
                if DEBUG_MODE:
                    print(f"âœ… ESå®¢æˆ¶ç«¯åˆå§‹åŒ–å®Œæˆï¼Œé¡žåž‹: {type(self.elasticsearch_client)}")
                
                return True
            else:
                st.error("âŒ ç„¡æ³•é€£æŽ¥åˆ° Elasticsearch")
                return False
                
        except Exception as e:
            st.error(f"âŒ Elasticsearch å®¢æˆ¶ç«¯è¨­ç½®å¤±æ•—: {str(e)}")
            return False
    
    def _check_and_update_mapping(self, es_client, new_mapping: dict) -> bool:
        """æª¢æŸ¥ä¸¦æ›´æ–°ç¾æœ‰ç´¢å¼•çš„ mappingï¼ˆå¦‚æžœéœ€è¦ï¼‰"""
        try:
            # ç²å–ç¾æœ‰ mapping
            current_mapping_response = es_client.indices.get_mapping(index=self.index_name)
            current_mapping = current_mapping_response[self.index_name]['mappings']
            
            # æª¢æŸ¥å‘é‡ç¶­åº¦
            current_embedding = current_mapping.get('properties', {}).get('embedding', {})
            current_dims = current_embedding.get('dims')
            new_dims = new_mapping['mappings']['properties']['embedding']['dims']
            
            if current_dims != new_dims:
                st.warning(f"âš ï¸ æª¢æ¸¬åˆ°å‘é‡ç¶­åº¦è®Šæ›´: {current_dims} â†’ {new_dims}")
                st.warning("ðŸ’¡ å‘é‡ç¶­åº¦è®Šæ›´éœ€è¦é‡å»ºç´¢å¼•ã€‚è«‹è€ƒæ…®:")
                st.warning("   1. å‚™ä»½ç¾æœ‰æ•¸æ“š")
                st.warning("   2. åˆªé™¤ç¾æœ‰ç´¢å¼•")
                st.warning("   3. é‡æ–°å‰µå»ºç´¢å¼•ä¸¦é‡æ–°ç´¢å¼•æ•¸æ“š")
                return False
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ æ–°å­—æ®µ
            current_props = current_mapping.get('properties', {})
            new_props = new_mapping['mappings']['properties']
            
            missing_fields = []
            for field_name, field_config in new_props.items():
                if field_name not in current_props:
                    missing_fields.append(field_name)
                elif field_name == 'metadata':
                    # æª¢æŸ¥ metadata å­å­—æ®µ
                    current_metadata_props = current_props[field_name].get('properties', {})
                    new_metadata_props = field_config.get('properties', {})
                    for sub_field, sub_config in new_metadata_props.items():
                        if sub_field not in current_metadata_props:
                            missing_fields.append(f"metadata.{sub_field}")
            
            if missing_fields:
                st.info(f"ðŸ“ æª¢æ¸¬åˆ°æ–°å­—æ®µéœ€è¦æ·»åŠ : {missing_fields}")
                try:
                    # å‹•æ…‹æ·»åŠ æ–°å­—æ®µåˆ°ç¾æœ‰ mapping
                    for field in missing_fields:
                        if '.' in field:
                            # metadata å­å­—æ®µ
                            field_parts = field.split('.')
                            parent_field = field_parts[0]
                            sub_field = field_parts[1]
                            
                            field_mapping = {
                                "properties": {
                                    sub_field: new_props[parent_field]['properties'][sub_field]
                                }
                            }
                            
                            es_client.indices.put_mapping(
                                index=self.index_name,
                                body={
                                    "properties": {
                                        parent_field: field_mapping
                                    }
                                }
                            )
                        else:
                            # é ‚å±¤å­—æ®µ
                            field_mapping = {
                                "properties": {
                                    field: new_props[field]
                                }
                            }
                            es_client.indices.put_mapping(
                                index=self.index_name,
                                body=field_mapping
                            )
                    
                    st.success(f"âœ… æˆåŠŸæ·»åŠ æ–°å­—æ®µ: {missing_fields}")
                    
                except Exception as update_error:
                    st.warning(f"âš ï¸ ç„¡æ³•è‡ªå‹•æ›´æ–° mapping: {update_error}")
                    st.info("ðŸ’¡ è«‹æ‰‹å‹•æª¢æŸ¥ mapping é…ç½®")
            
            else:
                _tech_info("âœ… ç¾æœ‰ mapping é…ç½®å·²æ˜¯æœ€æ–°")
            
            return True
            
        except Exception as e:
            st.warning(f"âš ï¸ æª¢æŸ¥ mapping æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return True  # ç¹¼çºŒåŸ·è¡Œï¼Œä¸é˜»å¡žç³»çµ±å•Ÿå‹•
    
    def _create_elasticsearch_index(self) -> bool:
        """å‰µå»º Elasticsearch ç´¢å¼•"""
        try:
            config = self.elasticsearch_config
            
            # é©—è­‰/å°é½ŠåµŒå…¥ç¶­åº¦
            try:
                # ä½¿ç”¨ EnhancedRAGSystem çš„ç¶­åº¦æª¢æ¸¬
                actual_dim = self._get_embed_dim()
            except Exception:
                actual_dim = None
            expected_dim = config.get('dimension')
            
            if actual_dim is not None and expected_dim and int(actual_dim) != int(expected_dim):
                st.warning(f"âš ï¸ æª¢æ¸¬åˆ°æ¨¡åž‹ç¶­åº¦ {actual_dim} èˆ‡é…ç½®ç¶­åº¦ {expected_dim} ä¸ä¸€è‡´ï¼Œå°‡ä»¥æ¨¡åž‹ç¶­åº¦ç‚ºæº–ã€‚")
                config['dimension'] = int(actual_dim)
            
            # æœ€çµ‚ç¶­åº¦é©—è­‰
            if not self._validate_embedding_dimension(config['dimension']):
                st.error("âŒ åµŒå…¥ç¶­åº¦é©—è­‰å¤±æ•—ï¼Œåœæ­¢å»ºç«‹ç´¢å¼•ã€‚")
                return False
            
            # ä½¿ç”¨é…ç½®æ–‡ä»¶åŠ è¼‰ç´¢å¼•æ˜ å°„
            try:
                from config.elasticsearch.mapping_loader import ElasticsearchMappingLoader
                mapping_loader = ElasticsearchMappingLoader()
                index_mapping = mapping_loader.create_mapping_with_config(config)
                
                # é©—è­‰é…ç½®
                if not mapping_loader.validate_mapping(index_mapping):
                    raise ValueError("Mapping é…ç½®é©—è­‰å¤±æ•—")
                
                _tech_info(f"âœ… æˆåŠŸå¾žé…ç½®æ–‡ä»¶åŠ è¼‰ Elasticsearch mapping")
                print(f"ðŸ“‹ Mapping ç¶­åº¦: {config['dimension']}")
                
            except Exception as mapping_error:
                # å¦‚æžœç„¡æ³•åŠ è¼‰é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨å¾Œå‚™çš„ç¡¬ç·¨ç¢¼é…ç½®
                st.warning(f"âš ï¸ ç„¡æ³•åŠ è¼‰ mapping é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜èªé…ç½®: {str(mapping_error)}")
                index_mapping = {
                    "settings": {
                        "number_of_shards": config['shards'],
                        "number_of_replicas": config['replicas'],
                        "analysis": {
                            "analyzer": {
                                "chinese_analyzer": {
                                    "type": "custom",
                                    "tokenizer": "standard",
                                    "filter": ["lowercase", "cjk_width", "cjk_bigram"]
                                }
                            }
                        }
                    },
                    "mappings": {
                        "properties": {
                            config['text_field']: {
                                "type": "text",
                                "analyzer": "chinese_analyzer",
                                "search_analyzer": "chinese_analyzer"
                            },
                            config['vector_field']: {
                                "type": "dense_vector",
                                "dims": config['dimension'],
                                "index": True,
                                "similarity": config['similarity']
                            },
                            "metadata": {
                                "type": "object",
                                "properties": {
                                    "source": {"type": "keyword"},
                                    "page": {"type": "integer"},
                                    "chunk_id": {"type": "keyword"},
                                    "timestamp": {"type": "date"},
                                    "file_type": {"type": "keyword"},
                                    "file_size": {"type": "integer"}
                                }
                            }
                        }
                    }
                }
            
            # æª¢æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨ï¼ˆä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯ï¼‰
            sync_client = getattr(self, 'sync_elasticsearch_client', None)
            if not sync_client:
                # å¦‚æžœæ²’æœ‰åŒæ­¥å®¢æˆ¶ç«¯ï¼Œå‰µå»ºä¸€å€‹
                from elasticsearch import Elasticsearch
                sync_client = Elasticsearch(**{
                    'hosts': [{'host': config['host'], 'port': config['port'], 'scheme': config['scheme']}],
                    'request_timeout': config['timeout'],
                })
                self.sync_elasticsearch_client = sync_client
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºç¬¬ä¸€æ¬¡å•Ÿå‹•ï¼ˆç´¢å¼•ä¸å­˜åœ¨ï¼‰
            index_exists = sync_client.indices.exists(index=self.index_name)
            is_first_time_startup = not index_exists
            
            if index_exists:
                _tech_info(f"ðŸ“š ç´¢å¼• '{self.index_name}' å·²å­˜åœ¨")
                # æª¢æŸ¥ç¾æœ‰ç´¢å¼•çš„ mapping æ˜¯å¦éœ€è¦æ›´æ–°
                self._check_and_update_mapping(sync_client, index_mapping)
                return True
            
            # ç¬¬ä¸€æ¬¡å•Ÿå‹• - è‡ªå‹•å‰µå»º mapping
            if is_first_time_startup:
                st.info(f"ðŸš€ æª¢æ¸¬åˆ°ç¬¬ä¸€æ¬¡å•Ÿå‹•ï¼Œå°‡è‡ªå‹•å‰µå»º Elasticsearch ç´¢å¼•å’Œ mapping")
                print(f"ðŸ“‹ ä½¿ç”¨é…ç½®: ç¶­åº¦={config['dimension']}, åˆ†ç‰‡={config['shards']}, å‰¯æœ¬={config['replicas']}")
                
                # åœ¨ç¬¬ä¸€æ¬¡å•Ÿå‹•æ™‚æä¾› mapping é¸æ“‡
                mapping_choice = self._get_mapping_choice_for_first_startup()
                if mapping_choice and mapping_choice != "index_mapping.json":
                    try:
                        from config.elasticsearch.mapping_loader import ElasticsearchMappingLoader
                        mapping_loader = ElasticsearchMappingLoader()
                        index_mapping = mapping_loader.create_mapping_with_config(config)
                        st.info(f"ðŸ“‹ ä½¿ç”¨ {mapping_choice} é…ç½®å‰µå»ºç´¢å¼•")
                    except Exception as e:
                        st.warning(f"âš ï¸ ç„¡æ³•åŠ è¼‰ {mapping_choice}ï¼Œä½¿ç”¨é»˜èªé…ç½®: {e}")
                
                # è¨˜éŒ„ç¬¬ä¸€æ¬¡å•Ÿå‹•çš„é…ç½®
                self._log_first_startup_config(config, mapping_choice or "index_mapping.json")
            
            # å‰µå»ºç´¢å¼•ï¼ˆä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯ï¼‰
            try:
                st.info(f"ðŸ”§ æ­£åœ¨åˆ›å»ºç´¢å¼•: {self.index_name}")
                response = sync_client.indices.create(
                    index=self.index_name,
                    body=index_mapping,
                    ignore=400  # å¿½ç•¥å·²å­˜åœ¨çš„éŒ¯èª¤
                )
                
                if response.get('acknowledged', False):
                    st.success(f"âœ… æˆåŠŸåˆ›å»ºç´¢å¼•: {self.index_name}")
                    # é©—è­‰ç´¢å¼•åˆ›å»º
                    if sync_client.indices.exists(index=self.index_name):
                        st.info("ðŸ“‹ ç´¢å¼•åˆ›å»ºéªŒè¯é€šè¿‡")
                    return True
                else:
                    st.error(f"âŒ ç´¢å¼•å‰µå»ºå¤±æ•—: {response}")
                    return False
                    
            except Exception as create_error:
                # å¦‚æžœæ˜¯ HeadApiResponse async éŒ¯èª¤ï¼Œå˜—è©¦åŒæ­¥æ–¹å¼
                error_msg = str(create_error)
                if "HeadApiResponse" in error_msg or "await" in error_msg:
                    st.warning(f"âš ï¸ æª¢æ¸¬åˆ°asyncå…¼å®¹æ€§å•é¡Œï¼Œå˜—è©¦åŒæ­¥æ–¹å¼å‰µå»ºç´¢å¼•...")
                    try:
                        # ä½¿ç”¨åŒæ­¥ Elasticsearch å®¢æˆ¶ç«¯é‡æ–°åˆå§‹åŒ–
                        from elasticsearch import Elasticsearch
                        sync_client = Elasticsearch(
                            [{'host': self.elasticsearch_config['host'], 'port': self.elasticsearch_config['port']}],
                            timeout=30,
                            request_timeout=30
                        )
                        
                        # æ¸¬è©¦é€£æŽ¥
                        if sync_client.ping():
                            # ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯å‰µå»ºç´¢å¼•
                            response = sync_client.indices.create(
                                index=self.index_name,
                                body=index_mapping,
                                ignore=400
                            )
                            # æ›´æ–°å®¢æˆ¶ç«¯ç‚ºåŒæ­¥ç‰ˆæœ¬
                            self.elasticsearch_client = sync_client
                            st.success(f"âœ… ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯æˆåŠŸå‰µå»ºç´¢å¼•: {self.index_name}")
                            return True
                        else:
                            st.error("âŒ åŒæ­¥å®¢æˆ¶ç«¯ç„¡æ³•é€£æŽ¥åˆ° Elasticsearch")
                            return False
                    except Exception as sync_error:
                        st.error(f"âŒ åŒæ­¥å‰µå»ºç´¢å¼•ä¹Ÿå¤±æ•—: {str(sync_error)}")
                        return False
                else:
                    st.error(f"âŒ å‰µå»ºç´¢å¼•å¤±æ•—: {error_msg}")
                    return False
        except Exception as e:
            st.error(f"âŒ å‰µå»ºç´¢å¼•æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return False
                
    def query(self, query_str: str, **kwargs) -> str:
        """åŸ·è¡ŒæŸ¥è©¢ä¸¦è¿”å›žçµæžœå­—ç¬¦ä¸²"""
        if not self.query_engine:
            # å˜—è©¦è‡ªå‹•é‡æ–°åˆå§‹åŒ–æŸ¥è©¢å¼•æ“Ž
            print("âš ï¸ æŸ¥è©¢å¼•æ“Žæœªè¨­ç½®ï¼Œå˜—è©¦è‡ªå‹•é‡æ–°åˆå§‹åŒ–...")
            if self.index and self._setup_elasticsearch_store():
                self.setup_query_engine()
                if self.query_engine:
                    print("âœ… æŸ¥è©¢å¼•æ“Žè‡ªå‹•é‡æ–°åˆå§‹åŒ–æˆåŠŸ")
                else:
                    print("âŒ æŸ¥è©¢å¼•æ“Žè‡ªå‹•é‡æ–°åˆå§‹åŒ–å¤±æ•—")
                    return "âŒ æŸ¥è©¢å¼•æ“Žåˆå§‹åŒ–å¤±æ•—ã€‚è«‹æª¢æŸ¥ç³»çµ±ç‹€æ…‹æˆ–é‡æ–°ä¸Šå‚³æ–‡æª”ã€‚"
            else:
                return "âŒ æŸ¥è©¢å¼•æ“Žå°šæœªè¨­ç½®ã€‚è«‹å…ˆä¸Šå‚³ä¸¦ç´¢å¼•æ–‡æª”ã€‚"
        
        tracker = get_performance_tracker()
        
        try:
            with track_rag_stage(RAGStages.TOTAL_QUERY_TIME, query=query_str):
                print(f"ðŸ” é–‹å§‹åŸ·è¡ŒæŸ¥è©¢: {query_str}")
                print(f"ðŸ”§ æŸ¥è©¢å¼•æ“Žé¡žåž‹: {type(self.query_engine)}")
                
                with track_rag_stage(RAGStages.QUERY_PROCESSING):
                    response = self.query_engine.query(query_str)
                
                print(f"âœ… æŸ¥è©¢å®Œæˆï¼ŒéŸ¿æ‡‰é¡žåž‹: {type(response)}")
                return str(response)
            
        except Exception as e:
            error_msg = str(e)
            error_type = type(e).__name__
            
            print(f"âŒ æŸ¥è©¢éŒ¯èª¤è©³æƒ…:")
            print(f"   éŒ¯èª¤é¡žåž‹: {error_type}")
            print(f"   éŒ¯èª¤æ¶ˆæ¯: {error_msg}")
            
            # æª¢æŸ¥æ˜¯å¦ç‚º ObjectApiResponse éŒ¯èª¤
            if "ObjectApiResponse" in error_msg or "await" in error_msg:
                print("ðŸš¨ æª¢æ¸¬åˆ°ObjectApiResponseéŒ¯èª¤ï¼")
                print(f"   æŸ¥è©¢å¼•æ“Ž: {type(self.query_engine)}")
                if hasattr(self.query_engine, '_retriever'):
                    print(f"   æª¢ç´¢å™¨: {type(self.query_engine._retriever)}")
                
            import traceback
            print(f"ðŸ” å®Œæ•´éŒ¯èª¤å †ç–Š:")
            print(traceback.format_exc())
            
            st.error(f"æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {error_msg}")
            st.write(traceback.format_exc())
            return f"æŸ¥è©¢å¤±æ•—: {error_msg}"
    
    def query_with_sources(self, query_str: str, save_to_history: bool = True, session_id: str = None, user_id: str = None, **kwargs) -> Dict[str, Any]:
        """åŸ·è¡ŒæŸ¥è©¢ä¸¦è¿”å›žå¸¶æœ‰ä¾†æºä¿¡æ¯çš„å®Œæ•´çµæžœ"""
        if not self.query_engine:
            # å˜—è©¦è‡ªå‹•é‡æ–°åˆå§‹åŒ–æŸ¥è©¢å¼•æ“Ž
            print("âš ï¸ æŸ¥è©¢å¼•æ“Žæœªè¨­ç½®ï¼Œå˜—è©¦è‡ªå‹•é‡æ–°åˆå§‹åŒ–...")
            if self.index and self._setup_elasticsearch_store():
                self.setup_query_engine()
                if self.query_engine:
                    print("âœ… æŸ¥è©¢å¼•æ“Žè‡ªå‹•é‡æ–°åˆå§‹åŒ–æˆåŠŸ")
                else:
                    print("âŒ æŸ¥è©¢å¼•æ“Žè‡ªå‹•é‡æ–°åˆå§‹åŒ–å¤±æ•—")
                    return {
                        "answer": "âŒ æŸ¥è©¢å¼•æ“Žåˆå§‹åŒ–å¤±æ•—ã€‚è«‹æª¢æŸ¥ç³»çµ±ç‹€æ…‹æˆ–é‡æ–°ä¸Šå‚³æ–‡æª”ã€‚",
                        "sources": [],
                        "metadata": {}
                    }
            else:
                return {
                    "answer": "âŒ æŸ¥è©¢å¼•æ“Žå°šæœªè¨­ç½®ã€‚è«‹å…ˆä¸Šå‚³ä¸¦ç´¢å¼•æ–‡æª”ã€‚",
                    "sources": [],
                    "metadata": {}
                }
        
        tracker = get_performance_tracker()
        start_time = datetime.now()
        
        try:
            with track_rag_stage(RAGStages.TOTAL_QUERY_TIME, query=query_str, has_sources=True):
                print(f"ðŸ” é–‹å§‹åŸ·è¡Œå¸¶ä¾†æºçš„æŸ¥è©¢: {query_str}")
                print(f"ðŸ”§ æŸ¥è©¢å¼•æ“Žé¡žåž‹: {type(self.query_engine)}")
                
                # æŸ¥è©¢è™•ç†éšŽæ®µ
                with track_rag_stage(RAGStages.SIMILARITY_SEARCH):
                    response = self.query_engine.query(query_str)
                
                print(f"âœ… æŸ¥è©¢å®Œæˆï¼ŒéŸ¿æ‡‰é¡žåž‹: {type(response)}")
                
                # ä¸Šä¸‹æ–‡æª¢ç´¢éšŽæ®µ
                with track_rag_stage(RAGStages.CONTEXT_RETRIEVAL):
                    # æå–ç­”æ¡ˆ
                    answer = str(response)
                    
                    # æå–ä¾†æºä¿¡æ¯
                    sources = []
                    if hasattr(response, 'source_nodes') and response.source_nodes:
                        print(f"ðŸ“š æ‰¾åˆ° {len(response.source_nodes)} å€‹ä¾†æºç¯€é»ž")
                        for i, node in enumerate(response.source_nodes):
                            source_info = {
                                "content": node.node.text[:200] + "..." if len(node.node.text) > 200 else node.node.text,
                                "source": node.node.metadata.get("source", "æœªçŸ¥ä¾†æº"),
                                "file_path": node.node.metadata.get("file_path", ""),
                                "score": float(node.score) if hasattr(node, 'score') else 0.0,
                                "page": node.node.metadata.get("page", ""),
                                "type": node.node.metadata.get("type", "user_document")
                            }
                            sources.append(source_info)
                            print(f"  [{i+1}] ä¾†æº: {source_info['source']}, è©•åˆ†: {source_info['score']}")
                    else:
                        print("âŒ éŸ¿æ‡‰ä¸­æ²’æœ‰æ‰¾åˆ°ä¾†æºç¯€é»ž")
                
                # è¨ˆç®—éŸ¿æ‡‰æ™‚é–“
                response_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
                
                # ç²å–æ€§èƒ½çµ±è¨ˆ
                performance_summary = tracker.get_session_summary()
                
                metadata = {
                    "query": query_str,
                    "total_sources": len(sources),
                    "response_time_ms": response_time_ms,
                    "model": "Groq LLama 3.3",
                    "backend": "Elasticsearch",
                    "performance": performance_summary
                }
                
                result = {
                    "answer": answer,
                    "sources": sources,
                    "metadata": metadata
                }
            
            # ä¿å­˜åˆ°å°è©±è¨˜éŒ„
            if save_to_history and self.conversation_manager:
                try:
                    conversation_id = self.conversation_manager.save_conversation(
                        question=query_str,
                        answer=answer,
                        sources=sources,
                        metadata=metadata,
                        session_id=session_id,
                        user_id=user_id
                    )
                    if conversation_id:
                        result["conversation_id"] = conversation_id
                        print(f"ðŸ’¾ å°è©±è¨˜éŒ„å·²ä¿å­˜: {conversation_id}")
                except Exception as save_error:
                    print(f"âš ï¸ ä¿å­˜å°è©±è¨˜éŒ„å¤±æ•—: {str(save_error)}")
            
            return result
            
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ å¸¶ä¾†æºæŸ¥è©¢å¤±æ•—: {error_msg}")
            import traceback
            print(f"ðŸ” å®Œæ•´éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
            
            return {
                "answer": f"æŸ¥è©¢å¤±æ•—: {error_msg}",
                "sources": [],
                "metadata": {"error": error_msg}
            }
                
    def _setup_elasticsearch_store(self) -> bool:
        """è¨­ç½® Elasticsearch å‘é‡å­˜å„²"""
        try:
            # ä½¿ç”¨çµ±ä¸€çš„åŒæ­¥å®¢æˆ¶ç«¯
            if not hasattr(self, 'elasticsearch_client') or not self.elasticsearch_client:
                st.error("âŒ Elasticsearch å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–")
                return False
            
            print(f"ðŸ”§ è¨­ç½®ElasticsearchStoreï¼Œå®¢æˆ¶ç«¯é¡žåž‹: {type(self.elasticsearch_client)}")
            
            # ä½¿ç”¨è‡ªå®šç¾©åŒæ­¥ Elasticsearch Store é¿å… async å•é¡Œ
            from ..storage.custom_elasticsearch_store import CustomElasticsearchStore
            self.elasticsearch_store = CustomElasticsearchStore(
                es_client=self.elasticsearch_client,
                index_name=self.index_name,
                vector_field=self.elasticsearch_config['vector_field'],
                text_field=self.elasticsearch_config['text_field'],
                metadata_field='metadata'
            )
            
            _tech_success("âœ… Elasticsearch å‘é‡å­˜å„²è¨­ç½®å®Œæˆ (ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯)")
            return True
                
        except Exception as e:
            st.error(f"âŒ Elasticsearch å‘é‡å­˜å„²è¨­ç½®å¤±æ•—: {str(e)}")
            import traceback
            st.error(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
            return False
    
    def load_existing_index(self) -> bool:
        """è¼‰å…¥ç¾æœ‰çš„ Elasticsearch ç´¢å¼•"""
        try:
            # è¨­ç½® Elasticsearch å®¢æˆ¶ç«¯
            if not self._setup_elasticsearch_client():
                st.error("âŒ ç„¡æ³•é€£æŽ¥åˆ° Elasticsearch")
                return False
            
            # æª¢æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨ï¼ˆä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯ï¼‰
            sync_client = getattr(self, 'sync_elasticsearch_client', None)
            if sync_client and sync_client.indices.exists(index=self.index_name):
                # ç²å–ç´¢å¼•çµ±è¨ˆè³‡è¨Š
                stats = sync_client.indices.stats(index=self.index_name)
                doc_count = stats['indices'][self.index_name]['total']['docs']['count']
                
                if doc_count > 0:
                    # é©—è­‰ç´¢å¼•ç¶­åº¦èˆ‡åµŒå…¥æ¨¡åž‹
                    try:
                        mapping = sync_client.indices.get_mapping(index=self.index_name)
                        props = mapping[self.index_name]['mappings']['properties']
                        vec_field = self.elasticsearch_config['vector_field']
                        index_dims = props.get(vec_field, {}).get('dims')
                    except Exception:
                        index_dims = None

                    model_dim = None
                    try:
                        model_dim = self._get_embed_dim()
                    except Exception:
                        pass

                    if index_dims is not None and model_dim is not None and int(index_dims) != int(model_dim):
                        st.error(f"âŒ åµŒå…¥ç¶­åº¦ä¸ä¸€è‡´ï¼šç´¢å¼•ç‚º {index_dims}ï¼Œæ¨¡åž‹ç‚º {model_dim}ã€‚è«‹å°é½Šå¾Œé‡è©¦ã€‚")
                        return False

                    # è¨­ç½®å‘é‡å­˜å„²
                    if self._setup_elasticsearch_store():
                        # ç¢ºä¿æ¨¡åž‹åˆå§‹åŒ–
                        self._ensure_models_initialized()
                        
                        # é‡æ–°å‰µå»ºç´¢å¼•å°è±¡
                        storage_context = StorageContext.from_defaults(vector_store=self.elasticsearch_store)
                        self.index = VectorStoreIndex.from_vector_store(
                            vector_store=self.elasticsearch_store,
                            storage_context=storage_context,
                            embed_model=self.embedding_model
                        )
                        
                        # è¨­ç½®æŸ¥è©¢å¼•æ“Ž
                        self.setup_query_engine()
                        return True
                    else:
                        st.error("âŒ Elasticsearch å‘é‡å­˜å„²è¨­ç½®å¤±æ•—")
                        return False
                else:
                    st.info(f"ðŸ“š ç´¢å¼• '{self.index_name}' å­˜åœ¨ä½†ç‚ºç©º")
                    return False
            else:
                st.info("ðŸ’¡ æ²’æœ‰ç™¼ç¾ç¾æœ‰çš„ Elasticsearch ç´¢å¼•ï¼Œè«‹ä¸Šå‚³æ–‡æª”ä¾†å»ºç«‹æ–°ç´¢å¼•")
                return False
                
        except Exception as e:
            st.error(f"âŒ è¼‰å…¥ Elasticsearch ç´¢å¼•å¤±æ•—: {str(e)}")
            return False
    
    def get_enhanced_statistics(self) -> Dict[str, Any]:
        """ç²å– Elasticsearch RAG ç³»çµ±çš„çµ±è¨ˆè³‡è¨Š"""
        try:
            stats = {
                "system_type": "elasticsearch",
                "base_statistics": self.memory_stats.copy(),
                "elasticsearch_stats": {}
            }
            
            # ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯é€²è¡Œçµ±è¨ˆæŸ¥è©¢
            sync_client = getattr(self, 'sync_elasticsearch_client', None)
            if sync_client and self.index_name:
                try:
                    # å…ˆåˆ·æ–°ç´¢å¼•ç¢ºä¿æœ€æ–°æ•¸æ“š
                    sync_client.indices.refresh(index=self.index_name)
                    
                    # ç²å–ç´¢å¼•çµ±è¨ˆ
                    index_stats = sync_client.indices.stats(index=self.index_name)
                    total_stats = index_stats['indices'][self.index_name]['total']
                    
                    stats["elasticsearch_stats"] = {
                        "index_name": self.index_name,
                        "document_count": total_stats['docs']['count'],
                        "index_size_bytes": total_stats['store']['size_in_bytes'],
                        "index_size_mb": round(total_stats['store']['size_in_bytes'] / 1024 / 1024, 2)
                    }
                    
                    # æ›´æ–°åŸºç¤Žçµ±è¨ˆ
                    stats["base_statistics"]["total_documents"] = total_stats['docs']['count']
                    stats["base_statistics"]["total_nodes"] = total_stats['docs']['count']
                    
                    # è¨˜éŒ„è©³ç´°çµ±è¨ˆæ—¥èªŒ
                    print(f"ðŸ“Š ESçµ±è¨ˆæ›´æ–°: ç´¢å¼•={self.index_name}, æ–‡æª”æ•¸={total_stats['docs']['count']}")
                    
                except Exception as e:
                    st.warning(f"ç„¡æ³•ç²å– Elasticsearch çµ±è¨ˆ: {e}")
                    # å¦‚æžœç´¢å¼•ä¸å­˜åœ¨ï¼Œçµ±è¨ˆç‚º0
                    if "index_not_found" in str(e).lower():
                        stats["elasticsearch_stats"] = {
                            "index_name": self.index_name,
                            "document_count": 0,
                            "index_size_bytes": 0,
                            "index_size_mb": 0
                        }
                    
            return stats
            
        except Exception as e:
            st.error(f"çµ±è¨ˆè³‡è¨Šç²å–å¤±æ•—: {e}")
            return {
                "system_type": "elasticsearch", 
                "base_statistics": self.memory_stats.copy(),
                "elasticsearch_stats": {}
            }
    
    def get_elasticsearch_statistics(self) -> Dict[str, Any]:
        """ç²å–è©³ç´°çš„ Elasticsearch çµ±è¨ˆè³‡è¨Š"""
        return self.get_enhanced_statistics()
    
    def get_document_statistics(self) -> Dict[str, Any]:
        """è¦†å¯«çˆ¶é¡žæ–¹æ³•ï¼Œä½¿ç”¨ Elasticsearch å°ˆç”¨çµ±è¨ˆ"""
        try:
            # ä½¿ç”¨ Elasticsearch å°ˆç”¨çµ±è¨ˆæ–¹æ³•
            enhanced_stats = self.get_enhanced_statistics()
            
            # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
            es_stats = enhanced_stats.get("elasticsearch_stats", {})
            base_stats = enhanced_stats.get("base_statistics", {})
            
            return {
                "total_documents": es_stats.get("document_count", 0),
                "total_nodes": es_stats.get("document_count", 0),
                "document_details": [],
                "total_pages": 0,
                "index_size_bytes": es_stats.get("index_size_bytes", 0),
                "index_size_mb": es_stats.get("index_size_mb", 0)
            }
        except Exception as e:
            st.error(f"ç²å–æ–‡æª”çµ±è¨ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return {
                "total_documents": 0,
                "total_nodes": 0,
                "document_details": [],
                "total_pages": 0
            }
    
    def get_indexed_files(self) -> List[Dict[str, Any]]:
        """ç²å–å·²ç´¢å¼•çš„æ–‡ä»¶åˆ—è¡¨ (Elasticsearch ç‰ˆæœ¬)"""
        try:
            return self.get_indexed_files_from_es()
        except Exception as e:
            st.error(f"ç²å–æ–‡ä»¶åˆ—è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return []
    
    def delete_documents_by_source(self, source_filename: str) -> bool:
        """æ ¹æ“šä¾†æºæ–‡ä»¶ååˆªé™¤æ–‡æª”"""
        sync_client = getattr(self, 'sync_elasticsearch_client', None)
        if not sync_client:
            st.error("âŒ Elasticsearch åŒæ­¥å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–")
            return False
        
        try:
            # æ§‹å»ºæŸ¥è©¢ä»¥æŸ¥æ‰¾æŒ‡å®šä¾†æºçš„æ–‡æª”
            query = {
                "query": {
                    "term": {
                        "metadata.source.keyword": source_filename
                    }
                }
            }
            
            # åˆªé™¤åŒ¹é…çš„æ–‡æª”ï¼Œæ·»åŠ æ›´å¤šåƒæ•¸ä»¥é¿å…è¡çª
            response = sync_client.delete_by_query(
                index=self.index_name,
                body=query,
                refresh=True,
                timeout='60s',
                conflicts='proceed'  # é‡åˆ°ç‰ˆæœ¬è¡çªæ™‚ç¹¼çºŒåŸ·è¡Œ
            )
            
            deleted_count = response.get('deleted', 0)
            version_conflicts = response.get('version_conflicts', 0)
            
            if deleted_count > 0:
                message = f"âœ… å¾ž Elasticsearch ä¸­åˆªé™¤äº† {deleted_count} å€‹æ–‡æª”å¡Šï¼ˆä¾†æºï¼š{source_filename}ï¼‰"
                if version_conflicts > 0:
                    message += f"ï¼Œæœ‰ {version_conflicts} å€‹ç‰ˆæœ¬è¡çªå·²å¿½ç•¥"
                st.success(message)
                return True
            else:
                st.info(f"ðŸ“ åœ¨ Elasticsearch ä¸­æ²’æœ‰æ‰¾åˆ°ä¾†æºç‚º '{source_filename}' çš„æ–‡æª”")
                return False
                
        except Exception as e:
            error_msg = str(e)
            if '409' in error_msg or 'version_conflicts' in error_msg:
                st.warning(f"âš ï¸ åˆªé™¤éŽç¨‹ä¸­é‡åˆ°ç‰ˆæœ¬è¡çªï¼Œä½†å·²å˜—è©¦è™•ç†: {error_msg}")
                # é‡è©¦ä¸€æ¬¡ï¼Œä½¿ç”¨æ›´å¯¬æ¾çš„åƒæ•¸
                try:
                    response = sync_client.delete_by_query(
                        index=self.index_name,
                        body=query,
                        refresh=True,
                        timeout='120s',
                        conflicts='proceed',
                        wait_for_completion=True
                    )
                    deleted_count = response.get('deleted', 0)
                    if deleted_count > 0:
                        st.success(f"âœ… é‡è©¦æˆåŠŸï¼Œåˆªé™¤äº† {deleted_count} å€‹æ–‡æª”å¡Š")
                        return True
                except Exception as retry_e:
                    st.error(f"âŒ é‡è©¦åˆªé™¤å¤±æ•—: {str(retry_e)}")
            else:
                st.error(f"âŒ å¾ž Elasticsearch åˆªé™¤æ–‡æª”å¤±æ•—: {error_msg}")
            return False
    
    def get_indexed_files_from_es(self) -> List[Dict[str, Any]]:
        """å¾ž ES ç´¢å¼•ä¸­ç²å–å·²ç´¢å¼•çš„æ–‡ä»¶åˆ—è¡¨"""
        # ä½¿ç”¨ Elasticsearch å®¢æˆ¶ç«¯é€²è¡ŒæŸ¥è©¢
        es_client = getattr(self, 'elasticsearch_client', None)
        if not es_client:
            return []
        
        try:
            # èšåˆæŸ¥è©¢ç²å–ä¸åŒçš„æ–‡ä»¶ä¾†æº
            query = {
                "size": 0,
                "aggs": {
                    "unique_sources": {
                        "terms": {
                            "field": "metadata.source.keyword",
                            "size": 1000
                        },
                        "aggs": {
                            "total_size": {
                                "sum": {
                                    "field": "metadata.file_size"
                                }
                            },
                            "file_type": {
                                "terms": {
                                    "field": "metadata.type.keyword",
                                    "size": 1
                                }
                            },
                            "latest_timestamp": {
                                "max": {
                                    "field": "metadata.timestamp"
                                }
                            }
                        }
                    }
                }
            }
            
            response = es_client.search(
                index=self.index_name,
                body=query
            )
            
            files = []
            buckets = response.get('aggregations', {}).get('unique_sources', {}).get('buckets', [])
            
            for bucket in buckets:
                source_file = bucket['key']
                chunk_count = bucket['doc_count']  # ç›´æŽ¥å¾ž terms èšåˆå–å¾—æ–‡æª”æ•¸
                total_size = bucket.get('total_size', {}).get('value', 0)
                file_type_buckets = bucket.get('file_type', {}).get('buckets', [])
                file_type = file_type_buckets[0]['key'] if file_type_buckets else 'unknown'
                timestamp = bucket.get('latest_timestamp', {}).get('value_as_string', '')
                
                files.append({
                    'id': source_file,  # ä½¿ç”¨sourceä½œç‚ºIDï¼Œé€™æ¨£åˆªé™¤æ™‚å¯ä»¥æ­£ç¢ºè­˜åˆ¥
                    'name': source_file,
                    'chunk_count': chunk_count,
                    'node_count': chunk_count,  # æ·»åŠ  node_count å­—æ®µå…¼å®¹æ€§
                    'total_size_bytes': total_size,
                    'size': total_size,  # ç‚ºäº†å…¼å®¹æ€§ä¿ç•™ size å­—æ®µ
                    'size_mb': round(total_size / (1024 * 1024), 1) if total_size > 0 else 0,
                    'file_type': file_type,
                    'type': file_type,  # æ·»åŠ  type å­—æ®µå…¼å®¹æ€§
                    'timestamp': timestamp,
                    'upload_time': timestamp,  # æ·»åŠ  upload_time å­—æ®µå…¼å®¹æ€§
                    'page_count': 0,  # æ·»åŠ  page_count å­—æ®µå…¼å®¹æ€§
                    'source': 'elasticsearch'
                })
            
            return files
            
        except Exception as e:
            st.warning(f"å¾ž ES ç²å–æ–‡ä»¶åˆ—è¡¨å¤±æ•—: {str(e)}")
            return []
    
    def get_knowledge_base_file_stats(self) -> Dict[str, Any]:
        """ç²å–çŸ¥è­˜åº«æ–‡ä»¶çµ±è¨ˆï¼ˆå¾žESç´¢å¼•ï¼‰"""
        files = self.get_indexed_files_from_es()
        
        total_files = len(files)
        total_chunks = sum(f['chunk_count'] for f in files)
        total_size_mb = sum(f['size_mb'] for f in files)
        
        # æŒ‰æ–‡ä»¶é¡žåž‹åˆ†é¡ž
        pdf_files = [f for f in files if f['file_type'] == 'pdf']
        image_files = [f for f in files if f['file_type'] in ['png', 'jpg', 'jpeg', 'webp', 'bmp']]
        doc_files = [f for f in files if f['file_type'] in ['txt', 'docx', 'md']]
        
        return {
            'total_files': total_files,
            'total_chunks': total_chunks,
            'total_size_mb': round(total_size_mb, 2),
            'pdf_count': len(pdf_files),
            'image_count': len(image_files),
            'document_count': len(doc_files),
            'files': files
        }
    
    def get_conversation_history(self, session_id: str = None, limit: int = 50) -> List[Dict[str, Any]]:
        """ç²å–å°è©±è¨˜éŒ„"""
        if self.conversation_manager:
            return self.conversation_manager.get_conversation_history(
                session_id=session_id, 
                limit=limit
            )
        return []
    
    def search_conversation_history(self, query_text: str, limit: int = 20) -> List[Dict[str, Any]]:
        """æœç´¢å°è©±è¨˜éŒ„"""
        if self.conversation_manager:
            return self.conversation_manager.search_conversations(query_text, limit)
        return []
    
    def get_conversation_statistics(self) -> Dict[str, Any]:
        """ç²å–å°è©±çµ±è¨ˆä¿¡æ¯"""
        if self.conversation_manager:
            return self.conversation_manager.get_conversation_statistics()
        return {}
    
    def delete_file_from_knowledge_base(self, file_id: str) -> bool:
        """å¾žçŸ¥è­˜åº«ä¸­åˆªé™¤æ–‡ä»¶ (Elasticsearch ç‰ˆæœ¬)
        
        Args:
            file_id: æ–‡ä»¶IDï¼Œåœ¨Elasticsearchç‰ˆæœ¬ä¸­é€™é€šå¸¸æ˜¯æ–‡ä»¶å/ä¾†æº
            
        Returns:
            bool: åˆªé™¤æ˜¯å¦æˆåŠŸ
        """
        try:
            # åœ¨Elasticsearchä¸­ï¼Œfile_idå¯¦éš›ä¸Šæ˜¯sourceåç¨±
            return self.delete_documents_by_source(file_id)
        except Exception as e:
            st.error(f"âŒ åˆªé™¤æ–‡ä»¶å¤±æ•—: {str(e)}")
            return False
    
    def clear_knowledge_base(self) -> bool:
        """æ¸…ç©ºæ•´å€‹çŸ¥è­˜åº«"""
        sync_client = getattr(self, 'sync_elasticsearch_client', None)
        if not sync_client:
            st.error("âŒ Elasticsearch åŒæ­¥å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–")
            return False
        
        try:
            # åˆªé™¤ç´¢å¼•ä¸­æ‰€æœ‰æ–‡æª”
            response = sync_client.delete_by_query(
                index=self.index_name,
                body={
                    "query": {
                        "match_all": {}
                    }
                },
                refresh=True,
                timeout='120s',
                conflicts='proceed',  # é‡åˆ°ç‰ˆæœ¬è¡çªæ™‚ç¹¼çºŒåŸ·è¡Œ
                wait_for_completion=True
            )
            
            deleted_count = response.get('deleted', 0)
            version_conflicts = response.get('version_conflicts', 0)
            
            message = f"âœ… å·²æ¸…ç©ºçŸ¥è­˜åº«ï¼Œåˆªé™¤äº† {deleted_count} å€‹æ–‡æª”"
            if version_conflicts > 0:
                message += f"ï¼Œæœ‰ {version_conflicts} å€‹ç‰ˆæœ¬è¡çªå·²å¿½ç•¥"
            
            st.success(message)
            return True
            
        except Exception as e:
            error_msg = str(e)
            if '409' in error_msg or 'version_conflicts' in error_msg:
                st.warning(f"âš ï¸ æ¸…ç©ºéŽç¨‹ä¸­é‡åˆ°ç‰ˆæœ¬è¡çªï¼Œé‡è©¦ä¸­...")
                # é‡è©¦ä¸€æ¬¡ï¼Œä½¿ç”¨åˆ†æ‰¹åˆªé™¤
                try:
                    # å…ˆç²å–æ‰€æœ‰æ–‡æª”ID
                    search_response = sync_client.search(
                        index=self.index_name,
                        body={
                            "query": {"match_all": {}},
                            "_source": False,
                            "size": 1000
                        }
                    )
                    
                    doc_ids = [hit['_id'] for hit in search_response['hits']['hits']]
                    
                    if doc_ids:
                        # æ‰¹é‡åˆªé™¤
                        actions = [
                            {"delete": {"_index": self.index_name, "_id": doc_id}}
                            for doc_id in doc_ids
                        ]
                        
                        from elasticsearch.helpers import bulk
                        success_count, failed_items = bulk(
                            sync_client,
                            actions,
                            refresh=True,
                            ignore_status=[404, 409]  # å¿½ç•¥å·²åˆªé™¤å’Œç‰ˆæœ¬è¡çª
                        )
                        
                        st.success(f"âœ… é‡è©¦æˆåŠŸï¼Œæ¸…ç©ºäº†çŸ¥è­˜åº«ï¼ˆåˆªé™¤ {success_count} å€‹æ–‡æª”ï¼‰")
                        return True
                    else:
                        st.info("ðŸ“ çŸ¥è­˜åº«å·²ç¶“ç‚ºç©º")
                        return True
                        
                except Exception as retry_e:
                    st.error(f"âŒ é‡è©¦æ¸…ç©ºå¤±æ•—: {str(retry_e)}")
            else:
                st.error(f"âŒ æ¸…ç©ºçŸ¥è­˜åº«å¤±æ•—: {error_msg}")
            return False

    def _get_embed_dim(self) -> int:
        """è¦†è“‹çˆ¶é¡žæ–¹æ³•ï¼Œå¾ž Elasticsearch ç³»çµ±çš„åµŒå…¥æ¨¡åž‹ç²å–ç¶­åº¦"""
        try:
            # é¦–å…ˆå˜—è©¦å¾žé…ç½®ä¸­ç²å–
            if hasattr(self, 'elasticsearch_config') and self.elasticsearch_config:
                dim = self.elasticsearch_config.get('dimension')
                if dim:
                    return int(dim)
            
            # ç„¶å¾Œå˜—è©¦å¾žåµŒå…¥æ¨¡åž‹ç²å–
            if hasattr(self, 'embedding_model') and self.embedding_model:
                for attr in ("embed_dim", "_embed_dim", "dimension", "dim"):
                    if hasattr(self.embedding_model, attr):
                        val = getattr(self.embedding_model, attr)
                        try:
                            return int(val)
                        except Exception:
                            continue
            
            # æœ€å¾Œå¾ž Settings ç²å–
            model = Settings.embed_model
            if model:
                for attr in ("embed_dim", "_embed_dim", "dimension", "dim"):
                    if hasattr(model, attr):
                        val = getattr(model, attr)
                        try:
                            return int(val)
                        except Exception:
                            continue
            
            # å¾žé…ç½®æ–‡ä»¶ç²å–é»˜èªå€¼
            from config.config import ELASTICSEARCH_VECTOR_DIMENSION
            return ELASTICSEARCH_VECTOR_DIMENSION
            
        except Exception as e:
            print(f"âš ï¸ ç²å–åµŒå…¥ç¶­åº¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

    def update_conversation_feedback(self, conversation_id: str, rating: int = None, feedback: str = None) -> bool:
        """æ›´æ–°å°è©±åé¥‹"""
        if self.conversation_manager:
            return self.conversation_manager.update_conversation_feedback(
                conversation_id, rating, feedback
            )
        return False

    def refresh_index(self):
        """åˆªé™¤æ–‡æª”å¾Œåˆ·æ–°ç´¢å¼•"""
        try:
            # ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯é€²è¡Œç´¢å¼•åˆ·æ–°
            sync_client = getattr(self, 'sync_elasticsearch_client', None)
            if sync_client:
                print("ðŸ”„ æ­£åœ¨ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯åˆ·æ–°ESç´¢å¼•...")
                sync_client.indices.refresh(index=self.index_name)
                st.info("ðŸ”„ Elasticsearch ç´¢å¼•å·²åˆ·æ–°")
                print("âœ… ESç´¢å¼•åˆ·æ–°å®Œæˆ")
            else:
                st.warning("âš ï¸ åŒæ­¥ESå®¢æˆ¶ç«¯ä¸å¯ç”¨ï¼Œç„¡æ³•åˆ·æ–°ç´¢å¼•")
        except Exception as e:
            st.warning(f"ç´¢å¼•åˆ·æ–°è­¦å‘Š: {str(e)}")
            print(f"âŒ ç´¢å¼•åˆ·æ–°å¤±æ•—: {str(e)}")

    def __del__(self):
        """æžæ§‹å‡½æ•¸ï¼šæ¸…ç†è³‡æº"""
        try:
            if hasattr(self, 'elasticsearch_client') and self.elasticsearch_client:
                self.elasticsearch_client.close()
        except:
            pass
    
    def setup_query_engine(self):
        """è¨­ç½®æŸ¥è©¢å¼•æ“Ž - æ”¯æ´ ES æ··åˆæª¢ç´¢ (å‘é‡ + é—œéµå­—)"""
        if self.index:
            # ä½¿ç”¨è‡ªå®šç¾©çš„æ··åˆæª¢ç´¢å™¨
            if self.use_elasticsearch and self.elasticsearch_store:
                # å‰µå»ºæ··åˆæª¢ç´¢å™¨ (å‘é‡ + é—œéµå­—)
                retriever = self._create_hybrid_retriever()
                
                from llama_index.core.query_engine import RetrieverQueryEngine
                self.query_engine = RetrieverQueryEngine.from_args(
                    retriever=retriever,
                    response_mode="compact"
                )
                if SHOW_TECHNICAL_MESSAGES:
                    st.success("âœ… ä½¿ç”¨ ES æ··åˆæª¢ç´¢å¼•æ“Ž (å‘é‡æœå°‹ + é—œéµå­—æœå°‹)")
                self._store_system_status('hybrid_retrieval', True)
            else:
                # å›žé€€åˆ°æ¨™æº–æŸ¥è©¢å¼•æ“Ž
                self.query_engine = self.index.as_query_engine(
                    similarity_top_k=3,
                    response_mode="compact"
                )
                st.info("âœ… ä½¿ç”¨æ¨™æº–å‘é‡æª¢ç´¢å¼•æ“Ž")
    
    def _create_hybrid_retriever(self):
        """å‰µå»º ES æ··åˆæª¢ç´¢å™¨ (å‘é‡ç›¸ä¼¼åº¦ + BM25 é—œéµå­—)"""
        from llama_index.core.retrievers import BaseRetriever
        from llama_index.core.schema import QueryBundle, NodeWithScore
        from typing import List
        
        class ESHybridRetriever(BaseRetriever):
            def __init__(self, es_client, index_name, embedding_model, top_k=5):
                self.es_client = es_client
                self.index_name = index_name
                self.embedding_model = embedding_model
                self.top_k = top_k
                print(f"ðŸ”§ ESHybridRetrieveråˆå§‹åŒ–: ESå®¢æˆ¶ç«¯é¡žåž‹={type(es_client)}")
                print(f"ðŸ”§ ç´¢å¼•åç¨±: {index_name}, top_k: {top_k}")
                super().__init__()
            
            def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
                """æ··åˆæª¢ç´¢ï¼šçµåˆå‘é‡æœå°‹å’Œé—œéµå­—æœå°‹"""
                query_text = query_bundle.query_str
                print(f"ðŸ” é–‹å§‹ESæ··åˆæª¢ç´¢ï¼ŒæŸ¥è©¢: {query_text}")
                print(f"ðŸ”§ ESå®¢æˆ¶ç«¯é¡žåž‹: {type(self.es_client)}")
                
                try:
                    # 1. ç²å–æŸ¥è©¢çš„ embedding å‘é‡
                    print("ðŸ“Š æ­£åœ¨ç²å–æŸ¥è©¢å‘é‡...")
                    query_embedding = self.embedding_model._get_query_embedding(query_text)
                    print(f"âœ… æŸ¥è©¢å‘é‡ç¶­åº¦: {len(query_embedding) if query_embedding else 'None'}")
                    
                    # 2. ES æ··åˆæŸ¥è©¢ (å‘é‡ + é—œéµå­—) - ä½¿ç”¨ Elasticsearch 8.x èªžæ³•
                    hybrid_query = {
                        "size": self.top_k,
                        "knn": {
                            "field": "embedding",
                            "query_vector": query_embedding,
                            "k": self.top_k,
                            "num_candidates": self.top_k * 2
                        },
                        "query": {
                            "bool": {
                                "should": [
                                    # BM25 é—œéµå­—æœå°‹ (è©žå½™æœå°‹)
                                    {
                                        "match": {
                                            "content": {
                                                "query": query_text,
                                                "boost": 1.2  # ç¨å¾®æå‡é—œéµå­—æ¬Šé‡
                                            }
                                        }
                                    },
                                    # çŸ­èªžåŒ¹é…
                                    {
                                        "match_phrase": {
                                            "content": {
                                                "query": query_text,
                                                "boost": 1.5
                                            }
                                        }
                                    }
                                ],
                                "minimum_should_match": 1
                            }
                        },
                        "_source": ["content", "metadata"]
                    }
                    
                    # 3. åŸ·è¡ŒæŸ¥è©¢
                    print(f"ðŸ” åŸ·è¡ŒESæœå°‹ï¼Œç´¢å¼•: {self.index_name}")
                    print(f"ðŸ”§ æŸ¥è©¢çµæ§‹: {hybrid_query}")
                    
                    try:
                        response = self.es_client.search(
                            index=self.index_name,
                            body=hybrid_query
                        )
                        print(f"âœ… ESæŸ¥è©¢æˆåŠŸï¼ŒéŸ¿æ‡‰é¡žåž‹: {type(response)}")
                        
                        # æª¢æŸ¥ response æ˜¯å¦ç‚º awaitable
                        if hasattr(response, '__await__'):
                            print("âŒ éŒ¯èª¤ï¼šæ”¶åˆ°awaitable responseï¼Œä½†åœ¨åŒæ­¥ç’°å¢ƒä¸­")
                            raise Exception("åŒæ­¥å®¢æˆ¶ç«¯è¿”å›žäº†awaitable response")
                            
                    except Exception as search_error:
                        print(f"âŒ ESæœå°‹å¤±æ•—: {str(search_error)}")
                        print(f"ðŸ”§ æœå°‹éŒ¯èª¤é¡žåž‹: {type(search_error)}")
                        raise search_error
                    
                    # 4. è½‰æ›ç‚º NodeWithScore
                    nodes = []
                    hits = response.get('hits', {}).get('hits', [])
                    print(f"ðŸ“Š æ‰¾åˆ° {len(hits)} å€‹åŒ¹é…çµæžœ")
                    
                    for i, hit in enumerate(hits):
                        from llama_index.core.schema import TextNode
                        
                        # å‰µå»ºæ–‡æœ¬ç¯€é»ž
                        node = TextNode(
                            text=hit['_source']['content'],
                            metadata=hit['_source'].get('metadata', {}),
                            id_=hit['_id']
                        )
                        
                        # ES è©•åˆ† (çµåˆå‘é‡å’Œé—œéµå­—)
                        score = hit['_score']
                        
                        # å‰µå»ºè©•åˆ†ç¯€é»ž
                        node_with_score = NodeWithScore(
                            node=node,
                            score=score
                        )
                        nodes.append(node_with_score)
                    
                    st.info(f"ðŸ” ES æ··åˆæª¢ç´¢æ‰¾åˆ° {len(nodes)} å€‹ç›¸é—œæ–‡æª”")
                    return nodes
                    
                except Exception as e:
                    print(f"âŒ ESæ··åˆæª¢ç´¢å¤±æ•—ï¼ŒéŒ¯èª¤è©³æƒ…: {str(e)}")
                    print(f"ðŸ”§ éŒ¯èª¤é¡žåž‹: {type(e)}")
                    import traceback
                    print(f"ðŸ” å®Œæ•´éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
                    st.error(f"âŒ ES æ··åˆæª¢ç´¢å¤±æ•—: {str(e)}")
                    
                    # æª¢æŸ¥æ˜¯å¦ç‚ºasync/awaitéŒ¯èª¤
                    if "ObjectApiResponse" in str(e) or "await" in str(e) or "coroutine" in str(e):
                        print("ðŸš¨ æª¢æ¸¬åˆ°async/syncå®¢æˆ¶ç«¯éŒ¯èª¤ï¼ŒESå®¢æˆ¶ç«¯å¯èƒ½ä»ç‚ºç•°æ­¥")
                        print(f"ðŸ”§ ç•¶å‰ESå®¢æˆ¶ç«¯é¡žåž‹: {type(self.es_client)}")
                    
                    # å›žé€€åˆ°åŸºæœ¬å‘é‡æª¢ç´¢
                    return self._fallback_vector_search(query_bundle)
            
            def _fallback_vector_search(self, query_bundle):
                """å›žé€€åˆ°ç´”å‘é‡æœå°‹"""
                try:
                    query_embedding = self.embedding_model._get_query_embedding(query_bundle.query_str)
                    
                    vector_query = {
                        "size": self.top_k,
                        "knn": {
                            "field": "embedding",
                            "query_vector": query_embedding,
                            "k": self.top_k,
                            "num_candidates": self.top_k * 2
                        },
                        "_source": ["content", "metadata"]
                    }
                    
                    response = self.es_client.search(
                        index=self.index_name,
                        body=vector_query
                    )
                    
                    nodes = []
                    for hit in response['hits']['hits']:
                        from llama_index.core.schema import TextNode
                        
                        node = TextNode(
                            text=hit['_source']['content'],
                            metadata=hit['_source'].get('metadata', {}),
                            id_=hit['_id']
                        )
                        
                        node_with_score = NodeWithScore(
                            node=node,
                            score=hit['_score']
                        )
                        nodes.append(node_with_score)
                    
                    st.warning("âš ï¸ å›žé€€åˆ°ç´”å‘é‡æœå°‹")
                    return nodes
                    
                except Exception as e:
                    st.error(f"âŒ å‘é‡æœå°‹ä¹Ÿå¤±æ•—: {str(e)}")
                    return []
        
        # å‰µå»ºä¸¦è¿”å›žæ··åˆæª¢ç´¢å™¨ - ä½¿ç”¨çµ±ä¸€åŒæ­¥å®¢æˆ¶ç«¯
        if not hasattr(self, 'elasticsearch_client') or not self.elasticsearch_client:
            st.error("âŒ ESå®¢æˆ¶ç«¯æœªåˆå§‹åŒ–ï¼Œç„¡æ³•å‰µå»ºæŸ¥è©¢å¼•æ“Ž")
            return None
            
        print(f"ðŸ”§ å‰µå»ºESHybridRetrieverï¼Œä½¿ç”¨å®¢æˆ¶ç«¯é¡žåž‹: {type(self.elasticsearch_client)}")
            
        return ESHybridRetriever(
            es_client=self.elasticsearch_client,  # çµ±ä¸€ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯
            index_name=self.index_name,
            embedding_model=self.embedding_model,
            top_k=10  # Change the top_k value from 5 to 10
        )
    
    def _recreate_sync_elasticsearch_client(self) -> bool:
        """å®Œå…¨é‡æ–°å‰µå»ºåŒæ­¥ESå®¢æˆ¶ç«¯ï¼Œè§£æ±ºasync/awaitå•é¡Œ"""
        try:
            print("ðŸ”§ é–‹å§‹é‡æ–°å‰µå»ºåŒæ­¥ESå®¢æˆ¶ç«¯...")
            
            # å¼·åˆ¶ä½¿ç”¨æœ€åŸºç¤Žçš„åŒæ­¥é…ç½®
            from elasticsearch import Elasticsearch
            
            config = self.elasticsearch_config
            basic_config = {
                'hosts': [f"{config['scheme']}://{config['host']}:{config['port']}"],
                'request_timeout': 60,
                'max_retries': 1,
                'retry_on_timeout': False
            }
            
            if config.get('username') and config.get('password'):
                basic_config['basic_auth'] = (config['username'], config['password'])
            
            # å‰µå»ºæ–°çš„åŒæ­¥å®¢æˆ¶ç«¯
            new_sync_client = Elasticsearch(**basic_config)
            
            # æ¸¬è©¦é€£æŽ¥
            if new_sync_client.ping():
                print("âœ… æ–°åŒæ­¥å®¢æˆ¶ç«¯é€£æŽ¥æˆåŠŸ")
                # æ›´æ–°åŒæ­¥å®¢æˆ¶ç«¯å¼•ç”¨ï¼ˆä¸è¦è¦†è“‹ç•°æ­¥å®¢æˆ¶ç«¯ï¼‰
                self.sync_elasticsearch_client = new_sync_client
                
                # é‡æ–°å‰µå»ºå‘é‡å­˜å„² - ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯
                self.elasticsearch_store = ElasticsearchStore(
                    es_client=self.sync_elasticsearch_client,
                    index_name=self.index_name,
                    vector_field=self.elasticsearch_config['vector_field'],
                    text_field=self.elasticsearch_config['text_field'],
                    metadata_field='metadata',
                    embedding_dim=self.elasticsearch_config.get('dimension', 384)
                )
                print("âœ… å‘é‡å­˜å„²é‡æ–°å‰µå»ºæˆåŠŸï¼ˆä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯ï¼‰")
                return True
            else:
                print("âŒ æ–°åŒæ­¥å®¢æˆ¶ç«¯é€£æŽ¥å¤±æ•—")
                return False
                
        except Exception as e:
            print(f"âŒ é‡æ–°å‰µå»ºåŒæ­¥å®¢æˆ¶ç«¯å¤±æ•—: {str(e)}")
            return False
    
    def create_index(self, documents: List[Document]) -> VectorStoreIndex:
        """è¦†å¯«çˆ¶é¡žæ–¹æ³•ï¼Œå¼·åˆ¶ä½¿ç”¨ Elasticsearch"""
        if not documents:
            st.warning("âš ï¸ æ²’æœ‰æ–‡æª”éœ€è¦ç´¢å¼•")
            return None
        
        tracker = get_performance_tracker()
        
        with st.spinner("æ­£åœ¨ä½¿ç”¨ Elasticsearch å»ºç«‹ç´¢å¼•..."):
            try:
                with track_rag_stage(RAGStages.TOTAL_INDEXING_TIME, document_count=len(documents)):
                    # ç¢ºä¿ ES é€£æŽ¥å’Œæ¨¡åž‹å·²åˆå§‹åŒ–
                    if not self.elasticsearch_client:
                        st.error("âŒ Elasticsearch å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–ï¼Œå˜—è©¦é‡æ–°åˆå§‹åŒ–...")
                        if not self._setup_elasticsearch_client():
                            return None
                    
                    if not self.elasticsearch_store:
                        st.error("âŒ Elasticsearch å‘é‡å­˜å„²æœªè¨­ç½®ï¼Œå˜—è©¦é‡æ–°è¨­ç½®...")
                        if not self._create_elasticsearch_index():
                            return None
                        if not self._setup_elasticsearch_store():
                            return None
                        
                    self._ensure_models_initialized()
                    
                    # å»ºç«‹ storage context
                    with track_rag_stage(RAGStages.TEXT_CHUNKING):
                        storage_context = StorageContext.from_defaults(
                            vector_store=self.elasticsearch_store
                        )
                    
                    st.info(f"ðŸ“Š æº–å‚™å‘é‡åŒ– {len(documents)} å€‹æ–‡æª”åˆ° {self.index_name}")
                    
                    # æª¢æŸ¥æ–‡æª”å…§å®¹
                    for i, doc in enumerate(documents[:3]):
                        content_preview = doc.text[:100] + "..." if len(doc.text) > 100 else doc.text
                        print(f"ðŸ“„ æ–‡æª” {i+1}: {len(doc.text)} å­—ç¬¦")
                        print(f"   å…§å®¹é è¦½: {content_preview}")
                        if hasattr(doc, 'metadata') and doc.metadata:
                            print(f"   å…ƒæ•¸æ“š: {doc.metadata}")
                
                    # å‰µå»ºç´¢å¼•éšŽæ®µ
                    with track_rag_stage(RAGStages.EMBEDDING_GENERATION, vectors_to_create=len(documents)):
                        print(f"ðŸ”§ é–‹å§‹å‰µå»ºVectorStoreIndexï¼Œä½¿ç”¨ESå­˜å„²: {type(self.elasticsearch_store)}")
                        print(f"ðŸ”§ ESå®¢æˆ¶ç«¯é¡žåž‹: {type(self.elasticsearch_client)}")
                        print(f"ðŸ”§ åµŒå…¥æ¨¡åž‹é¡žåž‹: {type(self.embedding_model)}")
                        
                        try:
                            with track_rag_stage(RAGStages.INDEX_CREATION):
                                index = VectorStoreIndex.from_documents(
                                    documents, 
                                    storage_context=storage_context,
                                    embed_model=self.embedding_model
                                )
                            print("âœ… VectorStoreIndex.from_documents åŸ·è¡ŒæˆåŠŸ")
                        except Exception as index_error:
                            print(f"âŒ VectorStoreIndex.from_documents å¤±æ•—: {str(index_error)}")
                            print(f"âŒ éŒ¯èª¤é¡žåž‹: {type(index_error)}")
                            import traceback
                            print(f"âŒ å®Œæ•´éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
                            
                            # å¦‚æžœæ˜¯ HeadApiResponse éŒ¯èª¤ï¼Œå˜—è©¦æ›¿ä»£æ–¹æ¡ˆ
                            if "HeadApiResponse" in str(index_error) or "await" in str(index_error):
                                print("ðŸ”„ æª¢æ¸¬åˆ°HeadApiResponseéŒ¯èª¤ï¼Œå˜—è©¦é‡æ–°åˆå§‹åŒ–ESå®¢æˆ¶ç«¯...")
                                
                                # å®Œå…¨é‡æ–°å‰µå»º ES å®¢æˆ¶ç«¯å’Œå­˜å„²
                                if self._recreate_sync_elasticsearch_client():
                                    print("ðŸ”„ é‡æ–°å‰µå»ºstorage_context...")
                                    storage_context = StorageContext.from_defaults(
                                        vector_store=self.elasticsearch_store
                                    )
                                    
                                    print("ðŸ”„ é‡æ–°å˜—è©¦å‰µå»ºç´¢å¼•...")
                                    index = VectorStoreIndex.from_documents(
                                        documents, 
                                        storage_context=storage_context,
                                        embed_model=self.embedding_model
                                    )
                                    print("âœ… ä½¿ç”¨é‡æ–°å‰µå»ºçš„å®¢æˆ¶ç«¯æˆåŠŸå‰µå»ºç´¢å¼•")
                                else:
                                    raise index_error
                            else:
                                raise index_error
                
                # å¼·åˆ¶åˆ·æ–°ä¸¦é©—è­‰ï¼ˆä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯ï¼‰
                sync_client = getattr(self, 'sync_elasticsearch_client', None)
                if sync_client:
                    sync_client.indices.refresh(index=self.index_name)
                    stats = sync_client.indices.stats(index=self.index_name)
                    doc_count = stats['indices'][self.index_name]['total']['docs']['count']
                else:
                    doc_count = 0
                
                print(f"âœ… ESç´¢å¼•é©—è­‰: {doc_count} å€‹æ–‡æª”å·²æˆåŠŸç´¢å¼•åˆ° {self.index_name}")
                st.success(f"âœ… æˆåŠŸç´¢å¼• {doc_count} å€‹æ–‡æª”åˆ° Elasticsearch")
                
                # æ›´æ–°çµ±è¨ˆ
                self.memory_stats['documents_processed'] = len(documents)
                self.memory_stats['vectors_stored'] = doc_count
                
                return index
                
            except Exception as e:
                st.error(f"âŒ Elasticsearch ç´¢å¼•å»ºç«‹å¤±æ•—: {str(e)}")
                print(f"âŒ è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
                return None
    
    def _get_mapping_choice_for_first_startup(self) -> Optional[str]:
        """ç‚ºç¬¬ä¸€æ¬¡å•Ÿå‹•ç²å– mapping é¸æ“‡
        
        Returns:
            str: é¸æ“‡çš„ mapping æ–‡ä»¶åï¼Œå¦‚æžœä½¿ç”¨é»˜èªå‰‡è¿”å›ž None
        """
        try:
            from config.elasticsearch.mapping_loader import ElasticsearchMappingLoader
            loader = ElasticsearchMappingLoader()
            available_mappings = loader.list_available_mappings()
            
            # å¦‚æžœåªæœ‰ä¸€å€‹é»˜èªé…ç½®æ–‡ä»¶ï¼Œç›´æŽ¥ä½¿ç”¨
            if len(available_mappings) <= 1:
                return "index_mapping.json"
            
            # ç°¡åŒ–é¸æ“‡ï¼šå„ªå…ˆä½¿ç”¨é»˜èªé…ç½®
            # åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­ï¼Œå¯ä»¥é€šéŽç’°å¢ƒè®Šæ•¸æˆ–é…ç½®æ–‡ä»¶æŒ‡å®šç‰¹å®šçš„ mapping
            default_mapping = "index_mapping.json"
            if default_mapping in available_mappings:
                print(f"ðŸ“‹ ç¬¬ä¸€æ¬¡å•Ÿå‹•ä½¿ç”¨é»˜èª mapping: {default_mapping}")
                return default_mapping
            
            # å¦‚æžœé»˜èªæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¬¬ä¸€å€‹å¯ç”¨çš„
            if available_mappings:
                selected = available_mappings[0]
                print(f"ðŸ“‹ ç¬¬ä¸€æ¬¡å•Ÿå‹•ä½¿ç”¨å¯ç”¨ mapping: {selected}")
                return selected
                
            return None
            
        except Exception as e:
            print(f"âš ï¸ ç²å– mapping é¸æ“‡å¤±æ•—: {e}")
            return None
    
    def _log_first_startup_config(self, config: Dict[str, Any], mapping_file: str):
        """è¨˜éŒ„ç¬¬ä¸€æ¬¡å•Ÿå‹•çš„é…ç½®ä¿¡æ¯
        
        Args:
            config: Elasticsearch é…ç½®
            mapping_file: ä½¿ç”¨çš„ mapping æ–‡ä»¶å
        """
        try:
            startup_info = {
                "timestamp": datetime.now().isoformat(),
                "elasticsearch_config": {
                    "host": config.get('host'),
                    "port": config.get('port'),
                    "index_name": config.get('index_name'),
                    "dimension": config.get('dimension'),
                    "shards": config.get('shards'),
                    "replicas": config.get('replicas'),
                    "similarity": config.get('similarity')
                },
                "mapping_file": mapping_file,
                "system_info": {
                    "embedding_provider": config.get('embedding_provider', 'unknown'),
                    "llm_model": config.get('llm_model', 'unknown')
                }
            }
            
            # è¨˜éŒ„åˆ°æŽ§åˆ¶å°
            print(f"ðŸ“‹ ç¬¬ä¸€æ¬¡å•Ÿå‹•é…ç½®è¨˜éŒ„:")
            print(f"   æ™‚é–“: {startup_info['timestamp']}")
            print(f"   ESä¸»æ©Ÿ: {config.get('host')}:{config.get('port')}")
            print(f"   ç´¢å¼•åç¨±: {config.get('index_name')}")
            print(f"   å‘é‡ç¶­åº¦: {config.get('dimension')}")
            print(f"   Mappingæ–‡ä»¶: {mapping_file}")
            print(f"   åˆ†ç‰‡é…ç½®: {config.get('shards')} åˆ†ç‰‡, {config.get('replicas')} å‰¯æœ¬")
            
            # å¯é¸ï¼šä¿å­˜åˆ°æ–‡ä»¶ï¼ˆå¦‚æžœéœ€è¦æŒä¹…åŒ–è¨˜éŒ„ï¼‰
            try:
                import os
                log_dir = os.path.join("data", "logs")
                os.makedirs(log_dir, exist_ok=True)
                
                log_file = os.path.join(log_dir, "elasticsearch_startup.log")
                with open(log_file, 'a', encoding='utf-8') as f:
                    f.write(f"{json.dumps(startup_info, indent=2, ensure_ascii=False)}\n")
                    f.write("-" * 50 + "\n")
                
                print(f"ðŸ“„ å•Ÿå‹•è¨˜éŒ„å·²ä¿å­˜åˆ°: {log_file}")
                
            except Exception as log_save_error:
                print(f"âš ï¸ ç„¡æ³•ä¿å­˜å•Ÿå‹•è¨˜éŒ„åˆ°æ–‡ä»¶: {log_save_error}")
            
        except Exception as e:
            print(f"âš ï¸ è¨˜éŒ„ç¬¬ä¸€æ¬¡å•Ÿå‹•é…ç½®å¤±æ•—: {e}")