import os
import json
from typing import List, Dict, Any, Optional
import streamlit as st
from datetime import datetime
import traceback

# LlamaIndex æ ¸å¿ƒ
from llama_index.core import VectorStoreIndex, Document, Settings
from llama_index.core.storage.storage_context import StorageContext
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.postprocessor import SimilarityPostprocessor

# å°è©±è¨˜éŒ„ç®¡ç†
from src.storage.conversation_history import ConversationHistoryManager

# Elasticsearch integration
try:
    from elasticsearch import Elasticsearch
    ELASTICSEARCH_AVAILABLE = True
except ImportError:
    ELASTICSEARCH_AVAILABLE = False
    st.warning("âš ï¸ Elasticsearch dependencies not installed. Install with: pip install elasticsearch")

# ç¹¼æ‰¿å¢å¼·ç‰ˆç³»çµ±
from .enhanced_rag_system import EnhancedRAGSystem
from config.config import (
    GROQ_API_KEY, EMBEDDING_MODEL, LLM_MODEL, 
    ELASTICSEARCH_HOST, ELASTICSEARCH_PORT, ELASTICSEARCH_SCHEME,
    ELASTICSEARCH_INDEX_NAME, ELASTICSEARCH_USERNAME, ELASTICSEARCH_PASSWORD,
    ELASTICSEARCH_TIMEOUT, ELASTICSEARCH_MAX_RETRIES, ELASTICSEARCH_VERIFY_CERTS,
    ELASTICSEARCH_SHARDS, ELASTICSEARCH_REPLICAS, ELASTICSEARCH_VECTOR_DIMENSION,
    ELASTICSEARCH_SIMILARITY
)

class ElasticsearchRAGSystem(EnhancedRAGSystem):
    """Elasticsearch RAG ç³»çµ± - é«˜æ•ˆèƒ½ã€å¯æ“´å±•çš„å‘é‡æª¢ç´¢"""
    
    def __init__(self, elasticsearch_config: Optional[Dict] = None):
        """åˆå§‹åŒ– Elasticsearch RAG ç³»çµ±"""
        # é¦–å…ˆè¨­ç½® elasticsearch_configï¼Œé¿å…åœ¨çˆ¶é¡åˆå§‹åŒ–æ™‚å¼•ç”¨éŒ¯èª¤
        self.elasticsearch_config = elasticsearch_config or self._get_default_config()
        self.elasticsearch_client = None
        self.elasticsearch_store = None
        
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ç´¢å¼•åç¨±
        from config.config import ELASTICSEARCH_INDEX_NAME
        self.index_name = ELASTICSEARCH_INDEX_NAME
        
        # è¨˜æ†¶é«”ä½¿ç”¨ç›£æ§
        self.memory_stats = {
            'documents_processed': 0,
            'vectors_stored': 0,
            'peak_memory_mb': 0
        }
        
        # æ¨¡å‹å¯¦ä¾‹
        self.embedding_model = None
        self.llm_model = None
        
        # åˆå§‹åŒ–å°è©±è¨˜éŒ„ç®¡ç†å™¨
        self.conversation_manager = ConversationHistoryManager(elasticsearch_config)
        
        # èª¿ç”¨çˆ¶é¡åˆå§‹åŒ–ï¼Œä½†ç¦ç”¨å…¶ Elasticsearch è‡ªå‹•åˆå§‹åŒ–
        super().__init__(use_elasticsearch=False, use_chroma=False)  # å…ˆè¨­ç½®ç‚º False
        
        # ç„¶å¾Œæ‰‹å‹•è¨­ç½®æ¨™èªŒä¸¦åˆå§‹åŒ– Elasticsearch 
        self.use_elasticsearch = True
        if self._initialize_elasticsearch():
            # å¦‚æœåˆå§‹åŒ–æˆåŠŸï¼Œå˜—è©¦è¼‰å…¥ç¾æœ‰ç´¢å¼•
            self.load_existing_index()
    
    def _initialize_elasticsearch(self):
        """åˆå§‹åŒ– Elasticsearch é€£æ¥å’Œå‘é‡å­˜å„²"""
        try:
            if self._setup_elasticsearch_client():
                if self._create_elasticsearch_index():
                    if self._setup_elasticsearch_store():
                        st.success("âœ… Elasticsearch RAG ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
                        # ç¢ºä¿ use_elasticsearch æ¨™èªŒæ­£ç¢ºè¨­ç½®
                        self.use_elasticsearch = True
                        return True
                    else:
                        st.error("âŒ Elasticsearch å‘é‡å­˜å„²è¨­ç½®å¤±æ•—")
                        self.use_elasticsearch = False
                else:
                    st.error("âŒ Elasticsearch ç´¢å¼•å‰µå»ºå¤±æ•—")
                    self.use_elasticsearch = False
            else:
                st.error("âŒ Elasticsearch å®¢æˆ¶ç«¯é€£æ¥å¤±æ•—")
                self.use_elasticsearch = False
            return False
        except Exception as e:
            st.error(f"âŒ Elasticsearch åˆå§‹åŒ–å¤±æ•—: {str(e)}")
            self.use_elasticsearch = False
            return False
    
    def _ensure_models_initialized(self):
        """ç¢ºä¿æ¨¡å‹å·²åˆå§‹åŒ–ä¸¦å­˜å„²ç‚ºå¯¦ä¾‹å±¬æ€§"""
        if not self.models_initialized or self.embedding_model is None:
            super()._ensure_models_initialized()
            
            # å¾ Settings ç²å–æ¨¡å‹ä¸¦å­˜å„²ç‚ºå¯¦ä¾‹å±¬æ€§
            from llama_index.core import Settings
            self.embedding_model = Settings.embed_model
            self.llm_model = Settings.llm
            
            st.info("âœ… Elasticsearch RAG ç³»çµ±æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def _get_default_config(self) -> Dict:
        """ç²å–é è¨­ Elasticsearch é…ç½®"""
        return {
            'host': ELASTICSEARCH_HOST or 'localhost',
            'port': ELASTICSEARCH_PORT or 9200,
            'scheme': ELASTICSEARCH_SCHEME or 'http',
            'username': ELASTICSEARCH_USERNAME,
            'password': ELASTICSEARCH_PASSWORD,
            'timeout': ELASTICSEARCH_TIMEOUT or 30,
            'max_retries': ELASTICSEARCH_MAX_RETRIES or 3,
            'verify_certs': ELASTICSEARCH_VERIFY_CERTS,
            'index_name': ELASTICSEARCH_INDEX_NAME or 'rag_intelligent_assistant',
            'shards': ELASTICSEARCH_SHARDS or 1,
            'replicas': ELASTICSEARCH_REPLICAS or 0,
            'dimension': ELASTICSEARCH_VECTOR_DIMENSION or 1024,
            'similarity': ELASTICSEARCH_SIMILARITY or 'cosine',
            'text_field': 'content',
            'vector_field': 'embedding',
            'metadata_fields': ['source', 'page', 'chunk_id', 'timestamp', 'file_type', 'file_size']
        }
    
    def _setup_elasticsearch_client(self) -> bool:
        """è¨­ç½® Elasticsearch å®¢æˆ¶ç«¯ï¼ˆçµ±ä¸€ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯ï¼‰"""
        if not ELASTICSEARCH_AVAILABLE:
            st.error("âŒ Elasticsearch ä¾è³´æœªå®‰è£")
            return False
            
        try:
            config = self.elasticsearch_config
            
            # çµ±ä¸€ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯ - LlamaIndex ElasticsearchStore éœ€è¦åŒæ­¥å®¢æˆ¶ç«¯
            from elasticsearch import Elasticsearch
            
            # å»ºç«‹é€£æ¥é…ç½® - ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯
            es_config = {
                'hosts': [{
                    'host': config['host'],
                    'port': config['port'],
                    'scheme': config['scheme']
                }],
                'request_timeout': config['timeout'],
                'max_retries': 3,
                'retry_on_timeout': True
            }
            
            # æ·»åŠ é©—è­‰ä¿¡æ¯ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            if config.get('username') and config.get('password'):
                es_config['basic_auth'] = (config['username'], config['password'])
            
            # å‰µå»ºåŒæ­¥å®¢æˆ¶ç«¯ï¼ˆElasticsearchStore è¦æ±‚ï¼‰
            sync_client = Elasticsearch(**es_config)
            
            # æ¸¬è©¦é€£æ¥
            if sync_client.ping():
                st.success(f"âœ… æˆåŠŸé€£æ¥åˆ° Elasticsearch: {config['host']}:{config['port']}")
                
                # é¡¯ç¤ºé›†ç¾¤ä¿¡æ¯
                try:
                    cluster_info = sync_client.info()
                    st.info(f"ğŸ“Š ES é›†ç¾¤ç‰ˆæœ¬: {cluster_info.get('version', {}).get('number', 'unknown')}")
                except:
                    pass
                
                # çµ±ä¸€ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯
                self.elasticsearch_client = sync_client
                self.sync_elasticsearch_client = sync_client
                
                print(f"âœ… ESå®¢æˆ¶ç«¯åˆå§‹åŒ–å®Œæˆï¼Œé¡å‹: {type(self.elasticsearch_client)}")
                
                return True
            else:
                st.error("âŒ ç„¡æ³•é€£æ¥åˆ° Elasticsearch")
                return False
                
        except Exception as e:
            st.error(f"âŒ Elasticsearch å®¢æˆ¶ç«¯è¨­ç½®å¤±æ•—: {str(e)}")
            return False
    
    def _create_elasticsearch_index(self) -> bool:
        """å‰µå»º Elasticsearch ç´¢å¼•"""
        try:
            config = self.elasticsearch_config
            
            # é©—è­‰/å°é½ŠåµŒå…¥ç¶­åº¦
            try:
                # ä½¿ç”¨ EnhancedRAGSystem çš„ç¶­åº¦æª¢æ¸¬
                actual_dim = self._get_embed_dim()
            except Exception:
                actual_dim = None
            expected_dim = config.get('dimension')
            
            if actual_dim is not None and expected_dim and int(actual_dim) != int(expected_dim):
                st.warning(f"âš ï¸ æª¢æ¸¬åˆ°æ¨¡å‹ç¶­åº¦ {actual_dim} èˆ‡é…ç½®ç¶­åº¦ {expected_dim} ä¸ä¸€è‡´ï¼Œå°‡ä»¥æ¨¡å‹ç¶­åº¦ç‚ºæº–ã€‚")
                config['dimension'] = int(actual_dim)
            
            # æœ€çµ‚ç¶­åº¦é©—è­‰
            if not self._validate_embedding_dimension(config['dimension']):
                st.error("âŒ åµŒå…¥ç¶­åº¦é©—è­‰å¤±æ•—ï¼Œåœæ­¢å»ºç«‹ç´¢å¼•ã€‚")
                return False
            
            # ç´¢å¼•æ˜ å°„è¨­å®š (ä½¿ç”¨ä¸­æ–‡åˆ†æå™¨)
            index_mapping = {
                "settings": {
                    "number_of_shards": config['shards'],
                    "number_of_replicas": config['replicas'],
                    "analysis": {
                        "analyzer": {
                            "chinese_analyzer": {
                                "type": "custom",
                                "tokenizer": "standard",
                                "filter": ["lowercase", "cjk_width", "cjk_bigram"]
                            }
                        }
                    }
                },
                "mappings": {
                    "properties": {
                        config['text_field']: {
                            "type": "text",
                            "analyzer": "chinese_analyzer",
                            "search_analyzer": "chinese_analyzer"
                        },
                        config['vector_field']: {
                            "type": "dense_vector",
                            "dims": config['dimension'],
                            "index": True,
                            "similarity": config['similarity']
                        },
                        "metadata": {
                            "type": "object",
                            "properties": {
                                "source": {"type": "keyword"},
                                "page": {"type": "integer"},
                                "chunk_id": {"type": "keyword"},
                                "timestamp": {"type": "date"},
                                "file_type": {"type": "keyword"},
                                "file_size": {"type": "integer"}
                            }
                        }
                    }
                }
            }
            
            # æª¢æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨ï¼ˆä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯ï¼‰
            sync_client = getattr(self, 'sync_elasticsearch_client', None)
            if not sync_client:
                # å¦‚æœæ²’æœ‰åŒæ­¥å®¢æˆ¶ç«¯ï¼Œå‰µå»ºä¸€å€‹
                from elasticsearch import Elasticsearch
                sync_client = Elasticsearch(**{
                    'hosts': [{'host': config['host'], 'port': config['port'], 'scheme': config['scheme']}],
                    'request_timeout': config['timeout'],
                })
                self.sync_elasticsearch_client = sync_client
            
            if sync_client.indices.exists(index=self.index_name):
                st.info(f"ğŸ“š ç´¢å¼• '{self.index_name}' å·²å­˜åœ¨")
                return True
            
            # å‰µå»ºç´¢å¼•ï¼ˆä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯ï¼‰
            try:
                st.info(f"ğŸ”§ æ­£åœ¨åˆ›å»ºç´¢å¼•: {self.index_name}")
                response = sync_client.indices.create(
                    index=self.index_name,
                    body=index_mapping,
                    ignore=400  # å¿½ç•¥å·²å­˜åœ¨çš„éŒ¯èª¤
                )
                
                if response.get('acknowledged', False):
                    st.success(f"âœ… æˆåŠŸåˆ›å»ºç´¢å¼•: {self.index_name}")
                    # é©—è­‰ç´¢å¼•åˆ›å»º
                    if sync_client.indices.exists(index=self.index_name):
                        st.info("ğŸ“‹ ç´¢å¼•åˆ›å»ºéªŒè¯é€šè¿‡")
                    return True
                else:
                    st.error(f"âŒ ç´¢å¼•å‰µå»ºå¤±æ•—: {response}")
                    return False
                    
            except Exception as create_error:
                # å¦‚æœæ˜¯ HeadApiResponse async éŒ¯èª¤ï¼Œå˜—è©¦åŒæ­¥æ–¹å¼
                error_msg = str(create_error)
                if "HeadApiResponse" in error_msg or "await" in error_msg:
                    st.warning(f"âš ï¸ æª¢æ¸¬åˆ°asyncå…¼å®¹æ€§å•é¡Œï¼Œå˜—è©¦åŒæ­¥æ–¹å¼å‰µå»ºç´¢å¼•...")
                    try:
                        # ä½¿ç”¨åŒæ­¥ Elasticsearch å®¢æˆ¶ç«¯é‡æ–°åˆå§‹åŒ–
                        from elasticsearch import Elasticsearch
                        sync_client = Elasticsearch(
                            [{'host': self.elasticsearch_config['host'], 'port': self.elasticsearch_config['port']}],
                            timeout=30,
                            request_timeout=30
                        )
                        
                        # æ¸¬è©¦é€£æ¥
                        if sync_client.ping():
                            # ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯å‰µå»ºç´¢å¼•
                            response = sync_client.indices.create(
                                index=self.index_name,
                                body=index_mapping,
                                ignore=400
                            )
                            # æ›´æ–°å®¢æˆ¶ç«¯ç‚ºåŒæ­¥ç‰ˆæœ¬
                            self.elasticsearch_client = sync_client
                            st.success(f"âœ… ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯æˆåŠŸå‰µå»ºç´¢å¼•: {self.index_name}")
                            return True
                        else:
                            st.error("âŒ åŒæ­¥å®¢æˆ¶ç«¯ç„¡æ³•é€£æ¥åˆ° Elasticsearch")
                            return False
                    except Exception as sync_error:
                        st.error(f"âŒ åŒæ­¥å‰µå»ºç´¢å¼•ä¹Ÿå¤±æ•—: {str(sync_error)}")
                        return False
                else:
                    st.error(f"âŒ å‰µå»ºç´¢å¼•å¤±æ•—: {error_msg}")
                    return False
        except Exception as e:
            st.error(f"âŒ å‰µå»ºç´¢å¼•æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return False
                
    def query(self, query_str: str, **kwargs) -> str:
        """åŸ·è¡ŒæŸ¥è©¢ä¸¦è¿”å›çµæœå­—ç¬¦ä¸²"""
        if not self.query_engine:
            return "âŒ æŸ¥è©¢å¼•æ“å°šæœªè¨­ç½®ã€‚è«‹å…ˆä¸Šå‚³ä¸¦ç´¢å¼•æ–‡æª”ã€‚"
        
        try:
            print(f"ğŸ” é–‹å§‹åŸ·è¡ŒæŸ¥è©¢: {query_str}")
            print(f"ğŸ”§ æŸ¥è©¢å¼•æ“é¡å‹: {type(self.query_engine)}")
            
            response = self.query_engine.query(query_str)
            
            print(f"âœ… æŸ¥è©¢å®Œæˆï¼ŒéŸ¿æ‡‰é¡å‹: {type(response)}")
            return str(response)
            
        except Exception as e:
            error_msg = str(e)
            error_type = type(e).__name__
            
            print(f"âŒ æŸ¥è©¢éŒ¯èª¤è©³æƒ…:")
            print(f"   éŒ¯èª¤é¡å‹: {error_type}")
            print(f"   éŒ¯èª¤æ¶ˆæ¯: {error_msg}")
            
            # æª¢æŸ¥æ˜¯å¦ç‚º ObjectApiResponse éŒ¯èª¤
            if "ObjectApiResponse" in error_msg or "await" in error_msg:
                print("ğŸš¨ æª¢æ¸¬åˆ°ObjectApiResponseéŒ¯èª¤ï¼")
                print(f"   æŸ¥è©¢å¼•æ“: {type(self.query_engine)}")
                if hasattr(self.query_engine, '_retriever'):
                    print(f"   æª¢ç´¢å™¨: {type(self.query_engine._retriever)}")
                
            import traceback
            print(f"ğŸ” å®Œæ•´éŒ¯èª¤å †ç–Š:")
            print(traceback.format_exc())
            
            st.error(f"æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {error_msg}")
            st.write(traceback.format_exc())
            return f"æŸ¥è©¢å¤±æ•—: {error_msg}"
    
    def query_with_sources(self, query_str: str, save_to_history: bool = True, session_id: str = None, user_id: str = None, **kwargs) -> Dict[str, Any]:
        """åŸ·è¡ŒæŸ¥è©¢ä¸¦è¿”å›å¸¶æœ‰ä¾†æºä¿¡æ¯çš„å®Œæ•´çµæœ"""
        if not self.query_engine:
            return {
                "answer": "âŒ æŸ¥è©¢å¼•æ“å°šæœªè¨­ç½®ã€‚è«‹å…ˆä¸Šå‚³ä¸¦ç´¢å¼•æ–‡æª”ã€‚",
                "sources": [],
                "metadata": {}
            }
        
        start_time = datetime.now()
        
        try:
            print(f"ğŸ” é–‹å§‹åŸ·è¡Œå¸¶ä¾†æºçš„æŸ¥è©¢: {query_str}")
            print(f"ğŸ”§ æŸ¥è©¢å¼•æ“é¡å‹: {type(self.query_engine)}")
            
            response = self.query_engine.query(query_str)
            
            print(f"âœ… æŸ¥è©¢å®Œæˆï¼ŒéŸ¿æ‡‰é¡å‹: {type(response)}")
            
            # æå–ç­”æ¡ˆ
            answer = str(response)
            
            # æå–ä¾†æºä¿¡æ¯
            sources = []
            if hasattr(response, 'source_nodes') and response.source_nodes:
                print(f"ğŸ“š æ‰¾åˆ° {len(response.source_nodes)} å€‹ä¾†æºç¯€é»")
                for i, node in enumerate(response.source_nodes):
                    source_info = {
                        "content": node.node.text[:200] + "..." if len(node.node.text) > 200 else node.node.text,
                        "source": node.node.metadata.get("source", "æœªçŸ¥ä¾†æº"),
                        "file_path": node.node.metadata.get("file_path", ""),
                        "score": float(node.score) if hasattr(node, 'score') else 0.0,
                        "page": node.node.metadata.get("page", ""),
                        "type": node.node.metadata.get("type", "user_document")
                    }
                    sources.append(source_info)
                    print(f"  [{i+1}] ä¾†æº: {source_info['source']}, è©•åˆ†: {source_info['score']}")
            else:
                print("âŒ éŸ¿æ‡‰ä¸­æ²’æœ‰æ‰¾åˆ°ä¾†æºç¯€é»")
            
            # è¨ˆç®—éŸ¿æ‡‰æ™‚é–“
            response_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            
            metadata = {
                "query": query_str,
                "total_sources": len(sources),
                "response_time_ms": response_time_ms,
                "model": "Groq LLama 3.3",
                "backend": "Elasticsearch"
            }
            
            result = {
                "answer": answer,
                "sources": sources,
                "metadata": metadata
            }
            
            # ä¿å­˜åˆ°å°è©±è¨˜éŒ„
            if save_to_history and self.conversation_manager:
                try:
                    conversation_id = self.conversation_manager.save_conversation(
                        question=query_str,
                        answer=answer,
                        sources=sources,
                        metadata=metadata,
                        session_id=session_id,
                        user_id=user_id
                    )
                    if conversation_id:
                        result["conversation_id"] = conversation_id
                        print(f"ğŸ’¾ å°è©±è¨˜éŒ„å·²ä¿å­˜: {conversation_id}")
                except Exception as save_error:
                    print(f"âš ï¸ ä¿å­˜å°è©±è¨˜éŒ„å¤±æ•—: {str(save_error)}")
            
            return result
            
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ å¸¶ä¾†æºæŸ¥è©¢å¤±æ•—: {error_msg}")
            import traceback
            print(f"ğŸ” å®Œæ•´éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
            
            return {
                "answer": f"æŸ¥è©¢å¤±æ•—: {error_msg}",
                "sources": [],
                "metadata": {"error": error_msg}
            }
                
    def _setup_elasticsearch_store(self) -> bool:
        """è¨­ç½® Elasticsearch å‘é‡å­˜å„²"""
        try:
            # ä½¿ç”¨çµ±ä¸€çš„åŒæ­¥å®¢æˆ¶ç«¯
            if not hasattr(self, 'elasticsearch_client') or not self.elasticsearch_client:
                st.error("âŒ Elasticsearch å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–")
                return False
            
            print(f"ğŸ”§ è¨­ç½®ElasticsearchStoreï¼Œå®¢æˆ¶ç«¯é¡å‹: {type(self.elasticsearch_client)}")
            
            # ä½¿ç”¨è‡ªå®šç¾©åŒæ­¥ Elasticsearch Store é¿å… async å•é¡Œ
            from ..storage.custom_elasticsearch_store import CustomElasticsearchStore
            self.elasticsearch_store = CustomElasticsearchStore(
                es_client=self.elasticsearch_client,
                index_name=self.index_name,
                vector_field=self.elasticsearch_config['vector_field'],
                text_field=self.elasticsearch_config['text_field'],
                metadata_field='metadata'
            )
            
            st.success("âœ… Elasticsearch å‘é‡å­˜å„²è¨­ç½®å®Œæˆ (ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯)")
            return True
                
        except Exception as e:
            st.error(f"âŒ Elasticsearch å‘é‡å­˜å„²è¨­ç½®å¤±æ•—: {str(e)}")
            import traceback
            st.error(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
            return False
    
    def load_existing_index(self) -> bool:
        """è¼‰å…¥ç¾æœ‰çš„ Elasticsearch ç´¢å¼•"""
        try:
            # è¨­ç½® Elasticsearch å®¢æˆ¶ç«¯
            if not self._setup_elasticsearch_client():
                st.error("âŒ ç„¡æ³•é€£æ¥åˆ° Elasticsearch")
                return False
            
            # æª¢æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨ï¼ˆä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯ï¼‰
            sync_client = getattr(self, 'sync_elasticsearch_client', None)
            if sync_client and sync_client.indices.exists(index=self.index_name):
                # ç²å–ç´¢å¼•çµ±è¨ˆè³‡è¨Š
                stats = sync_client.indices.stats(index=self.index_name)
                doc_count = stats['indices'][self.index_name]['total']['docs']['count']
                
                if doc_count > 0:
                    # é©—è­‰ç´¢å¼•ç¶­åº¦èˆ‡åµŒå…¥æ¨¡å‹
                    try:
                        mapping = sync_client.indices.get_mapping(index=self.index_name)
                        props = mapping[self.index_name]['mappings']['properties']
                        vec_field = self.elasticsearch_config['vector_field']
                        index_dims = props.get(vec_field, {}).get('dims')
                    except Exception:
                        index_dims = None

                    model_dim = None
                    try:
                        model_dim = self._get_embed_dim()
                    except Exception:
                        pass

                    if index_dims is not None and model_dim is not None and int(index_dims) != int(model_dim):
                        st.error(f"âŒ åµŒå…¥ç¶­åº¦ä¸ä¸€è‡´ï¼šç´¢å¼•ç‚º {index_dims}ï¼Œæ¨¡å‹ç‚º {model_dim}ã€‚è«‹å°é½Šå¾Œé‡è©¦ã€‚")
                        return False

                    # è¨­ç½®å‘é‡å­˜å„²
                    if self._setup_elasticsearch_store():
                        # ç¢ºä¿æ¨¡å‹åˆå§‹åŒ–
                        self._ensure_models_initialized()
                        
                        # é‡æ–°å‰µå»ºç´¢å¼•å°è±¡
                        storage_context = StorageContext.from_defaults(vector_store=self.elasticsearch_store)
                        self.index = VectorStoreIndex.from_vector_store(
                            vector_store=self.elasticsearch_store,
                            storage_context=storage_context,
                            embed_model=self.embedding_model
                        )
                        
                        # è¨­ç½®æŸ¥è©¢å¼•æ“
                        self.setup_query_engine()
                        return True
                    else:
                        st.error("âŒ Elasticsearch å‘é‡å­˜å„²è¨­ç½®å¤±æ•—")
                        return False
                else:
                    st.info(f"ğŸ“š ç´¢å¼• '{self.index_name}' å­˜åœ¨ä½†ç‚ºç©º")
                    return False
            else:
                st.info("ğŸ’¡ æ²’æœ‰ç™¼ç¾ç¾æœ‰çš„ Elasticsearch ç´¢å¼•ï¼Œè«‹ä¸Šå‚³æ–‡æª”ä¾†å»ºç«‹æ–°ç´¢å¼•")
                return False
                
        except Exception as e:
            st.error(f"âŒ è¼‰å…¥ Elasticsearch ç´¢å¼•å¤±æ•—: {str(e)}")
            return False
    
    def get_enhanced_statistics(self) -> Dict[str, Any]:
        """ç²å– Elasticsearch RAG ç³»çµ±çš„çµ±è¨ˆè³‡è¨Š"""
        try:
            stats = {
                "system_type": "elasticsearch",
                "base_statistics": self.memory_stats.copy(),
                "elasticsearch_stats": {}
            }
            
            # ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯é€²è¡Œçµ±è¨ˆæŸ¥è©¢
            sync_client = getattr(self, 'sync_elasticsearch_client', None)
            if sync_client and self.index_name:
                try:
                    # å…ˆåˆ·æ–°ç´¢å¼•ç¢ºä¿æœ€æ–°æ•¸æ“š
                    sync_client.indices.refresh(index=self.index_name)
                    
                    # ç²å–ç´¢å¼•çµ±è¨ˆ
                    index_stats = sync_client.indices.stats(index=self.index_name)
                    total_stats = index_stats['indices'][self.index_name]['total']
                    
                    stats["elasticsearch_stats"] = {
                        "index_name": self.index_name,
                        "document_count": total_stats['docs']['count'],
                        "index_size_bytes": total_stats['store']['size_in_bytes'],
                        "index_size_mb": round(total_stats['store']['size_in_bytes'] / 1024 / 1024, 2)
                    }
                    
                    # æ›´æ–°åŸºç¤çµ±è¨ˆ
                    stats["base_statistics"]["total_documents"] = total_stats['docs']['count']
                    stats["base_statistics"]["total_nodes"] = total_stats['docs']['count']
                    
                    # è¨˜éŒ„è©³ç´°çµ±è¨ˆæ—¥èªŒ
                    print(f"ğŸ“Š ESçµ±è¨ˆæ›´æ–°: ç´¢å¼•={self.index_name}, æ–‡æª”æ•¸={total_stats['docs']['count']}")
                    
                except Exception as e:
                    st.warning(f"ç„¡æ³•ç²å– Elasticsearch çµ±è¨ˆ: {e}")
                    # å¦‚æœç´¢å¼•ä¸å­˜åœ¨ï¼Œçµ±è¨ˆç‚º0
                    if "index_not_found" in str(e).lower():
                        stats["elasticsearch_stats"] = {
                            "index_name": self.index_name,
                            "document_count": 0,
                            "index_size_bytes": 0,
                            "index_size_mb": 0
                        }
                    
            return stats
            
        except Exception as e:
            st.error(f"çµ±è¨ˆè³‡è¨Šç²å–å¤±æ•—: {e}")
            return {
                "system_type": "elasticsearch", 
                "base_statistics": self.memory_stats.copy(),
                "elasticsearch_stats": {}
            }
    
    def get_elasticsearch_statistics(self) -> Dict[str, Any]:
        """ç²å–è©³ç´°çš„ Elasticsearch çµ±è¨ˆè³‡è¨Š"""
        return self.get_enhanced_statistics()
    
    def get_document_statistics(self) -> Dict[str, Any]:
        """è¦†å¯«çˆ¶é¡æ–¹æ³•ï¼Œä½¿ç”¨ Elasticsearch å°ˆç”¨çµ±è¨ˆ"""
        try:
            # ä½¿ç”¨ Elasticsearch å°ˆç”¨çµ±è¨ˆæ–¹æ³•
            enhanced_stats = self.get_enhanced_statistics()
            
            # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
            es_stats = enhanced_stats.get("elasticsearch_stats", {})
            base_stats = enhanced_stats.get("base_statistics", {})
            
            return {
                "total_documents": es_stats.get("document_count", 0),
                "total_nodes": es_stats.get("document_count", 0),
                "document_details": [],
                "total_pages": 0,
                "index_size_bytes": es_stats.get("index_size_bytes", 0),
                "index_size_mb": es_stats.get("index_size_mb", 0)
            }
        except Exception as e:
            st.error(f"ç²å–æ–‡æª”çµ±è¨ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return {
                "total_documents": 0,
                "total_nodes": 0,
                "document_details": [],
                "total_pages": 0
            }
    
    def get_indexed_files(self) -> List[Dict[str, Any]]:
        """ç²å–å·²ç´¢å¼•çš„æ–‡ä»¶åˆ—è¡¨ (Elasticsearch ç‰ˆæœ¬)"""
        try:
            return self.get_indexed_files_from_es()
        except Exception as e:
            st.error(f"ç²å–æ–‡ä»¶åˆ—è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return []
    
    def delete_documents_by_source(self, source_filename: str) -> bool:
        """æ ¹æ“šä¾†æºæ–‡ä»¶ååˆªé™¤æ–‡æª”"""
        sync_client = getattr(self, 'sync_elasticsearch_client', None)
        if not sync_client:
            st.error("âŒ Elasticsearch åŒæ­¥å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–")
            return False
        
        try:
            # æ§‹å»ºæŸ¥è©¢ä»¥æŸ¥æ‰¾æŒ‡å®šä¾†æºçš„æ–‡æª”
            query = {
                "query": {
                    "term": {
                        "metadata.source.keyword": source_filename
                    }
                }
            }
            
            # åˆ·é™¤åŒ¹é…çš„æ–‡æª”
            response = sync_client.delete_by_query(
                index=self.index_name,
                body=query
            )
            
            deleted_count = response.get('deleted', 0)
            if deleted_count > 0:
                st.success(f"âœ… å¾ Elasticsearch ä¸­åˆªé™¤äº† {deleted_count} å€‹æ–‡æª”å¡Šï¼ˆä¾†æºï¼š{source_filename}ï¼‰")
                return True
            else:
                st.info(f"ğŸ“ åœ¨ Elasticsearch ä¸­æ²’æœ‰æ‰¾åˆ°ä¾†æºç‚º '{source_filename}' çš„æ–‡æª”")
                return False
                
        except Exception as e:
            st.error(f"âŒ å¾ Elasticsearch åˆªé™¤æ–‡æª”å¤±æ•—: {str(e)}")
            return False
    
    def get_indexed_files_from_es(self) -> List[Dict[str, Any]]:
        """å¾ ES ç´¢å¼•ä¸­ç²å–å·²ç´¢å¼•çš„æ–‡ä»¶åˆ—è¡¨"""
        # ä½¿ç”¨ Elasticsearch å®¢æˆ¶ç«¯é€²è¡ŒæŸ¥è©¢
        es_client = getattr(self, 'elasticsearch_client', None)
        if not es_client:
            return []
        
        try:
            # èšåˆæŸ¥è©¢ç²å–ä¸åŒçš„æ–‡ä»¶ä¾†æº
            query = {
                "size": 0,
                "aggs": {
                    "unique_sources": {
                        "terms": {
                            "field": "metadata.source",
                            "size": 1000
                        },
                        "aggs": {
                            "total_size": {
                                "sum": {
                                    "field": "metadata.file_size"
                                }
                            },
                            "file_type": {
                                "terms": {
                                    "field": "metadata.file_type",
                                    "size": 1
                                }
                            },
                            "latest_timestamp": {
                                "max": {
                                    "field": "metadata.timestamp"
                                }
                            }
                        }
                    }
                }
            }
            
            response = es_client.search(
                index=self.index_name,
                body=query
            )
            
            files = []
            buckets = response.get('aggregations', {}).get('unique_sources', {}).get('buckets', [])
            
            for bucket in buckets:
                source_file = bucket['key']
                chunk_count = bucket['doc_count']  # ç›´æ¥å¾ terms èšåˆå–å¾—æ–‡æª”æ•¸
                total_size = bucket.get('total_size', {}).get('value', 0)
                file_type_buckets = bucket.get('file_type', {}).get('buckets', [])
                file_type = file_type_buckets[0]['key'] if file_type_buckets else 'unknown'
                timestamp = bucket.get('latest_timestamp', {}).get('value_as_string', '')
                
                files.append({
                    'name': source_file,
                    'chunk_count': chunk_count,
                    'node_count': chunk_count,  # æ·»åŠ  node_count å­—æ®µå…¼å®¹æ€§
                    'total_size_bytes': total_size,
                    'size': total_size,  # ç‚ºäº†å…¼å®¹æ€§ä¿ç•™ size å­—æ®µ
                    'size_mb': round(total_size / (1024 * 1024), 1) if total_size > 0 else 0,
                    'file_type': file_type,
                    'timestamp': timestamp,
                    'source': 'elasticsearch'
                })
            
            return files
            
        except Exception as e:
            st.warning(f"å¾ ES ç²å–æ–‡ä»¶åˆ—è¡¨å¤±æ•—: {str(e)}")
            return []
    
    def get_knowledge_base_file_stats(self) -> Dict[str, Any]:
        """ç²å–çŸ¥è­˜åº«æ–‡ä»¶çµ±è¨ˆï¼ˆå¾ESç´¢å¼•ï¼‰"""
        files = self.get_indexed_files_from_es()
        
        total_files = len(files)
        total_chunks = sum(f['chunk_count'] for f in files)
        total_size_mb = sum(f['size_mb'] for f in files)
        
        # æŒ‰æ–‡ä»¶é¡å‹åˆ†é¡
        pdf_files = [f for f in files if f['file_type'] == 'pdf']
        image_files = [f for f in files if f['file_type'] in ['png', 'jpg', 'jpeg', 'webp', 'bmp']]
        doc_files = [f for f in files if f['file_type'] in ['txt', 'docx', 'md']]
        
        return {
            'total_files': total_files,
            'total_chunks': total_chunks,
            'total_size_mb': round(total_size_mb, 2),
            'pdf_count': len(pdf_files),
            'image_count': len(image_files),
            'document_count': len(doc_files),
            'files': files
        }
    
    def get_conversation_history(self, session_id: str = None, limit: int = 50) -> List[Dict[str, Any]]:
        """ç²å–å°è©±è¨˜éŒ„"""
        if self.conversation_manager:
            return self.conversation_manager.get_conversation_history(
                session_id=session_id, 
                limit=limit
            )
        return []
    
    def search_conversation_history(self, query_text: str, limit: int = 20) -> List[Dict[str, Any]]:
        """æœç´¢å°è©±è¨˜éŒ„"""
        if self.conversation_manager:
            return self.conversation_manager.search_conversations(query_text, limit)
        return []
    
    def get_conversation_statistics(self) -> Dict[str, Any]:
        """ç²å–å°è©±çµ±è¨ˆä¿¡æ¯"""
        if self.conversation_manager:
            return self.conversation_manager.get_conversation_statistics()
        return {}
    
    def update_conversation_feedback(self, conversation_id: str, rating: int = None, feedback: str = None) -> bool:
        """æ›´æ–°å°è©±åé¥‹"""
        if self.conversation_manager:
            return self.conversation_manager.update_conversation_feedback(
                conversation_id, rating, feedback
            )
        return False

    def refresh_index(self):
        """åˆªé™¤æ–‡æª”å¾Œåˆ·æ–°ç´¢å¼•"""
        try:
            # ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯é€²è¡Œç´¢å¼•åˆ·æ–°
            sync_client = getattr(self, 'sync_elasticsearch_client', None)
            if sync_client:
                print("ğŸ”„ æ­£åœ¨ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯åˆ·æ–°ESç´¢å¼•...")
                sync_client.indices.refresh(index=self.index_name)
                st.info("ğŸ”„ Elasticsearch ç´¢å¼•å·²åˆ·æ–°")
                print("âœ… ESç´¢å¼•åˆ·æ–°å®Œæˆ")
            else:
                st.warning("âš ï¸ åŒæ­¥ESå®¢æˆ¶ç«¯ä¸å¯ç”¨ï¼Œç„¡æ³•åˆ·æ–°ç´¢å¼•")
        except Exception as e:
            st.warning(f"ç´¢å¼•åˆ·æ–°è­¦å‘Š: {str(e)}")
            print(f"âŒ ç´¢å¼•åˆ·æ–°å¤±æ•—: {str(e)}")

    def __del__(self):
        """ææ§‹å‡½æ•¸ï¼šæ¸…ç†è³‡æº"""
        try:
            if hasattr(self, 'elasticsearch_client') and self.elasticsearch_client:
                self.elasticsearch_client.close()
        except:
            pass
    
    def setup_query_engine(self):
        """è¨­ç½®æŸ¥è©¢å¼•æ“ - æ”¯æ´ ES æ··åˆæª¢ç´¢ (å‘é‡ + é—œéµå­—)"""
        if self.index:
            # ä½¿ç”¨è‡ªå®šç¾©çš„æ··åˆæª¢ç´¢å™¨
            if self.use_elasticsearch and self.elasticsearch_store:
                # å‰µå»ºæ··åˆæª¢ç´¢å™¨ (å‘é‡ + é—œéµå­—)
                retriever = self._create_hybrid_retriever()
                
                from llama_index.core.query_engine import RetrieverQueryEngine
                self.query_engine = RetrieverQueryEngine.from_args(
                    retriever=retriever,
                    response_mode="compact"
                )
                st.success("âœ… ä½¿ç”¨ ES æ··åˆæª¢ç´¢å¼•æ“ (å‘é‡æœå°‹ + é—œéµå­—æœå°‹)")
            else:
                # å›é€€åˆ°æ¨™æº–æŸ¥è©¢å¼•æ“
                self.query_engine = self.index.as_query_engine(
                    similarity_top_k=3,
                    response_mode="compact"
                )
                st.info("âœ… ä½¿ç”¨æ¨™æº–å‘é‡æª¢ç´¢å¼•æ“")
    
    def _create_hybrid_retriever(self):
        """å‰µå»º ES æ··åˆæª¢ç´¢å™¨ (å‘é‡ç›¸ä¼¼åº¦ + BM25 é—œéµå­—)"""
        from llama_index.core.retrievers import BaseRetriever
        from llama_index.core.schema import QueryBundle, NodeWithScore
        from typing import List
        
        class ESHybridRetriever(BaseRetriever):
            def __init__(self, es_client, index_name, embedding_model, top_k=5):
                self.es_client = es_client
                self.index_name = index_name
                self.embedding_model = embedding_model
                self.top_k = top_k
                print(f"ğŸ”§ ESHybridRetrieveråˆå§‹åŒ–: ESå®¢æˆ¶ç«¯é¡å‹={type(es_client)}")
                print(f"ğŸ”§ ç´¢å¼•åç¨±: {index_name}, top_k: {top_k}")
                super().__init__()
            
            def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
                """æ··åˆæª¢ç´¢ï¼šçµåˆå‘é‡æœå°‹å’Œé—œéµå­—æœå°‹"""
                query_text = query_bundle.query_str
                print(f"ğŸ” é–‹å§‹ESæ··åˆæª¢ç´¢ï¼ŒæŸ¥è©¢: {query_text}")
                print(f"ğŸ”§ ESå®¢æˆ¶ç«¯é¡å‹: {type(self.es_client)}")
                
                try:
                    # 1. ç²å–æŸ¥è©¢çš„ embedding å‘é‡
                    print("ğŸ“Š æ­£åœ¨ç²å–æŸ¥è©¢å‘é‡...")
                    query_embedding = self.embedding_model._get_query_embedding(query_text)
                    print(f"âœ… æŸ¥è©¢å‘é‡ç¶­åº¦: {len(query_embedding) if query_embedding else 'None'}")
                    
                    # 2. ES æ··åˆæŸ¥è©¢ (å‘é‡ + é—œéµå­—) - ä½¿ç”¨ Elasticsearch 8.x èªæ³•
                    hybrid_query = {
                        "size": self.top_k,
                        "knn": {
                            "field": "embedding",
                            "query_vector": query_embedding,
                            "k": self.top_k,
                            "num_candidates": self.top_k * 2
                        },
                        "query": {
                            "bool": {
                                "should": [
                                    # BM25 é—œéµå­—æœå°‹ (è©å½™æœå°‹)
                                    {
                                        "match": {
                                            "content": {
                                                "query": query_text,
                                                "boost": 1.2  # ç¨å¾®æå‡é—œéµå­—æ¬Šé‡
                                            }
                                        }
                                    },
                                    # çŸ­èªåŒ¹é…
                                    {
                                        "match_phrase": {
                                            "content": {
                                                "query": query_text,
                                                "boost": 1.5
                                            }
                                        }
                                    }
                                ],
                                "minimum_should_match": 1
                            }
                        },
                        "_source": ["content", "metadata"]
                    }
                    
                    # 3. åŸ·è¡ŒæŸ¥è©¢
                    print(f"ğŸ” åŸ·è¡ŒESæœå°‹ï¼Œç´¢å¼•: {self.index_name}")
                    print(f"ğŸ”§ æŸ¥è©¢çµæ§‹: {hybrid_query}")
                    
                    try:
                        response = self.es_client.search(
                            index=self.index_name,
                            body=hybrid_query
                        )
                        print(f"âœ… ESæŸ¥è©¢æˆåŠŸï¼ŒéŸ¿æ‡‰é¡å‹: {type(response)}")
                        
                        # æª¢æŸ¥ response æ˜¯å¦ç‚º awaitable
                        if hasattr(response, '__await__'):
                            print("âŒ éŒ¯èª¤ï¼šæ”¶åˆ°awaitable responseï¼Œä½†åœ¨åŒæ­¥ç’°å¢ƒä¸­")
                            raise Exception("åŒæ­¥å®¢æˆ¶ç«¯è¿”å›äº†awaitable response")
                            
                    except Exception as search_error:
                        print(f"âŒ ESæœå°‹å¤±æ•—: {str(search_error)}")
                        print(f"ğŸ”§ æœå°‹éŒ¯èª¤é¡å‹: {type(search_error)}")
                        raise search_error
                    
                    # 4. è½‰æ›ç‚º NodeWithScore
                    nodes = []
                    hits = response.get('hits', {}).get('hits', [])
                    print(f"ğŸ“Š æ‰¾åˆ° {len(hits)} å€‹åŒ¹é…çµæœ")
                    
                    for i, hit in enumerate(hits):
                        from llama_index.core.schema import TextNode
                        
                        # å‰µå»ºæ–‡æœ¬ç¯€é»
                        node = TextNode(
                            text=hit['_source']['content'],
                            metadata=hit['_source'].get('metadata', {}),
                            id_=hit['_id']
                        )
                        
                        # ES è©•åˆ† (çµåˆå‘é‡å’Œé—œéµå­—)
                        score = hit['_score']
                        
                        # å‰µå»ºè©•åˆ†ç¯€é»
                        node_with_score = NodeWithScore(
                            node=node,
                            score=score
                        )
                        nodes.append(node_with_score)
                    
                    st.info(f"ğŸ” ES æ··åˆæª¢ç´¢æ‰¾åˆ° {len(nodes)} å€‹ç›¸é—œæ–‡æª”")
                    return nodes
                    
                except Exception as e:
                    print(f"âŒ ESæ··åˆæª¢ç´¢å¤±æ•—ï¼ŒéŒ¯èª¤è©³æƒ…: {str(e)}")
                    print(f"ğŸ”§ éŒ¯èª¤é¡å‹: {type(e)}")
                    import traceback
                    print(f"ğŸ” å®Œæ•´éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
                    st.error(f"âŒ ES æ··åˆæª¢ç´¢å¤±æ•—: {str(e)}")
                    
                    # æª¢æŸ¥æ˜¯å¦ç‚ºasync/awaitéŒ¯èª¤
                    if "ObjectApiResponse" in str(e) or "await" in str(e) or "coroutine" in str(e):
                        print("ğŸš¨ æª¢æ¸¬åˆ°async/syncå®¢æˆ¶ç«¯éŒ¯èª¤ï¼ŒESå®¢æˆ¶ç«¯å¯èƒ½ä»ç‚ºç•°æ­¥")
                        print(f"ğŸ”§ ç•¶å‰ESå®¢æˆ¶ç«¯é¡å‹: {type(self.es_client)}")
                    
                    # å›é€€åˆ°åŸºæœ¬å‘é‡æª¢ç´¢
                    return self._fallback_vector_search(query_bundle)
            
            def _fallback_vector_search(self, query_bundle):
                """å›é€€åˆ°ç´”å‘é‡æœå°‹"""
                try:
                    query_embedding = self.embedding_model._get_query_embedding(query_bundle.query_str)
                    
                    vector_query = {
                        "size": self.top_k,
                        "knn": {
                            "field": "embedding",
                            "query_vector": query_embedding,
                            "k": self.top_k,
                            "num_candidates": self.top_k * 2
                        },
                        "_source": ["content", "metadata"]
                    }
                    
                    response = self.es_client.search(
                        index=self.index_name,
                        body=vector_query
                    )
                    
                    nodes = []
                    for hit in response['hits']['hits']:
                        from llama_index.core.schema import TextNode
                        
                        node = TextNode(
                            text=hit['_source']['content'],
                            metadata=hit['_source'].get('metadata', {}),
                            id_=hit['_id']
                        )
                        
                        node_with_score = NodeWithScore(
                            node=node,
                            score=hit['_score']
                        )
                        nodes.append(node_with_score)
                    
                    st.warning("âš ï¸ å›é€€åˆ°ç´”å‘é‡æœå°‹")
                    return nodes
                    
                except Exception as e:
                    st.error(f"âŒ å‘é‡æœå°‹ä¹Ÿå¤±æ•—: {str(e)}")
                    return []
        
        # å‰µå»ºä¸¦è¿”å›æ··åˆæª¢ç´¢å™¨ - ä½¿ç”¨çµ±ä¸€åŒæ­¥å®¢æˆ¶ç«¯
        if not hasattr(self, 'elasticsearch_client') or not self.elasticsearch_client:
            st.error("âŒ ESå®¢æˆ¶ç«¯æœªåˆå§‹åŒ–ï¼Œç„¡æ³•å‰µå»ºæŸ¥è©¢å¼•æ“")
            return None
            
        print(f"ğŸ”§ å‰µå»ºESHybridRetrieverï¼Œä½¿ç”¨å®¢æˆ¶ç«¯é¡å‹: {type(self.elasticsearch_client)}")
            
        return ESHybridRetriever(
            es_client=self.elasticsearch_client,  # çµ±ä¸€ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯
            index_name=self.index_name,
            embedding_model=self.embedding_model,
            top_k=10  # Change the top_k value from 5 to 10
        )
    
    def _recreate_sync_elasticsearch_client(self) -> bool:
        """å®Œå…¨é‡æ–°å‰µå»ºåŒæ­¥ESå®¢æˆ¶ç«¯ï¼Œè§£æ±ºasync/awaitå•é¡Œ"""
        try:
            print("ğŸ”§ é–‹å§‹é‡æ–°å‰µå»ºåŒæ­¥ESå®¢æˆ¶ç«¯...")
            
            # å¼·åˆ¶ä½¿ç”¨æœ€åŸºç¤çš„åŒæ­¥é…ç½®
            from elasticsearch import Elasticsearch
            
            config = self.elasticsearch_config
            basic_config = {
                'hosts': [f"{config['scheme']}://{config['host']}:{config['port']}"],
                'request_timeout': 60,
                'max_retries': 1,
                'retry_on_timeout': False
            }
            
            if config.get('username') and config.get('password'):
                basic_config['basic_auth'] = (config['username'], config['password'])
            
            # å‰µå»ºæ–°çš„åŒæ­¥å®¢æˆ¶ç«¯
            new_sync_client = Elasticsearch(**basic_config)
            
            # æ¸¬è©¦é€£æ¥
            if new_sync_client.ping():
                print("âœ… æ–°åŒæ­¥å®¢æˆ¶ç«¯é€£æ¥æˆåŠŸ")
                # æ›´æ–°åŒæ­¥å®¢æˆ¶ç«¯å¼•ç”¨ï¼ˆä¸è¦è¦†è“‹ç•°æ­¥å®¢æˆ¶ç«¯ï¼‰
                self.sync_elasticsearch_client = new_sync_client
                
                # é‡æ–°å‰µå»ºå‘é‡å­˜å„² - ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯
                self.elasticsearch_store = ElasticsearchStore(
                    es_client=self.sync_elasticsearch_client,
                    index_name=self.index_name,
                    vector_field=self.elasticsearch_config['vector_field'],
                    text_field=self.elasticsearch_config['text_field'],
                    metadata_field='metadata',
                    embedding_dim=self.elasticsearch_config.get('dimension', 1024)
                )
                print("âœ… å‘é‡å­˜å„²é‡æ–°å‰µå»ºæˆåŠŸï¼ˆä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯ï¼‰")
                return True
            else:
                print("âŒ æ–°åŒæ­¥å®¢æˆ¶ç«¯é€£æ¥å¤±æ•—")
                return False
                
        except Exception as e:
            print(f"âŒ é‡æ–°å‰µå»ºåŒæ­¥å®¢æˆ¶ç«¯å¤±æ•—: {str(e)}")
            return False
    
    def create_index(self, documents: List[Document]) -> VectorStoreIndex:
        """è¦†å¯«çˆ¶é¡æ–¹æ³•ï¼Œå¼·åˆ¶ä½¿ç”¨ Elasticsearch"""
        if not documents:
            st.warning("âš ï¸ æ²’æœ‰æ–‡æª”éœ€è¦ç´¢å¼•")
            return None
        
        with st.spinner("æ­£åœ¨ä½¿ç”¨ Elasticsearch å»ºç«‹ç´¢å¼•..."):
            try:
                # ç¢ºä¿ ES é€£æ¥å’Œæ¨¡å‹å·²åˆå§‹åŒ–
                if not self.elasticsearch_client:
                    st.error("âŒ Elasticsearch å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–ï¼Œå˜—è©¦é‡æ–°åˆå§‹åŒ–...")
                    if not self._setup_elasticsearch_client():
                        return None
                
                if not self.elasticsearch_store:
                    st.error("âŒ Elasticsearch å‘é‡å­˜å„²æœªè¨­ç½®ï¼Œå˜—è©¦é‡æ–°è¨­ç½®...")
                    if not self._create_elasticsearch_index():
                        return None
                    if not self._setup_elasticsearch_store():
                        return None
                    
                self._ensure_models_initialized()
                
                # å»ºç«‹ storage context
                storage_context = StorageContext.from_defaults(
                    vector_store=self.elasticsearch_store
                )
                
                st.info(f"ğŸ“Š æº–å‚™å‘é‡åŒ– {len(documents)} å€‹æ–‡æª”åˆ° {self.index_name}")
                
                # æª¢æŸ¥æ–‡æª”å…§å®¹
                for i, doc in enumerate(documents[:3]):
                    content_preview = doc.text[:100] + "..." if len(doc.text) > 100 else doc.text
                    print(f"ğŸ“„ æ–‡æª” {i+1}: {len(doc.text)} å­—ç¬¦")
                    print(f"   å…§å®¹é è¦½: {content_preview}")
                    if hasattr(doc, 'metadata') and doc.metadata:
                        print(f"   å…ƒæ•¸æ“š: {doc.metadata}")
                
                # å‰µå»ºç´¢å¼• - åŠ å…¥è©³ç´°æ—¥èªŒ
                print(f"ğŸ”§ é–‹å§‹å‰µå»ºVectorStoreIndexï¼Œä½¿ç”¨ESå­˜å„²: {type(self.elasticsearch_store)}")
                print(f"ğŸ”§ ESå®¢æˆ¶ç«¯é¡å‹: {type(self.elasticsearch_client)}")
                print(f"ğŸ”§ åµŒå…¥æ¨¡å‹é¡å‹: {type(self.embedding_model)}")
                
                try:
                    index = VectorStoreIndex.from_documents(
                        documents, 
                        storage_context=storage_context,
                        embed_model=self.embedding_model
                    )
                    print("âœ… VectorStoreIndex.from_documents åŸ·è¡ŒæˆåŠŸ")
                except Exception as index_error:
                    print(f"âŒ VectorStoreIndex.from_documents å¤±æ•—: {str(index_error)}")
                    print(f"âŒ éŒ¯èª¤é¡å‹: {type(index_error)}")
                    import traceback
                    print(f"âŒ å®Œæ•´éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
                    
                    # å¦‚æœæ˜¯ HeadApiResponse éŒ¯èª¤ï¼Œå˜—è©¦æ›¿ä»£æ–¹æ¡ˆ
                    if "HeadApiResponse" in str(index_error) or "await" in str(index_error):
                        print("ğŸ”„ æª¢æ¸¬åˆ°HeadApiResponseéŒ¯èª¤ï¼Œå˜—è©¦é‡æ–°åˆå§‹åŒ–ESå®¢æˆ¶ç«¯...")
                        
                        # å®Œå…¨é‡æ–°å‰µå»º ES å®¢æˆ¶ç«¯å’Œå­˜å„²
                        if self._recreate_sync_elasticsearch_client():
                            print("ğŸ”„ é‡æ–°å‰µå»ºstorage_context...")
                            storage_context = StorageContext.from_defaults(
                                vector_store=self.elasticsearch_store
                            )
                            
                            print("ğŸ”„ é‡æ–°å˜—è©¦å‰µå»ºç´¢å¼•...")
                            index = VectorStoreIndex.from_documents(
                                documents, 
                                storage_context=storage_context,
                                embed_model=self.embedding_model
                            )
                            print("âœ… ä½¿ç”¨é‡æ–°å‰µå»ºçš„å®¢æˆ¶ç«¯æˆåŠŸå‰µå»ºç´¢å¼•")
                        else:
                            raise index_error
                    else:
                        raise index_error
                
                # å¼·åˆ¶åˆ·æ–°ä¸¦é©—è­‰ï¼ˆä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯ï¼‰
                sync_client = getattr(self, 'sync_elasticsearch_client', None)
                if sync_client:
                    sync_client.indices.refresh(index=self.index_name)
                    stats = sync_client.indices.stats(index=self.index_name)
                    doc_count = stats['indices'][self.index_name]['total']['docs']['count']
                else:
                    doc_count = 0
                
                print(f"âœ… ESç´¢å¼•é©—è­‰: {doc_count} å€‹æ–‡æª”å·²æˆåŠŸç´¢å¼•åˆ° {self.index_name}")
                st.success(f"âœ… æˆåŠŸç´¢å¼• {doc_count} å€‹æ–‡æª”åˆ° Elasticsearch")
                
                # æ›´æ–°çµ±è¨ˆ
                self.memory_stats['documents_processed'] = len(documents)
                self.memory_stats['vectors_stored'] = doc_count
                
                return index
                
            except Exception as e:
                st.error(f"âŒ Elasticsearch ç´¢å¼•å»ºç«‹å¤±æ•—: {str(e)}")
                print(f"âŒ è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
                return None