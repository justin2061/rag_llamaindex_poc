import os
from typing import List, Optional, Dict, Any
import streamlit as st
from llama_index.core import VectorStoreIndex, Document, Settings
import traceback

# Elasticsearch æ”¯æ´
try:
    from elasticsearch import Elasticsearch
    from llama_index.core.storage.storage_context import StorageContext
    from ..storage.custom_elasticsearch_store import CustomElasticsearchStore
    ELASTICSEARCH_AVAILABLE = True
except ImportError:
    ELASTICSEARCH_AVAILABLE = False

from .rag_system import RAGSystem
from ..storage.conversation_memory import ConversationMemory
from ..processors.user_file_manager import UserFileManager
from ..processors.gemini_ocr import GeminiOCRProcessor
from ..utils.embedding_fix import setup_safe_embedding, prevent_openai_fallback
# from chroma_vector_store import ChromaVectorStoreManager  # å·²æ”¹ç”¨ Elasticsearch

class EnhancedRAGSystem(RAGSystem):
    def __init__(self, use_elasticsearch: bool = True, use_chroma: bool = False):
        super().__init__()
        
        # åˆå§‹åŒ–æ–°åŠŸèƒ½æ¨¡çµ„
        self.memory = ConversationMemory()
        self.file_manager = UserFileManager()
        self.ocr_processor = GeminiOCRProcessor()
        
        # å„ªå…ˆä½¿ç”¨ Elasticsearchï¼Œåœç”¨ ChromaDB
        self.use_elasticsearch = use_elasticsearch
        self.use_chroma = False  # å¼·åˆ¶åœç”¨ ChromaDB
        self.chroma_manager = None  # ä¸å†ä½¿ç”¨ ChromaDB
        
        # Elasticsearch è¨­å®š
        self.elasticsearch_client = None
        self.elasticsearch_store = None
        
        # å¦‚æžœå•Ÿç”¨ Elasticsearchï¼Œå˜—è©¦åˆå§‹åŒ–
        if self.use_elasticsearch and ELASTICSEARCH_AVAILABLE:
            self._initialize_elasticsearch()
    
    def _initialize_elasticsearch(self):
        """åˆå§‹åŒ– Elasticsearch é€£æŽ¥"""
        try:
            from config.config import (
                ELASTICSEARCH_HOST, ELASTICSEARCH_PORT, ELASTICSEARCH_SCHEME,
                ELASTICSEARCH_INDEX_NAME, ELASTICSEARCH_USERNAME, ELASTICSEARCH_PASSWORD,
                ELASTICSEARCH_TIMEOUT, ELASTICSEARCH_MAX_RETRIES, ELASTICSEARCH_VERIFY_CERTS,
                ELASTICSEARCH_VECTOR_DIMENSION
            )
            
            # å»ºç«‹ Elasticsearch å®¢æˆ¶ç«¯
            es_config = {
                'hosts': [f'{ELASTICSEARCH_SCHEME}://{ELASTICSEARCH_HOST}:{ELASTICSEARCH_PORT}'],
                'verify_certs': ELASTICSEARCH_VERIFY_CERTS,
                'ssl_show_warn': False,
                'timeout': ELASTICSEARCH_TIMEOUT,
                'max_retries': ELASTICSEARCH_MAX_RETRIES,
                'retry_on_timeout': True
            }
            
            if ELASTICSEARCH_USERNAME and ELASTICSEARCH_PASSWORD:
                es_config['basic_auth'] = (ELASTICSEARCH_USERNAME, ELASTICSEARCH_PASSWORD)
            
            self.elasticsearch_client = Elasticsearch(**es_config)
            print(f"ðŸ”§ EnhancedRAGSystem ESå®¢æˆ¶ç«¯é¡žåž‹: {type(self.elasticsearch_client)}")
            
            # æª¢æŸ¥é€£æŽ¥
            if self.elasticsearch_client.ping():
                st.info("âœ… Elasticsearch é€£æŽ¥æˆåŠŸ")
                # ç¶­åº¦ä¸€è‡´æ€§æª¢æŸ¥
                if not self._validate_embedding_dimension(ELASTICSEARCH_VECTOR_DIMENSION):
                    st.error("âŒ åµŒå…¥ç¶­åº¦èˆ‡ Elasticsearch é…ç½®ä¸ä¸€è‡´ï¼Œè«‹æª¢æŸ¥è¨­å®šã€‚")
                    self.use_elasticsearch = False
                    return False

                # å»ºç«‹ vector storeï¼ˆä½¿ç”¨è‡ªå®šç¾©å¯¦ç¾é¿å… async/await å•é¡Œï¼‰
                self.elasticsearch_store = CustomElasticsearchStore(
                    es_client=self.elasticsearch_client,
                    index_name=ELASTICSEARCH_INDEX_NAME,
                    vector_field="embedding",
                    text_field="content",
                    metadata_field="metadata"
                )
                return True
            else:
                st.warning("âš ï¸ ç„¡æ³•é€£æŽ¥åˆ° Elasticsearchï¼Œå°‡ä½¿ç”¨ SimpleVectorStore")
                self.use_elasticsearch = False
                return False
                
        except Exception as e:
            st.warning(f"âš ï¸ Elasticsearch åˆå§‹åŒ–å¤±æ•—: {str(e)}ï¼Œå°‡ä½¿ç”¨ SimpleVectorStore")
            self.use_elasticsearch = False
            return False
    
    def _ensure_models_initialized(self):
        """ç¢ºä¿æ¨¡åž‹å·²åˆå§‹åŒ–"""
        if not self.models_initialized:
            self._setup_models()
            self.models_initialized = True
    
    def _setup_models(self):
        """è¨­å®šæ¨¡åž‹ - è¦†å¯«çˆ¶é¡žæ–¹æ³•ä»¥ç¢ºä¿æ­£ç¢ºåˆå§‹åŒ–"""
        from config.config import GROQ_API_KEY, LLM_MODEL, JINA_API_KEY
        from llama_index.llms.groq import Groq
        from llama_index.core.node_parser import SimpleNodeParser
        from llama_index.core import Settings
        import streamlit as st
        
        # é˜²æ­¢ OpenAI å›žé€€ä¸¦è¨­ç½®å®‰å…¨çš„åµŒå…¥æ¨¡åž‹
        prevent_openai_fallback()
        
        # è¨­å®šLLM
        if GROQ_API_KEY:
            llm = Groq(model=LLM_MODEL, api_key=GROQ_API_KEY)
        else:
            st.error("è«‹è¨­å®šGROQ_API_KEYç’°å¢ƒè®Šæ•¸")
            return
        
        # è¨­å®šå®‰å…¨åµŒå…¥æ¨¡åž‹ - åƒ…ä½¿ç”¨ Jina APIï¼ˆå«æœ¬åœ°å¾Œå‚™ï¼‰
        try:
            embed_model = setup_safe_embedding(JINA_API_KEY)
            st.success("âœ… æˆåŠŸåˆå§‹åŒ–åµŒå…¥æ¨¡åž‹ï¼ˆJinaï¼‰")
        except Exception as e2:
            st.error(f"åµŒå…¥æ¨¡åž‹åˆå§‹åŒ–å¤±æ•—: {str(e2)}")
            return
        
        # è¨­å®šå…¨åŸŸé…ç½®
        Settings.llm = llm
        Settings.embed_model = embed_model
        Settings.node_parser = SimpleNodeParser.from_defaults(chunk_size=1024)
        
        st.success("ðŸ”§ æ¨¡åž‹åˆå§‹åŒ–å®Œæˆ")
        
    def _get_embed_dim(self) -> int:
        """å˜—è©¦å¾žç•¶å‰åµŒå…¥æ¨¡åž‹å–å¾—ç¶­åº¦ã€‚æ‰¾ä¸åˆ°å‰‡è¿”å›ž Noneã€‚"""
        try:
            model = Settings.embed_model
            for attr in ("embed_dim", "_embed_dim", "dimension", "dim"):
                if hasattr(model, attr):
                    val = getattr(model, attr)
                    try:
                        return int(val)
                    except Exception:
                        continue
        except Exception:
            pass
        return None

    def _validate_embedding_dimension(self, expected_dim: int) -> bool:
        """é©—è­‰ç•¶å‰åµŒå…¥æ¨¡åž‹ç¶­åº¦èˆ‡é æœŸä¸€è‡´ã€‚"""
        actual = self._get_embed_dim()
        if actual is None:
            st.warning("ç„¡æ³•æª¢æ¸¬åµŒå…¥ç¶­åº¦ï¼Œè·³éŽç¶­åº¦é©—è­‰ã€‚")
            return True
        if actual != int(expected_dim):
            st.error(f"åµŒå…¥ç¶­åº¦ä¸åŒ¹é…ï¼šæ¨¡åž‹ç‚º {actual}ï¼ŒElasticsearch é æœŸç‚º {expected_dim}")
            return False
        return True

    def query_with_context(self, question: str) -> str:
        """å¸¶ä¸Šä¸‹æ–‡è¨˜æ†¶çš„æŸ¥è©¢"""
        if not self.query_engine:
            return "ç³»çµ±å°šæœªåˆå§‹åŒ–ï¼Œè«‹å…ˆè¼‰å…¥æ–‡ä»¶ã€‚"
        
        try:
            # å»ºæ§‹åŒ…å«æ­·å²å°è©±çš„å®Œæ•´æŸ¥è©¢
            context_prompt = self.memory.get_context_prompt()
            
            if context_prompt and self.memory.is_enabled():
                enhanced_question = f"""
{context_prompt}

ç•¶å‰å•é¡Œ: {question}

è«‹åŸºæ–¼ä»¥ä¸Šå°è©±æ­·å²å’ŒçŸ¥è­˜åº«å…§å®¹å›žç­”ç•¶å‰å•é¡Œã€‚å¦‚æžœç•¶å‰å•é¡Œèˆ‡ä¹‹å‰çš„å°è©±ç›¸é—œï¼Œè«‹è€ƒæ…®ä¸Šä¸‹æ–‡èªžå¢ƒã€‚
"""
            else:
                enhanced_question = question
            
            with st.spinner("æ­£åœ¨æ€è€ƒæ‚¨çš„å•é¡Œ..."):
                response = self.query_engine.query(enhanced_question)
                response_str = str(response)
                
                # å°‡é€™è¼ªå°è©±åŠ å…¥è¨˜æ†¶
                self.memory.add_exchange(question, response_str)
                
                return response_str
                
        except Exception as e:
            st.error(f"æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            st.write(traceback.format_exc())
            return "æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"
    
    def process_uploaded_files(self, uploaded_files) -> List[Document]:
        """è™•ç†ä¸Šå‚³çš„æª”æ¡ˆ"""
        if not uploaded_files:
            return []
        
        documents = []
        
        for uploaded_file in uploaded_files:
            try:
                # å„²å­˜æª”æ¡ˆ
                file_path = self.file_manager.save_uploaded_file(uploaded_file)
                if not file_path:
                    continue
                
                # æ ¹æ“šæª”æ¡ˆé¡žåž‹è™•ç†
                if self.file_manager.is_image_file(uploaded_file.name):
                    # åœ–ç‰‡OCRè™•ç†
                    doc = self._process_image_file(uploaded_file, file_path)
                elif self.file_manager.is_document_file(uploaded_file.name):
                    # æ–‡æª”è™•ç†
                    doc = self._process_document_file(uploaded_file, file_path)
                else:
                    st.warning(f"ä¸æ”¯æ´çš„æª”æ¡ˆé¡žåž‹: {uploaded_file.name}")
                    continue
                
                if doc:
                    documents.append(doc)
                    
            except Exception as e:
                st.error(f"è™•ç†æª”æ¡ˆ {uploaded_file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                continue
        
        return documents
    
    def _process_image_file(self, uploaded_file, file_path: str) -> Optional[Document]:
        """è™•ç†åœ–ç‰‡æª”æ¡ˆ"""
        if not self.ocr_processor.is_available():
            st.warning(f"OCRæœå‹™ä¸å¯ç”¨ï¼Œè·³éŽåœ–ç‰‡æª”æ¡ˆ: {uploaded_file.name}")
            return None
        
        try:
            # è®€å–åœ–ç‰‡æ•¸æ“š
            image_data = self.file_manager.get_file_content(os.path.basename(file_path))
            if not image_data:
                return None
            
            # å–å¾—æª”æ¡ˆæ“´å±•å
            file_ext = os.path.splitext(uploaded_file.name)[1].lower().lstrip('.')
            
            # OCRè™•ç†
            with st.spinner(f"æ­£åœ¨é€²è¡ŒOCRè™•ç†: {uploaded_file.name}"):
                ocr_result = self.ocr_processor.extract_text_from_image(image_data, file_ext)
            
            if ocr_result['success']:
                # å»ºç«‹æ–‡æª”
                document = Document(
                    text=ocr_result['text'],
                    metadata={
                        "source": uploaded_file.name,
                        "type": "image_ocr",
                        "original_format": file_ext,
                        "file_size": uploaded_file.size,
                        "ocr_confidence": ocr_result.get('confidence', 'unknown'),
                        "processed_at": st.session_state.get('current_time', 'unknown')
                    }
                )
                
                st.success(f"âœ… OCRè™•ç†æˆåŠŸ: {uploaded_file.name}")
                return document
            else:
                st.error(f"âŒ OCRè™•ç†å¤±æ•—: {uploaded_file.name} - {ocr_result['error']}")
                return None
                
        except Exception as e:
            st.error(f"è™•ç†åœ–ç‰‡æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return None
    
    def _process_document_file(self, uploaded_file, file_path: str) -> Optional[Document]:
        """è™•ç†æ–‡æª”æª”æ¡ˆ"""
        try:
            file_ext = os.path.splitext(uploaded_file.name)[1].lower()
            
            if file_ext == '.pdf':
                # PDFè™•ç†
                docs = self.load_pdfs([file_path])
                if docs:
                    doc = docs[0]
                    # æ›´æ–°å…ƒæ•¸æ“š
                    doc.metadata.update({
                        "type": "user_document",
                        "file_size": uploaded_file.size,
                        "uploaded_at": st.session_state.get('current_time', 'unknown')
                    })
                    return doc
            
            elif file_ext == '.txt':
                # æ–‡å­—æª”è™•ç†
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                
                document = Document(
                    text=text,
                    metadata={
                        "source": uploaded_file.name,
                        "type": "user_document",
                        "original_format": "txt",
                        "file_size": uploaded_file.size,
                        "uploaded_at": st.session_state.get('current_time', 'unknown')
                    }
                )
                return document
            
            elif file_ext in ['.md', '.markdown']:
                # Markdownæª”è™•ç†
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                
                document = Document(
                    text=text,
                    metadata={
                        "source": uploaded_file.name,
                        "type": "user_document",
                        "original_format": "markdown",
                        "file_size": uploaded_file.size,
                        "uploaded_at": st.session_state.get('current_time', 'unknown')
                    }
                )
                return document
            
            elif file_ext == '.docx':
                # DOCXæª”è™•ç†
                try:
                    import docx
                    doc = docx.Document(file_path)
                    text = ""
                    for paragraph in doc.paragraphs:
                        text += paragraph.text + "\n"
                    
                    document = Document(
                        text=text,
                        metadata={
                            "source": uploaded_file.name,
                            "type": "user_document",
                            "original_format": "docx",
                            "file_size": uploaded_file.size,
                            "uploaded_at": st.session_state.get('current_time', 'unknown')
                        }
                    )
                    return document
                    
                except ImportError:
                    st.error("éœ€è¦å®‰è£ python-docx å¥—ä»¶ä¾†è™•ç† DOCX æª”æ¡ˆ")
                    return None
                except Exception as e:
                    st.error(f"DOCX æª”æ¡ˆè™•ç†å¤±æ•—: {str(e)}")
                    return None
            
            else:
                st.warning(f"æš«ä¸æ”¯æ´çš„æ–‡æª”æ ¼å¼: {file_ext}")
                return None
        except Exception as e:
            st.error(f"è™•ç†æ–‡æª”æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return None
    
    def create_index(self, documents: List[Document]) -> VectorStoreIndex:
        """å»ºç«‹æ–°çš„å‘é‡ç´¢å¼• (å„ªå…ˆä½¿ç”¨ Elasticsearch)"""
        with st.spinner("æ­£åœ¨å»ºç«‹å‘é‡ç´¢å¼•..."):
            # ç¢ºä¿æ¨¡åž‹å·²æ­£ç¢ºåˆå§‹åŒ–
            self._ensure_models_initialized()
            
            index = None
            
            try:
                # å„ªå…ˆä½¿ç”¨ Elasticsearch
                if self.use_elasticsearch and self.elasticsearch_store:
                    st.info("ä½¿ç”¨ Elasticsearch å»ºç«‹ç´¢å¼•...")
                    try:
                        # å»ºç«‹å‰åšç¶­åº¦é©—è­‰
                        from config.config import ELASTICSEARCH_VECTOR_DIMENSION
                        if not self._validate_embedding_dimension(ELASTICSEARCH_VECTOR_DIMENSION):
                            st.error("âŒ ç¶­åº¦ä¸ä¸€è‡´ï¼Œåœæ­¢å»ºç«‹ç´¢å¼•ã€‚")
                            return None

                        # å‰µå»ºç´¢å¼• - å¢žåŠ è©³ç´°æ—¥èªŒ
                        print(f"ðŸš€ é–‹å§‹ä½¿ç”¨ ES å»ºç«‹ç´¢å¼•ï¼Œæ–‡æª”æ•¸é‡: {len(documents)}")
                        st.info(f"ðŸ“Š æº–å‚™å‘é‡åŒ– {len(documents)} å€‹æ–‡æª”")
                        
                        # æª¢æŸ¥æ–‡æª”å…§å®¹
                        for i, doc in enumerate(documents[:3]):  # åªæª¢æŸ¥å‰3å€‹
                            content_preview = doc.text[:100] + "..." if len(doc.text) > 100 else doc.text
                            print(f"ðŸ“„ æ–‡æª” {i+1}: {len(doc.text)} å­—ç¬¦")
                            print(f"   å…§å®¹é è¦½: {content_preview}")
                            if hasattr(doc, 'metadata') and doc.metadata:
                                print(f"   å…ƒæ•¸æ“š: {doc.metadata}")
                        
                        # å»ºç«‹ storage context
                        storage_context = StorageContext.from_defaults(
                            vector_store=self.elasticsearch_store
                        )
                        
                        # å…ˆå‰µå»ºç©ºç´¢å¼•ï¼Œç„¶å¾Œé€å€‹æ·»åŠ æ–‡æª”ä»¥é¿å… async å•é¡Œ
                        index = VectorStoreIndex([], storage_context=storage_context)
                        
                        # é€å€‹æ·»åŠ æ–‡æª”åˆ°ç´¢å¼•
                        st.info("æ­£åœ¨é€å€‹æ·»åŠ æ–‡æª”åˆ°ç´¢å¼•...")
                        progress_bar = st.progress(0)
                        for i, doc in enumerate(documents):
                            try:
                                index.insert(doc)
                                progress_bar.progress((i + 1) / len(documents))
                            except Exception as doc_error:
                                st.warning(f"æ–‡æª” {i+1} æ·»åŠ å¤±æ•—: {str(doc_error)}")
                                continue
                        progress_bar.empty()
                        
                        # å¼·åˆ¶åˆ·æ–° ES ç´¢å¼•
                        if hasattr(self, 'elasticsearch_client') and self.elasticsearch_client:
                            try:
                                print(f"ðŸ”„ EnhancedRAGSystemåˆ·æ–°ESç´¢å¼•ï¼Œå®¢æˆ¶ç«¯é¡žåž‹: {type(self.elasticsearch_client)}")
                                # ä½¿ç”¨æ­£ç¢ºçš„ç´¢å¼•åç¨±
                                index_name = getattr(self, 'index_name', None)
                                if not index_name and hasattr(self, 'elasticsearch_store'):
                                    index_name = getattr(self.elasticsearch_store, 'index_name', 'rag_intelligent_assistant')
                                self.elasticsearch_client.indices.refresh(index=index_name)
                                print("âœ… ESç´¢å¼•å·²åˆ·æ–°")
                                
                                # é©—è­‰ç´¢å¼•çµæžœ
                                stats = self.elasticsearch_client.indices.stats(index=index_name)
                                doc_count = stats['indices'][index_name]['total']['docs']['count']
                                print(f"ðŸ“Š ç´¢å¼•é©—è­‰: {doc_count} å€‹æ–‡æª”å·²ç´¢å¼•")
                                st.info(f"ðŸ“Š å·²æˆåŠŸç´¢å¼• {doc_count} å€‹æ–‡æª”åˆ° Elasticsearch")
                                
                            except Exception as refresh_error:
                                print(f"âš ï¸ ç´¢å¼•åˆ·æ–°æˆ–é©—è­‰å¤±æ•—: {refresh_error}")
                        
                        st.success("âœ… æˆåŠŸä½¿ç”¨ Elasticsearch å»ºç«‹ç´¢å¼•")
                        
                    except Exception as e:
                        st.warning(f"Elasticsearch ç´¢å¼•å‰µå»ºå¤±æ•—: {str(e)}")
                        st.info("å›žé€€åˆ° SimpleVectorStore...")
                        self.use_elasticsearch = False
                
                # å¦‚æžœ Elasticsearch å¤±æ•—æˆ–æœªå•Ÿç”¨ï¼Œä½¿ç”¨ SimpleVectorStore
                if not self.use_elasticsearch:
                    st.info("ä½¿ç”¨ SimpleVectorStore å»ºç«‹ç´¢å¼•...")
                    index = VectorStoreIndex.from_documents(documents)
                    st.success("âœ… æˆåŠŸä½¿ç”¨ SimpleVectorStore å»ºç«‹ç´¢å¼•")
                    
                    # æŒä¹…åŒ–ç´¢å¼•
                    from config.config import INDEX_DIR
                    index.storage_context.persist(persist_dir=INDEX_DIR)
                    st.success("âœ… ç´¢å¼•å·²æŒä¹…åŒ–ä¿å­˜")
                
                if index:
                    self.index = index
                    return index
                else:
                    st.error("ç´¢å¼•å‰µå»ºå¤±æ•—")
                    return None
                    
            except Exception as e:
                st.error(f"å»ºç«‹ç´¢å¼•æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {str(e)}")
                return None
    
    def load_existing_index(self) -> bool:
        """è¼‰å…¥ç¾æœ‰çš„å‘é‡ç´¢å¼• (å„ªå…ˆä½¿ç”¨ Elasticsearch)"""
        # ç¢ºä¿æ¨¡åž‹å·²åˆå§‹åŒ–
        self._ensure_models_initialized()
        
        try:
            # å„ªå…ˆå˜—è©¦ Elasticsearch
            if self.use_elasticsearch and self.elasticsearch_store:
                st.info("å˜—è©¦å¾ž Elasticsearch è¼‰å…¥ç´¢å¼•...")
                try:
                    # è¼‰å…¥å‰åšç¶­åº¦é©—è­‰
                    from config.config import ELASTICSEARCH_VECTOR_DIMENSION
                    if not self._validate_embedding_dimension(ELASTICSEARCH_VECTOR_DIMENSION):
                        st.error("âŒ ç¶­åº¦ä¸ä¸€è‡´ï¼Œåœæ­¢è¼‰å…¥ç´¢å¼•ã€‚")
                        self.use_elasticsearch = False
                        # ç¹¼çºŒå˜—è©¦å›žé€€åˆ° SimpleVectorStore
                    
                    # æª¢æŸ¥ Elasticsearch æ˜¯å¦æœ‰è³‡æ–™
                    es_stats = self.elasticsearch_client.indices.stats(
                        index=self.elasticsearch_store.index_name
                    )
                    doc_count = es_stats['indices'][self.elasticsearch_store.index_name]['total']['docs']['count']
                    
                    if doc_count > 0:
                        # å¾ž Elasticsearch é‡å»ºç´¢å¼• - ä½¿ç”¨åŒæ­¥æ–¹å¼
                        storage_context = StorageContext.from_defaults(
                            vector_store=self.elasticsearch_store
                        )
                        # ç›´æŽ¥å‰µå»ºç´¢å¼•å¯¦ä¾‹è€Œä¸ä½¿ç”¨ from_vector_store
                        self.index = VectorStoreIndex(
                            nodes=[],
                            storage_context=storage_context
                        )
                        self.setup_query_engine()
                        st.success(f"âœ… æˆåŠŸå¾ž Elasticsearch è¼‰å…¥ {doc_count} å€‹æ–‡æª”")
                        return True
                    else:
                        st.info("Elasticsearch ç´¢å¼•ç‚ºç©º")
                        
                except Exception as e:
                    st.warning(f"Elasticsearch è¼‰å…¥å¤±æ•—: {str(e)}")
                    self.use_elasticsearch = False
            
            # å›žé€€åˆ° SimpleVectorStore
            from config.config import INDEX_DIR
            if os.path.exists(INDEX_DIR) and os.listdir(INDEX_DIR):
                st.info("å˜—è©¦å¾ž SimpleVectorStore è¼‰å…¥ç´¢å¼•...")
                try:
                    from llama_index.core import load_index_from_storage
                    from llama_index.core.storage.storage_context import StorageContext
                    
                    storage_context = StorageContext.from_defaults(persist_dir=INDEX_DIR)
                    self.index = load_index_from_storage(storage_context)
                    self.setup_query_engine()
                    st.success("âœ… æˆåŠŸè¼‰å…¥ç¾æœ‰ç´¢å¼• (SimpleVectorStore)")
                    return True
                    
                except Exception as e:
                    st.error(f"è¼‰å…¥ SimpleVectorStore å¤±æ•—: {str(e)}")
                    return False
            else:
                st.info("æ²’æœ‰æ‰¾åˆ°ç¾æœ‰ç´¢å¼•æª”æ¡ˆ")
                return False
                
        except Exception as e:
            st.error(f"è¼‰å…¥ç´¢å¼•æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {str(e)}")
            return False
    
    
    

    def rebuild_index_with_user_files(self) -> bool:
        """é‡å»ºç´¢å¼•ï¼ŒåŒ…å«ç”¨æˆ¶ä¸Šå‚³çš„æª”æ¡ˆ"""
        try:
            all_documents = []
            
            # è¼‰å…¥å®˜æ–¹èŒ¶è‘‰è³‡æ–™
            from ..processors.enhanced_pdf_downloader import EnhancedPDFDownloader
            downloader = EnhancedPDFDownloader()
            official_pdfs = downloader.get_existing_pdfs()
            
            if official_pdfs:
                st.info("è¼‰å…¥å®˜æ–¹èŒ¶è‘‰è³‡æ–™...")
                official_docs = self.load_pdfs(official_pdfs)
                # æ¨™è¨˜ç‚ºå®˜æ–¹è³‡æ–™
                for doc in official_docs:
                    doc.metadata["data_source"] = "official"
                all_documents.extend(official_docs)
            
            # è¼‰å…¥ç”¨æˆ¶ä¸Šå‚³çš„æª”æ¡ˆ
            user_files = self.file_manager.get_uploaded_files()
            if user_files:
                st.info("è¼‰å…¥ç”¨æˆ¶ä¸Šå‚³çš„æª”æ¡ˆ...")
                
                # æ¨¡æ“¬uploaded_fileå°è±¡ä¾†é‡ç”¨ç¾æœ‰é‚è¼¯
                class MockUploadedFile:
                    def __init__(self, file_info):
                        self.name = file_info['name']
                        self.size = file_info['size']
                
                mock_files = [MockUploadedFile(f) for f in user_files]
                user_docs = []
                
                for i, file_info in enumerate(user_files):
                    if file_info['type'] == 'image':
                        doc = self._process_image_file(mock_files[i], file_info['path'])
                    else:
                        doc = self._process_document_file(mock_files[i], file_info['path'])
                    
                    if doc:
                        doc.metadata["data_source"] = "user_upload"
                        user_docs.append(doc)
                
                all_documents.extend(user_docs)
            
            if all_documents:
                # é‡å»ºç´¢å¼•
                st.info("é‡å»ºå‘é‡ç´¢å¼•...")
                index = self.create_index(all_documents)
                
                if index:
                    self.setup_query_engine()
                    return True
            
            return False
            
        except Exception as e:
            st.error(f"é‡å»ºç´¢å¼•æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return False
    
    def get_document_statistics(self) -> dict:
        """å–å¾—æ–‡ä»¶çµ±è¨ˆè³‡è¨Š (æ”¯æ´ Elasticsearch å’Œ SimpleVectorStore)"""
        if not self.index:
            return {}
        
        stats = {
            "total_documents": 0,
            "total_nodes": 0,
            "document_details": [],
            "total_pages": 0
        }
        
        try:
            if self.use_elasticsearch and self.elasticsearch_client:
                # ä½¿ç”¨ Elasticsearch çµ±è¨ˆ
                try:
                    from config.config import ELASTICSEARCH_INDEX_NAME
                    index_name = ELASTICSEARCH_INDEX_NAME or 'rag_intelligent_assistant'
                    
                    es_stats = self.elasticsearch_client.indices.stats(
                        index=index_name
                    )
                    doc_count = es_stats['indices'][index_name]['total']['docs']['count']
                    index_size = es_stats['indices'][index_name]['total']['store']['size_in_bytes']
                    
                    stats["total_documents"] = doc_count
                    stats["total_nodes"] = doc_count
                    stats["index_size_bytes"] = index_size
                    stats["index_size_mb"] = round(index_size / 1024 / 1024, 2)
                    
                    # å¾ž Elasticsearch ç²å–æ–‡æª”é¡žåž‹çµ±è¨ˆ
                    print(f"ðŸ” EnhancedRAGSystemåŸ·è¡ŒESæœå°‹ï¼Œå®¢æˆ¶ç«¯é¡žåž‹: {type(self.elasticsearch_client)}")
                    search_result = self.elasticsearch_client.search(
                        index=index_name,
                        body={
                            "size": 0,
                            "aggs": {
                                "source_types": {
                                    "terms": {
                                        "field": "metadata.source.keyword",
                                        "size": 100
                                    }
                                }
                            }
                        }
                    )
                    print(f"âœ… EnhancedRAGSystem ESæŸ¥è©¢éŸ¿æ‡‰é¡žåž‹: {type(search_result)}")
                    if hasattr(search_result, '__await__'):
                        print("ðŸš¨ EnhancedRAGSystemæª¢æ¸¬åˆ°awaitable response - ç•°æ­¥å®¢æˆ¶ç«¯éŒ¯èª¤ï¼")
                        raise Exception("EnhancedRAGSystemåŒæ­¥å®¢æˆ¶ç«¯è¿”å›žäº†awaitable response")
                    
                    source_buckets = search_result.get('aggregations', {}).get('source_types', {}).get('buckets', [])
                    for bucket in source_buckets:
                        stats["document_details"].append({
                            "name": bucket['key'],
                            "pages": bucket['doc_count'],
                            "node_count": bucket['doc_count']
                        })
                    
                    st.info(f"ðŸ“Š å¾ž Elasticsearch ç²å–çµ±è¨ˆ: {stats['total_documents']} å€‹æ–‡æª”")
                    
                except Exception as es_e:
                    error_msg = str(es_e)
                    print(f"âŒ EnhancedRAGSystem ESçµ±è¨ˆéŒ¯èª¤: {error_msg}")
                    print(f"ðŸ”§ éŒ¯èª¤é¡žåž‹: {type(es_e)}")
                    if "ObjectApiResponse" in error_msg or "await" in error_msg or "coroutine" in error_msg:
                        print("ðŸš¨ EnhancedRAGSystemæª¢æ¸¬åˆ°ObjectApiResponseéŒ¯èª¤ï¼")
                        print(f"ðŸ”§ ç•¶å‰ESå®¢æˆ¶ç«¯é¡žåž‹: {type(self.elasticsearch_client)}")
                    import traceback
                    print(f"ðŸ” EnhancedRAGSystemå®Œæ•´éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
                    st.warning(f"ç„¡æ³•å¾ž Elasticsearch ç²å–çµ±è¨ˆè³‡è¨Š: {str(es_e)}")
                    # å›žé€€åˆ° SimpleVectorStore çµ±è¨ˆ
                    return self._get_simple_vector_store_stats()
            else:
                # ä½¿ç”¨ SimpleVectorStore çµ±è¨ˆ
                return self._get_simple_vector_store_stats()
            
        except Exception as e:
            st.error(f"ç²å–æ–‡æª”çµ±è¨ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        
        return stats
    
    def _get_simple_vector_store_stats(self) -> dict:
        """ç²å– SimpleVectorStore çµ±è¨ˆè³‡è¨Š"""
        stats = {
            "total_documents": 0,
            "total_nodes": 0,
            "document_details": [],
            "total_pages": 0
        }
        
        try:
            doc_info = {}
            if hasattr(self.index, 'docstore') and self.index.docstore.docs:
                for node in self.index.docstore.docs.values():
                    source = node.metadata.get("source", "æœªçŸ¥")
                    pages = node.metadata.get("pages", 1)
                    
                    if source not in doc_info:
                        doc_info[source] = {
                            "name": source,
                            "pages": pages,
                            "node_count": 0
                        }
                    doc_info[source]["node_count"] += 1
                
                stats["total_documents"] = len(doc_info)
                stats["total_nodes"] = len(self.index.docstore.docs)
                stats["document_details"] = list(doc_info.values())
                stats["total_pages"] = sum(doc["pages"] for doc in doc_info.values())
                
                st.info(f"ðŸ“Š å¾ž SimpleVectorStore ç²å–çµ±è¨ˆ: {stats['total_documents']} å€‹æ–‡æª”")
            else:
                st.warning("ç´¢å¼•ä¸­æ²’æœ‰æ‰¾åˆ°æ–‡æª”è³‡æ–™")
        except Exception as e:
            st.error(f"ç²å– SimpleVectorStore çµ±è¨ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        
        return stats
    
    def get_indexed_files(self) -> List[Dict[str, Any]]:
        """ç²å–å·²ç´¢å¼•çš„æ–‡ä»¶åˆ—è¡¨"""
        files = []
        
        try:
            if self.use_elasticsearch and self.elasticsearch_store:
                # å¾ž Elasticsearch ç²å–æ–‡ä»¶åˆ—è¡¨
                files = self._get_elasticsearch_files()
            elif hasattr(self.index, 'docstore') and self.index.docstore.docs:
                # å¾ž SimpleVectorStore ç²å–æ–‡ä»¶åˆ—è¡¨
                files = self._get_simple_vector_store_files()
        except Exception as e:
            st.error(f"ç²å–æ–‡ä»¶åˆ—è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        
        return files
    
    def _get_elasticsearch_files(self) -> List[Dict[str, Any]]:
        """å¾ž Elasticsearch ç²å–æ–‡ä»¶åˆ—è¡¨"""
        files = []
        try:
            # æœç´¢æ‰€æœ‰æ–‡æª”
            response = self.elasticsearch_client.search(
                index=self.elasticsearch_store.index_name,
                body={
                    "query": {"match_all": {}},
                    "size": 1000,
                    "_source": ["metadata", "content"]
                }
            )
            
            file_map = {}
            for hit in response['hits']['hits']:
                metadata = hit['_source'].get('metadata', {})
                source = metadata.get('source', 'æœªçŸ¥æ–‡ä»¶')
                
                if source not in file_map:
                    file_map[source] = {
                        'id': source,
                        'name': os.path.basename(source),
                        'path': source,
                        'type': metadata.get('file_type', 'unknown'),
                        'upload_time': metadata.get('upload_time', 'æœªçŸ¥'),
                        'size': metadata.get('file_size', 0),
                        'page_count': metadata.get('pages', 1),
                        'node_count': 0
                    }
                
                file_map[source]['node_count'] += 1
            
            files = list(file_map.values())
            
        except Exception as e:
            st.error(f"å¾ž Elasticsearch ç²å–æ–‡ä»¶åˆ—è¡¨å¤±æ•—: {str(e)}")
        
        return files
    
    def _get_simple_vector_store_files(self) -> List[Dict[str, Any]]:
        """å¾ž SimpleVectorStore ç²å–æ–‡ä»¶åˆ—è¡¨"""
        files = []
        try:
            file_map = {}
            for node in self.index.docstore.docs.values():
                source = node.metadata.get("source", "æœªçŸ¥æ–‡ä»¶")
                
                if source not in file_map:
                    file_map[source] = {
                        'id': source,
                        'name': os.path.basename(source),
                        'path': source,
                        'type': node.metadata.get('file_type', 'unknown'),
                        'upload_time': node.metadata.get('upload_time', 'æœªçŸ¥'),
                        'size': node.metadata.get('file_size', 0),
                        'page_count': node.metadata.get('pages', 1),
                        'node_count': 0
                    }
                
                file_map[source]['node_count'] += 1
            
            files = list(file_map.values())
            
        except Exception as e:
            st.error(f"å¾ž SimpleVectorStore ç²å–æ–‡ä»¶åˆ—è¡¨å¤±æ•—: {str(e)}")
        
        return files
    
    def delete_file_from_knowledge_base(self, file_id: str) -> bool:
        """å¾žçŸ¥è­˜åº«ä¸­åˆªé™¤æŒ‡å®šæ–‡ä»¶"""
        try:
            success = False
            
            if self.use_elasticsearch and self.elasticsearch_store:
                # å¾ž Elasticsearch åˆªé™¤æ–‡ä»¶
                success = self._delete_from_elasticsearch(file_id)
            elif hasattr(self.index, 'docstore'):
                # å¾ž SimpleVectorStore åˆªé™¤æ–‡ä»¶
                success = self._delete_from_simple_vector_store(file_id)
            
            if success:
                # åŒæ™‚å¾žæ–‡ä»¶ç³»çµ±åˆªé™¤ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
                self._delete_from_filesystem(file_id)
                st.success(f"âœ… æ–‡ä»¶ {os.path.basename(file_id)} å·²å¾žçŸ¥è­˜åº«ä¸­åˆªé™¤")
            
            return success
            
        except Exception as e:
            st.error(f"åˆªé™¤æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return False
    
    def _delete_from_elasticsearch(self, file_id: str) -> bool:
        """å¾ž Elasticsearch åˆªé™¤æ–‡ä»¶çš„æ‰€æœ‰ç¯€é»ž"""
        try:
            # æŸ¥è©¢è©²æ–‡ä»¶çš„æ‰€æœ‰æ–‡æª”
            response = self.elasticsearch_client.search(
                index=self.elasticsearch_store.index_name,
                body={
                    "query": {
                        "term": {
                            "metadata.source.keyword": file_id
                        }
                    },
                    "size": 1000
                }
            )
            
            # åˆªé™¤æ‰€æœ‰ç›¸é—œæ–‡æª”
            for hit in response['hits']['hits']:
                self.elasticsearch_client.delete(
                    index=self.elasticsearch_store.index_name,
                    id=hit['_id']
                )
            
            # åˆ·æ–°ç´¢å¼•
            self.elasticsearch_client.indices.refresh(
                index=self.elasticsearch_store.index_name
            )
            
            return True
            
        except Exception as e:
            st.error(f"å¾ž Elasticsearch åˆªé™¤æ–‡ä»¶å¤±æ•—: {str(e)}")
            return False
    
    def _delete_from_simple_vector_store(self, file_id: str) -> bool:
        """å¾ž SimpleVectorStore åˆªé™¤æ–‡ä»¶çš„æ‰€æœ‰ç¯€é»ž"""
        try:
            nodes_to_delete = []
            
            # æ‰¾åˆ°æ‰€æœ‰å±¬æ–¼è©²æ–‡ä»¶çš„ç¯€é»ž
            for node_id, node in self.index.docstore.docs.items():
                if node.metadata.get("source") == file_id:
                    nodes_to_delete.append(node_id)
            
            # åˆªé™¤ç¯€é»ž
            for node_id in nodes_to_delete:
                del self.index.docstore.docs[node_id]
                # ä¹Ÿå¾žå‘é‡å­˜å„²ä¸­åˆªé™¤
                if hasattr(self.index.vector_store, 'delete'):
                    self.index.vector_store.delete(node_id)
            
            # ä¿å­˜ç´¢å¼•
            if hasattr(self.index, 'storage_context') and hasattr(self.index.storage_context, 'persist'):
                self.index.storage_context.persist()
            
            return len(nodes_to_delete) > 0
            
        except Exception as e:
            st.error(f"å¾ž SimpleVectorStore åˆªé™¤æ–‡ä»¶å¤±æ•—: {str(e)}")
            return False
    
    def _delete_from_filesystem(self, file_id: str):
        """å¾žæ–‡ä»¶ç³»çµ±åˆªé™¤æ–‡ä»¶ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰"""
        try:
            if os.path.exists(file_id):
                os.remove(file_id)
                print(f"âœ… å·²å¾žæ–‡ä»¶ç³»çµ±åˆªé™¤: {file_id}")
        except Exception as e:
            print(f"âš ï¸ å¾žæ–‡ä»¶ç³»çµ±åˆªé™¤æ–‡ä»¶å¤±æ•—: {str(e)}")
    
    def get_enhanced_statistics(self) -> Dict[str, Any]:
        """å–å¾—å¢žå¼·çš„çµ±è¨ˆè³‡è¨Š"""
        base_stats = self.get_document_statistics()
        
        # ç”¨æˆ¶æª”æ¡ˆçµ±è¨ˆ
        file_stats = self.file_manager.get_file_stats()
        
        # å°è©±è¨˜æ†¶çµ±è¨ˆ
        memory_stats = self.memory.get_memory_stats()
        
        # OCRå¯ç”¨æ€§
        ocr_available = self.ocr_processor.is_available()
        
        return {
            "base_statistics": base_stats,
            "user_files": file_stats,
            "conversation_memory": memory_stats,
            "ocr_available": ocr_available,
            "total_data_sources": {
                "official_documents": base_stats.get("total_documents", 0) - file_stats.get("total_files", 0),
                "user_documents": file_stats.get("document_count", 0),
                "user_images": file_stats.get("image_count", 0)
            }
        }
    
    def clear_conversation_memory(self):
        """æ¸…é™¤å°è©±è¨˜æ†¶"""
        self.memory.clear_memory()
    
    def get_memory_for_display(self):
        """å–å¾—ç”¨æ–¼é¡¯ç¤ºçš„è¨˜æ†¶å…§å®¹"""
        return self.memory.get_memory_for_display()
    
    def delete_user_file(self, filename: str) -> bool:
        """åˆªé™¤ç”¨æˆ¶æª”æ¡ˆ"""
        return self.file_manager.delete_file(filename)
