"""
Enhanced RAG System V2.0
æ•´åˆæ‰€æœ‰Phase 1-3å„ªåŒ–çš„å¢å¼·RAGç³»çµ±
"""

import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import traceback
from llama_index.core import Document, VectorStoreIndex, Settings
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.response_synthesizers import ResponseMode
from llama_index.core.postprocessor import SimilarityPostprocessor
from elasticsearch import Elasticsearch

# å°å…¥æ–°çš„å„ªåŒ–çµ„ä»¶
from src.processors.enhanced_document_processor import EnhancedDocumentProcessor
from src.indexers.hierarchical_indexer import HierarchicalIndexer
from src.retrievers.hybrid_retriever import HybridRetriever, HybridSearchConfig
from src.embeddings.multi_embedding_manager import MultiEmbeddingManager
from src.rerankers.contextual_reranker import ContextualReranker, RerankingContext
from src.rag_system.elasticsearch_rag_system import ElasticsearchRAGSystem

# é…ç½®logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedRAGSystemV2(ElasticsearchRAGSystem):
    """
    å¢å¼·RAGç³»çµ±V2.0
    æ•´åˆPhase 1-3çš„æ‰€æœ‰å„ªåŒ–åŠŸèƒ½
    """
    
    def __init__(self, elasticsearch_config: Optional[Dict] = None):
        """åˆå§‹åŒ–å¢å¼·RAGç³»çµ±V2.0"""
        
        logger.info("ğŸš€ åˆå§‹åŒ– Enhanced RAG System V2.0")
        
        # é¦–å…ˆè¨­ç½®æ­£ç¢ºçš„embeddingæ¨¡å‹
        from src.utils.jina_embedding_setup import ensure_embedding_initialized
        if not ensure_embedding_initialized():
            logger.error("âŒ Embeddingæ¨¡å‹åˆå§‹åŒ–å¤±æ•—")
            raise RuntimeError("Embeddingæ¨¡å‹åˆå§‹åŒ–å¤±æ•—")
        
        # ç„¶å¾Œåˆå§‹åŒ–åŸºç¤ç³»çµ±
        super().__init__(elasticsearch_config)
        
        # åˆå§‹åŒ–ç´¢å¼•å±¬æ€§
        self.vector_store_index = None
        
        # è¼‰å…¥å„ªåŒ–é…ç½®
        from config.config import (
            ENABLE_HIERARCHICAL_CHUNKING,
            ENABLE_HYBRID_SEARCH,
            ENABLE_MULTI_EMBEDDING,
            ENABLE_CONTEXTUAL_RERANKING,
            HYBRID_SEARCH_WEIGHTS
        )
        
        # åˆå§‹åŒ–å„ªåŒ–çµ„ä»¶
        self.enable_hierarchical_chunking = ENABLE_HIERARCHICAL_CHUNKING
        self.enable_hybrid_search = ENABLE_HYBRID_SEARCH
        self.enable_multi_embedding = ENABLE_MULTI_EMBEDDING
        self.enable_contextual_reranking = ENABLE_CONTEXTUAL_RERANKING
        
        # åˆå§‹åŒ–æ–°çµ„ä»¶
        self._initialize_v2_components()
        
        # ç¢ºä¿æœ‰åŸºæœ¬çš„ç´¢å¼•è¨­ç½®
        try:
            # å˜—è©¦å¾çˆ¶é¡ç²å–å·²åˆå§‹åŒ–çš„ç´¢å¼•
            if hasattr(self, 'index') and self.index:
                self.vector_store_index = self.index
                logger.info("âœ… ä½¿ç”¨çˆ¶é¡å·²åˆå§‹åŒ–çš„ç´¢å¼•")
            elif hasattr(self, 'vector_store') and self.vector_store and not self.vector_store_index:
                from llama_index.core import VectorStoreIndex
                self.vector_store_index = VectorStoreIndex.from_vector_store(self.vector_store)
                logger.info("âœ… Vector store index å¾ vector_store åˆå§‹åŒ–å®Œæˆ")
            elif hasattr(self, 'elasticsearch_store') and self.elasticsearch_store and not self.vector_store_index:
                # ä½¿ç”¨ elasticsearch_store ä¾†åˆå§‹åŒ–ç´¢å¼•
                from llama_index.core import VectorStoreIndex
                self.vector_store_index = VectorStoreIndex.from_vector_store(self.elasticsearch_store)
                logger.info("âœ… Vector store index å¾ elasticsearch_store åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.warning(f"âš ï¸ Vector store index åˆå§‹åŒ–å¤±æ•—: {e}")
        
        logger.info("âœ… Enhanced RAG System V2.0 åˆå§‹åŒ–å®Œæˆ")
        self._log_optimization_status()
    
    def _initialize_v2_components(self):
        """åˆå§‹åŒ–V2.0æ–°çµ„ä»¶ - ç°¡åŒ–ç‰ˆæœ¬ï¼Œåªä¿ç•™å¿…è¦çµ„ä»¶"""
        
        try:
            logger.info("ğŸ”§ ä½¿ç”¨ç°¡åŒ–æ¨¡å¼åˆå§‹åŒ– - åªä¿ç•™ Jina embedding")
            
            # åªæœ‰åœ¨å•Ÿç”¨åŠŸèƒ½æ™‚æ‰åˆå§‹åŒ–å°æ‡‰çµ„ä»¶
            if self.enable_hierarchical_chunking:
                # 1. å¢å¼·æ–‡æª”è™•ç†å™¨
                self.enhanced_processor = EnhancedDocumentProcessor()
                logger.info("ğŸ“„ Enhanced Document Processor å·²è¼‰å…¥")
                
                # 3. éšå±¤å¼ç´¢å¼•å™¨
                if self.elasticsearch_client:
                    self.hierarchical_indexer = HierarchicalIndexer(
                        elasticsearch_client=self.elasticsearch_client,
                        index_name=self.index_name,
                        processor=self.enhanced_processor
                    )
                    logger.info("ğŸ—ï¸ Hierarchical Indexer å·²è¼‰å…¥")
            else:
                logger.info("â­ï¸ è·³ééšå±¤åˆ‡å‰²çµ„ä»¶")
            
            if self.enable_multi_embedding:
                # 2. å¤šEmbeddingç®¡ç†å™¨
                self.multi_embedding_manager = MultiEmbeddingManager(
                    enable_multi_embedding=self.enable_multi_embedding
                )
                logger.info("ğŸ¯ Multi-Embedding Manager å·²è¼‰å…¥")
            else:
                logger.info("â­ï¸ è·³éå¤šEmbeddingç®¡ç†å™¨")
            
            # 4. æ··åˆæª¢ç´¢å™¨
            if self.enable_hybrid_search and self.elasticsearch_client:
                from config.config import HYBRID_SEARCH_WEIGHTS
                hybrid_config = HybridSearchConfig(
                    vector_weight=HYBRID_SEARCH_WEIGHTS.get("vector", 0.6),
                    keyword_weight=HYBRID_SEARCH_WEIGHTS.get("keyword", 0.3),
                    semantic_weight=HYBRID_SEARCH_WEIGHTS.get("semantic", 0.1)
                )
                
                # ä½¿ç”¨åŸºæœ¬çš„embeddingæ¨¡å‹è€Œä¸æ˜¯å¤šembeddingç®¡ç†å™¨
                embedding_model = getattr(self, 'multi_embedding_manager', None)
                if embedding_model:
                    embedding_model = embedding_model.models.get("general")
                
                self.hybrid_retriever = HybridRetriever(
                    elasticsearch_client=self.elasticsearch_client,
                    index_name=self.index_name,
                    config=hybrid_config,
                    embedding_model=embedding_model
                )
                logger.info("ğŸ” Hybrid Retriever å·²è¼‰å…¥")
            else:
                logger.info("â­ï¸ è·³éæ··åˆæª¢ç´¢å™¨")
            
            # 5. ä¸Šä¸‹æ–‡é‡æ’åºå™¨
            if self.enable_contextual_reranking:
                self.contextual_reranker = ContextualReranker(
                    enable_contextual_reranking=self.enable_contextual_reranking
                )
                logger.info("ğŸ¯ Contextual Reranker å·²è¼‰å…¥")
            else:
                logger.info("â­ï¸ è·³éä¸Šä¸‹æ–‡é‡æ’åºå™¨")
            
            logger.info("âœ… ç°¡åŒ–æ¨¡å¼åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ V2.0çµ„ä»¶åˆå§‹åŒ–å¤±æ•—: {e}")
            logger.info("ğŸ”„ å˜—è©¦fallbackåˆ°åŸºç¤æ¨¡å¼")
            # ä¸æ‹‹å‡ºç•°å¸¸ï¼Œå…è¨±ç³»çµ±ä»¥åŸºç¤æ¨¡å¼é‹è¡Œ
    
    def _log_optimization_status(self):
        """è¨˜éŒ„å„ªåŒ–ç‹€æ…‹"""
        logger.info("ğŸ“Š å„ªåŒ–åŠŸèƒ½ç‹€æ…‹:")
        logger.info(f"   - éšå±¤åˆ‡å‰²: {'âœ…' if self.enable_hierarchical_chunking else 'âŒ'}")
        logger.info(f"   - æ··åˆæª¢ç´¢: {'âœ…' if self.enable_hybrid_search else 'âŒ'}")
        logger.info(f"   - å¤šEmbedding: {'âœ…' if self.enable_multi_embedding else 'âŒ'}")
        logger.info(f"   - ä¸Šä¸‹æ–‡é‡æ’åº: {'âœ…' if self.enable_contextual_reranking else 'âŒ'}")
    
    def process_uploaded_file_v2(self, uploaded_file, file_manager) -> Dict[str, Any]:
        """
        V2.0æ–‡ä»¶è™•ç†æµç¨‹
        ä½¿ç”¨å¢å¼·çš„æ–‡æª”è™•ç†å’Œéšå±¤ç´¢å¼•
        """
        
        # ç²å–æ–‡ä»¶åå’Œå¤§å°ï¼ˆå…¼å®¹FastAPIå’ŒStreamlitï¼‰
        filename = getattr(uploaded_file, 'filename', getattr(uploaded_file, 'name', 'unknown'))
        file_size = getattr(uploaded_file, 'size', 0)
        
        logger.info(f"ğŸ”„ V2.0æ–‡ä»¶è™•ç†é–‹å§‹: {filename}")
        
        processing_stats = {
            "filename": filename,
            "file_size": file_size,
            "start_time": datetime.now(),
            "chunks_created": 0,
            "optimization_used": [],
            "processing_stages": {}
        }
        
        try:
            # Stage 1: åŸºç¤æ–‡ä»¶è™•ç†
            stage_start = datetime.now()
            file_path = file_manager.save_uploaded_file(uploaded_file)
            if not file_path:
                raise ValueError("æ–‡ä»¶ä¿å­˜å¤±æ•—")
            
            processing_stats["processing_stages"]["file_save"] = {
                "duration": (datetime.now() - stage_start).total_seconds(),
                "status": "success"
            }
            
            # Stage 2: æ–‡æª”è¼‰å…¥å’Œé è™•ç†
            stage_start = datetime.now()
            documents = self._load_documents_from_file(file_path)
            
            processing_stats["processing_stages"]["document_load"] = {
                "duration": (datetime.now() - stage_start).total_seconds(),
                "documents_loaded": len(documents),
                "status": "success"
            }
            
            # Stage 3: å¢å¼·æ–‡æª”è™•ç†
            if self.enable_hierarchical_chunking:
                stage_start = datetime.now()
                enhanced_documents = []
                
                for doc in documents:
                    processed_docs = self.enhanced_processor.process_document(doc)
                    enhanced_documents.extend(processed_docs)
                
                documents = enhanced_documents
                processing_stats["optimization_used"].append("hierarchical_chunking")
                processing_stats["processing_stages"]["enhanced_processing"] = {
                    "duration": (datetime.now() - stage_start).total_seconds(),
                    "chunks_created": len(documents),
                    "status": "success"
                }
                logger.info(f"ğŸ“„ éšå±¤è™•ç†å®Œæˆï¼Œç”¢ç”Ÿ {len(documents)} å€‹chunks")
            
            # Stage 4: éšå±¤å¼ç´¢å¼•
            if hasattr(self, 'hierarchical_indexer'):
                stage_start = datetime.now()
                indexing_stats = self.hierarchical_indexer.create_hierarchical_index(documents)
                
                processing_stats["optimization_used"].append("hierarchical_indexing")
                processing_stats["processing_stages"]["hierarchical_indexing"] = {
                    "duration": (datetime.now() - stage_start).total_seconds(),
                    "indexed_chunks": indexing_stats["indexed_chunks"],
                    "indexing_strategies": indexing_stats["indexing_strategies"],
                    "status": "success"
                }
                processing_stats["chunks_created"] = indexing_stats["indexed_chunks"]
                logger.info(f"ğŸ—ï¸ éšå±¤ç´¢å¼•å®Œæˆï¼Œç´¢å¼• {indexing_stats['indexed_chunks']} å€‹chunks")
            else:
                # å‚™ç”¨ï¼šå‚³çµ±ç´¢å¼•æ–¹å¼
                stage_start = datetime.now()
                index = self.create_index(documents)
                if index:
                    self.vector_store_index = index
                
                processing_stats["processing_stages"]["traditional_indexing"] = {
                    "duration": (datetime.now() - stage_start).total_seconds(),
                    "status": "success"
                }
                processing_stats["chunks_created"] = len(documents)
            
            # æ›´æ–°ç³»çµ±ç‹€æ…‹
            processing_stats["end_time"] = datetime.now()
            processing_stats["total_duration"] = (
                processing_stats["end_time"] - processing_stats["start_time"]
            ).total_seconds()
            processing_stats["status"] = "success"
            
            # è¨˜éŒ„è™•ç†çµ±è¨ˆ
            self._store_processing_stats(processing_stats)
            
            logger.info(f"âœ… V2.0æ–‡ä»¶è™•ç†å®Œæˆ: {filename}")
            logger.info(f"   - ç¸½è€—æ™‚: {processing_stats['total_duration']:.2f}ç§’")
            logger.info(f"   - ç”¢ç”Ÿchunks: {processing_stats['chunks_created']}")
            logger.info(f"   - ä½¿ç”¨å„ªåŒ–: {', '.join(processing_stats['optimization_used'])}")
            
            return processing_stats
            
        except Exception as e:
            logger.error(f"âŒ V2.0æ–‡ä»¶è™•ç†å¤±æ•—: {e}")
            processing_stats["status"] = "error"
            processing_stats["error"] = str(e)
            processing_stats["end_time"] = datetime.now()
            return processing_stats
    
    def query_with_sources_v2(
        self,
        question: str,
        conversation_history: List[Dict[str, str]] = None,
        user_preferences: Dict[str, Any] = None,
        max_sources: int = 3
    ) -> Dict[str, Any]:
        """
        V2.0æ™ºèƒ½æŸ¥è©¢ï¼Œæ•´åˆæ‰€æœ‰å„ªåŒ–åŠŸèƒ½
        """
        
        logger.info(f"ğŸ” V2.0æ™ºèƒ½æŸ¥è©¢é–‹å§‹: {question}")
        
        query_stats = {
            "question": question,
            "start_time": datetime.now(),
            "optimization_used": [],
            "query_stages": {},
            "sources_found": 0
        }
        
        try:
            # Stage 1: æŸ¥è©¢é è™•ç†å’Œæ„åœ–åˆ†æ
            stage_start = datetime.now()
            
            # åˆ†ææœç´¢æ„åœ–
            from src.rerankers.contextual_reranker import SemanticAnalyzer
            analyzer = SemanticAnalyzer()
            search_intent = analyzer.analyze_search_intent(question)
            
            query_stats["query_stages"]["preprocessing"] = {
                "duration": (datetime.now() - stage_start).total_seconds(),
                "search_intent": search_intent,
                "status": "success"
            }
            
            # Stage 2: æ™ºèƒ½æª¢ç´¢
            stage_start = datetime.now()
            
            if self.enable_hybrid_search and hasattr(self, 'hybrid_retriever'):
                # ä½¿ç”¨æ··åˆæª¢ç´¢
                from llama_index.core.schema import QueryBundle
                query_bundle = QueryBundle(query_str=question)
                retrieved_nodes = self.hybrid_retriever.retrieve(query_bundle)
                query_stats["optimization_used"].append("hybrid_search")
                logger.info(f"ğŸ” æ··åˆæª¢ç´¢æ‰¾åˆ° {len(retrieved_nodes)} å€‹çµæœ")
            else:
                # å‚™ç”¨ï¼šå‚³çµ±æª¢ç´¢
                if self.vector_store_index:
                    retriever = self.vector_store_index.as_retriever(similarity_top_k=max_sources*2)
                    retrieved_nodes = retriever.retrieve(question)
                else:
                    raise ValueError("æ²’æœ‰å¯ç”¨çš„æª¢ç´¢å™¨")
            
            query_stats["query_stages"]["retrieval"] = {
                "duration": (datetime.now() - stage_start).total_seconds(),
                "nodes_retrieved": len(retrieved_nodes),
                "status": "success"
            }
            
            # Stage 3: ä¸Šä¸‹æ–‡é‡æ’åº
            stage_start = datetime.now()
            
            if self.enable_contextual_reranking and retrieved_nodes:
                # æ§‹å»ºé‡æ’åºä¸Šä¸‹æ–‡
                reranking_context = RerankingContext(
                    query=question,
                    conversation_history=conversation_history or [],
                    user_preferences=user_preferences or {},
                    domain_context="general",
                    search_intent=search_intent
                )
                
                # åŸ·è¡Œé‡æ’åº
                reranked_nodes = self.contextual_reranker.rerank_results(
                    retrieved_nodes, reranking_context
                )
                retrieved_nodes = reranked_nodes
                query_stats["optimization_used"].append("contextual_reranking")
                logger.info(f"ğŸ¯ é‡æ’åºå®Œæˆï¼Œæœ€çµ‚ {len(retrieved_nodes)} å€‹çµæœ")
            
            query_stats["query_stages"]["reranking"] = {
                "duration": (datetime.now() - stage_start).total_seconds(),
                "final_nodes": len(retrieved_nodes),
                "status": "success"
            }
            
            # Stage 4: ç­”æ¡ˆç”Ÿæˆ
            stage_start = datetime.now()
            
            # é™åˆ¶æœ€çµ‚çµæœæ•¸é‡
            final_nodes = retrieved_nodes[:max_sources]
            
            # ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
            context_str = self._build_context_from_nodes(final_nodes)
            answer = self._generate_answer_with_context(question, context_str, conversation_history)
            
            # æ§‹å»ºä¾†æºä¿¡æ¯
            sources = []
            for i, node in enumerate(final_nodes):
                source_info = {
                    "source": node.node.metadata.get("source", f"æ–‡æª”_{i+1}"),
                    "file_path": node.node.metadata.get("file_path", ""),
                    "score": float(node.score),
                    "content": node.node.text[:200] + "..." if len(node.node.text) > 200 else node.node.text,
                    "page": node.node.metadata.get("page", ""),
                    "type": "enhanced_document"
                }
                sources.append(source_info)
            
            query_stats["query_stages"]["answer_generation"] = {
                "duration": (datetime.now() - stage_start).total_seconds(),
                "answer_length": len(answer),
                "sources_used": len(sources),
                "status": "success"
            }
            
            # å®Œæˆçµ±è¨ˆ
            query_stats["end_time"] = datetime.now()
            query_stats["total_duration"] = (
                query_stats["end_time"] - query_stats["start_time"]
            ).total_seconds()
            query_stats["sources_found"] = len(sources)
            query_stats["status"] = "success"
            
            # æ§‹å»ºéŸ¿æ‡‰
            response = {
                "answer": answer,
                "sources": sources,
                "metadata": {
                    "query": question,
                    "total_sources": len(sources),
                    "response_time_ms": int(query_stats["total_duration"] * 1000),
                    "model": "Enhanced RAG V2.0",
                    "backend": "Elasticsearch + Multi-Optimization",
                    "optimization_used": query_stats["optimization_used"],
                    "performance": {
                        "total_stages": len(query_stats["query_stages"]),
                        "total_time": query_stats["total_duration"],
                        "stages": [
                            {
                                "stage": stage_name,
                                "duration": stage_data["duration"],
                                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
                            }
                            for stage_name, stage_data in query_stats["query_stages"].items()
                        ]
                    }
                }
            }
            
            logger.info(f"âœ… V2.0æ™ºèƒ½æŸ¥è©¢å®Œæˆ")
            logger.info(f"   - ç¸½è€—æ™‚: {query_stats['total_duration']:.3f}ç§’")
            logger.info(f"   - ä¾†æºæ•¸é‡: {len(sources)}")
            logger.info(f"   - ä½¿ç”¨å„ªåŒ–: {', '.join(query_stats['optimization_used'])}")
            
            return response
            
        except Exception as e:
            logger.error(f"âŒ V2.0æ™ºèƒ½æŸ¥è©¢å¤±æ•—: {e}")
            traceback.print_exc()
            
            # è¿”å›éŒ¯èª¤éŸ¿æ‡‰
            return {
                "answer": "æŠ±æ­‰ï¼ŒæŸ¥è©¢è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ã€‚è«‹ç¨å¾Œå†è©¦ã€‚",
                "sources": [],
                "metadata": {
                    "error": str(e),
                    "status": "error",
                    "timestamp": datetime.now().isoformat()
                }
            }
    
    def _build_context_from_nodes(self, nodes) -> str:
        """å¾ç¯€é»æ§‹å»ºä¸Šä¸‹æ–‡"""
        context_parts = []
        for i, node in enumerate(nodes, 1):
            source = node.node.metadata.get("source", f"æ–‡æª”{i}")
            content = node.node.text
            context_parts.append(f"[ä¾†æº {i}: {source}]\n{content}")
        
        return "\n\n".join(context_parts)
    
    def _generate_answer_with_context(
        self, 
        question: str, 
        context: str, 
        conversation_history: List[Dict[str, str]] = None
    ) -> str:
        """åŸºæ–¼ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ"""
        
        # æ§‹å»ºå°è©±æ­·å²
        history_context = ""
        if conversation_history:
            recent_history = conversation_history[-3:]  # æœ€è¿‘3è¼ªå°è©±
            history_parts = []
            for msg in recent_history:
                role = "ç”¨æˆ¶" if msg.get("role") == "user" else "åŠ©ç†"
                content = msg.get("content", "")
                history_parts.append(f"{role}: {content}")
            
            if history_parts:
                history_context = "å°è©±æ­·å²:\n" + "\n".join(history_parts) + "\n\n"
        
        # æ§‹å»ºå®Œæ•´æç¤º
        prompt = f"""{history_context}è«‹åŸºæ–¼ä»¥ä¸‹æä¾›çš„è³‡æ–™å›ç­”å•é¡Œï¼Œå¦‚æœè³‡æ–™ä¸­æ²’æœ‰ç›¸é—œä¿¡æ¯ï¼Œè«‹æ˜ç¢ºèªªæ˜ã€‚

å•é¡Œ: {question}

åƒè€ƒè³‡æ–™:
{context}

è«‹æä¾›æº–ç¢ºã€è©³ç´°çš„ç­”æ¡ˆ:"""
        
        try:
            # ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
            llm = Settings.llm
            response = llm.complete(prompt)
            return response.text.strip()
            
        except Exception as e:
            logger.error(f"âŒ ç­”æ¡ˆç”Ÿæˆå¤±æ•—: {e}")
            return "æŠ±æ­‰ï¼Œç„¡æ³•åŸºæ–¼æä¾›çš„è³‡æ–™ç”Ÿæˆç­”æ¡ˆã€‚è«‹å˜—è©¦é‡æ–°è¡¨é”æ‚¨çš„å•é¡Œã€‚"
    
    def _store_processing_stats(self, stats: Dict[str, Any]):
        """å„²å­˜è™•ç†çµ±è¨ˆ"""
        try:
            # å°‡çµ±è¨ˆä¿¡æ¯å„²å­˜åˆ°ç³»çµ±ç‹€æ…‹ä¸­
            if not hasattr(self, 'processing_history'):
                self.processing_history = []
            
            self.processing_history.append(stats)
            
            # åªä¿ç•™æœ€è¿‘20æ¬¡è™•ç†è¨˜éŒ„
            if len(self.processing_history) > 20:
                self.processing_history = self.processing_history[-20:]
                
        except Exception as e:
            logger.warning(f"âš ï¸ çµ±è¨ˆå„²å­˜å¤±æ•—: {e}")
    
    def get_system_performance_stats(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±æ€§èƒ½çµ±è¨ˆ"""
        
        stats = {
            "system_version": "Enhanced RAG System V2.0",
            "optimization_status": {
                "hierarchical_chunking": self.enable_hierarchical_chunking,
                "hybrid_search": self.enable_hybrid_search,
                "multi_embedding": self.enable_multi_embedding,
                "contextual_reranking": self.enable_contextual_reranking
            },
            "processing_history": getattr(self, 'processing_history', []),
            "component_status": {}
        }
        
        # æª¢æŸ¥å„çµ„ä»¶ç‹€æ…‹
        if hasattr(self, 'multi_embedding_manager'):
            stats["component_status"]["multi_embedding_manager"] = {
                "available_models": list(self.multi_embedding_manager.models.keys()),
                "model_info": self.multi_embedding_manager.get_model_info()
            }
        
        if hasattr(self, 'hierarchical_indexer'):
            stats["component_status"]["hierarchical_indexer"] = {
                "index_statistics": self.hierarchical_indexer.get_index_statistics()
            }
        
        return stats
    
    def optimize_system_performance(self):
        """å„ªåŒ–ç³»çµ±æ€§èƒ½"""
        logger.info("ğŸ”§ é–‹å§‹ç³»çµ±æ€§èƒ½å„ªåŒ–")
        
        try:
            # å„ªåŒ–Elasticsearchç´¢å¼•
            if hasattr(self, 'hierarchical_indexer'):
                self.hierarchical_indexer.optimize_index()
            
            # æ¸…ç†è™•ç†æ­·å²ï¼ˆä¿ç•™æœ‰ç”¨çµ±è¨ˆï¼‰
            if hasattr(self, 'processing_history') and len(self.processing_history) > 50:
                self.processing_history = self.processing_history[-20:]
            
            logger.info("âœ… ç³»çµ±æ€§èƒ½å„ªåŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ç³»çµ±æ€§èƒ½å„ªåŒ–å¤±æ•—: {e}")
    
    def benchmark_v2_system(self, test_queries: List[str]) -> Dict[str, Any]:
        """V2.0ç³»çµ±åŸºæº–æ¸¬è©¦"""
        
        logger.info(f"ğŸ§ª é–‹å§‹V2.0ç³»çµ±åŸºæº–æ¸¬è©¦ï¼ŒæŸ¥è©¢æ•¸é‡: {len(test_queries)}")
        
        benchmark_results = {
            "test_start_time": datetime.now(),
            "total_queries": len(test_queries),
            "query_results": [],
            "performance_summary": {}
        }
        
        for i, query in enumerate(test_queries):
            logger.info(f"æ¸¬è©¦æŸ¥è©¢ {i+1}/{len(test_queries)}: {query}")
            
            query_start = datetime.now()
            result = self.query_with_sources_v2(query)
            query_duration = (datetime.now() - query_start).total_seconds()
            
            query_result = {
                "query_index": i,
                "query": query,
                "duration": query_duration,
                "sources_found": len(result.get("sources", [])),
                "optimization_used": result.get("metadata", {}).get("optimization_used", []),
                "success": result.get("metadata", {}).get("status") != "error"
            }
            
            benchmark_results["query_results"].append(query_result)
        
        # è¨ˆç®—æ€§èƒ½æ‘˜è¦
        successful_queries = [r for r in benchmark_results["query_results"] if r["success"]]
        
        if successful_queries:
            durations = [r["duration"] for r in successful_queries]
            sources_counts = [r["sources_found"] for r in successful_queries]
            
            benchmark_results["performance_summary"] = {
                "success_rate": len(successful_queries) / len(test_queries),
                "average_query_time": sum(durations) / len(durations),
                "min_query_time": min(durations),
                "max_query_time": max(durations),
                "average_sources_found": sum(sources_counts) / len(sources_counts),
                "optimization_usage": {}
            }
            
            # çµ±è¨ˆå„ªåŒ–åŠŸèƒ½ä½¿ç”¨æƒ…æ³
            all_optimizations = []
            for result in successful_queries:
                all_optimizations.extend(result["optimization_used"])
            
            from collections import Counter
            opt_counter = Counter(all_optimizations)
            total_uses = sum(opt_counter.values())
            
            if total_uses > 0:
                benchmark_results["performance_summary"]["optimization_usage"] = {
                    opt: count/total_uses for opt, count in opt_counter.items()
                }
        
        benchmark_results["test_end_time"] = datetime.now()
        benchmark_results["total_test_duration"] = (
            benchmark_results["test_end_time"] - benchmark_results["test_start_time"]
        ).total_seconds()
        
        logger.info("âœ… V2.0ç³»çµ±åŸºæº–æ¸¬è©¦å®Œæˆ")
        logger.info(f"   - æˆåŠŸç‡: {benchmark_results['performance_summary'].get('success_rate', 0)*100:.1f}%")
        logger.info(f"   - å¹³å‡æŸ¥è©¢æ™‚é–“: {benchmark_results['performance_summary'].get('average_query_time', 0):.3f}ç§’")
        
        return benchmark_results
    
    def _load_documents_from_file(self, file_path: str) -> List:
        """
        å¾æ–‡ä»¶è¼‰å…¥æ–‡æª”
        æ”¯æ´å¤šç¨®æ–‡ä»¶æ ¼å¼ï¼šPDFã€TXTã€DOCXã€MDç­‰
        """
        from llama_index.core import Document, SimpleDirectoryReader
        import os
        
        logger.info(f"ğŸ“ è¼‰å…¥æ–‡æª”: {file_path}")
        
        try:
            # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            # ç²å–æ–‡ä»¶æ“´å±•å
            file_ext = os.path.splitext(file_path)[1].lower()
            
            if file_ext == '.txt':
                # è™•ç†ç´”æ–‡æœ¬æ–‡ä»¶
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                document = Document(
                    text=content,
                    metadata={
                        "file_path": file_path,
                        "file_name": os.path.basename(file_path),
                        "file_type": "text",
                        "source": file_path
                    }
                )
                documents = [document]
                
            elif file_ext == '.md':
                # è™•ç†Markdownæ–‡ä»¶
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                document = Document(
                    text=content,
                    metadata={
                        "file_path": file_path,
                        "file_name": os.path.basename(file_path),
                        "file_type": "markdown",
                        "source": file_path
                    }
                )
                documents = [document]
                
            else:
                # ä½¿ç”¨SimpleDirectoryReaderè™•ç†å…¶ä»–æ ¼å¼
                reader = SimpleDirectoryReader(
                    input_files=[file_path],
                    required_exts=[file_ext]
                )
                raw_documents = reader.load_data()
                
                # å°æ–¼PDFç­‰å¤šé é¢æ–‡æª”ï¼Œåˆä½µé é¢å…§å®¹
                if file_ext == '.pdf' and len(raw_documents) > 1:
                    logger.info(f"ğŸ“„ PDFåŒ…å« {len(raw_documents)} é ï¼Œé€²è¡Œé é¢åˆä½µ...")
                    
                    # åˆä½µæ‰€æœ‰é é¢çš„æ–‡æœ¬
                    combined_text = ""
                    page_contents = []
                    
                    for i, doc in enumerate(raw_documents):
                        page_num = i + 1
                        page_text = doc.text.strip()
                        
                        # è·³éç©ºé é¢
                        if not page_text:
                            continue
                            
                        # æ·»åŠ é é¢åˆ†éš”ç¬¦
                        page_contents.append(f"\n--- ç¬¬{page_num}é  ---\n{page_text}")
                    
                    combined_text = "\n".join(page_contents)
                    
                    # å‰µå»ºåˆä½µå¾Œçš„æ–‡æª”
                    combined_doc = Document(
                        text=combined_text,
                        metadata={
                            "file_path": file_path,
                            "file_name": os.path.basename(file_path),
                            "file_type": "pdf",
                            "source": file_path,
                            "total_pages": len(raw_documents),
                            "content_length": len(combined_text),
                            "processing_method": "page_merged"
                        }
                    )
                    
                    documents = [combined_doc]
                    logger.info(f"âœ… PDFé é¢åˆä½µå®Œæˆï¼Œç¸½æ–‡æœ¬é•·åº¦: {len(combined_text)} å­—ç¬¦")
                    
                else:
                    # å°æ–¼å–®é æ–‡æª”æˆ–éPDFï¼Œä¿æŒåŸæ¨£
                    documents = raw_documents
                    
                    # æ›´æ–°å…ƒæ•¸æ“š
                    for doc in documents:
                        if hasattr(doc, 'metadata'):
                            doc.metadata.update({
                                "file_path": file_path,
                                "file_name": os.path.basename(file_path),
                                "source": file_path
                            })
            
            logger.info(f"âœ… æˆåŠŸè¼‰å…¥æ–‡æª”: {len(documents)} å€‹æ–‡æª”")
            return documents
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æª”è¼‰å…¥å¤±æ•—: {e}")
            # å‰µå»ºä¸€å€‹éŒ¯èª¤æ–‡æª”
            error_doc = Document(
                text=f"æ–‡æª”è¼‰å…¥å¤±æ•—: {str(e)}",
                metadata={
                    "file_path": file_path,
                    "file_name": os.path.basename(file_path),
                    "file_type": "error",
                    "error": str(e),
                    "source": file_path
                }
            )
            return [error_doc]
    
    def get_indexed_files(self) -> List[Dict[str, Any]]:
        """
        ç²å–å·²ç´¢å¼•æ–‡ä»¶çš„åˆ—è¡¨å’Œç‹€æ…‹ä¿¡æ¯
        Returns:
            List[Dict]: åŒ…å«æ–‡ä»¶ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        """
        try:
            if not self.vector_store_index:
                logger.warning("âš ï¸ å‘é‡å­˜å„²æœªåˆå§‹åŒ–")
                return []
            
            # å˜—è©¦å¾ Elasticsearch ç²å–æ–‡ä»¶ä¿¡æ¯
            vector_store = getattr(self.vector_store_index, 'vector_store', None)
            if vector_store and hasattr(vector_store, '_client') and hasattr(vector_store, '_index_name'):
                try:
                    # ä½¿ç”¨èšåˆæŸ¥è©¢ç²å–å”¯ä¸€æ–‡ä»¶ä¿¡æ¯
                    query = {
                        "size": 0,
                        "aggs": {
                            "unique_files": {
                                "terms": {
                                    "field": "metadata.file_name",
                                    "size": 1000
                                },
                                "aggs": {
                                    "file_info": {
                                        "top_hits": {
                                            "size": 1,
                                            "_source": ["metadata"],
                                            "sort": [{"_timestamp": {"order": "desc"}}]
                                        }
                                    },
                                    "chunk_count": {
                                        "value_count": {
                                            "field": "_id"
                                        }
                                    }
                                }
                            }
                        }
                    }
                    
                    response = vector_store._client.search(
                        index=vector_store._index_name,
                        body=query
                    )
                    
                    files = []
                    for bucket in response['aggregations']['unique_files']['buckets']:
                        file_name = bucket['key']
                        chunk_count = bucket['chunk_count']['value']
                        
                        # ç²å–æ–‡ä»¶å…ƒæ•¸æ“š
                        hits = bucket['file_info']['hits']['hits']
                        if hits:
                            metadata = hits[0]['_source']['metadata']
                            
                            # è¨ˆç®—æ–‡ä»¶å¤§å°ï¼ˆä¼°ç®—ï¼‰
                            file_size_mb = metadata.get('content_length', 0) / (1024 * 1024) if metadata.get('content_length') else 0.0
                            
                            files.append({
                                'id': metadata.get('file_path', file_name),
                                'name': file_name,
                                'size_mb': round(file_size_mb, 2),
                                'type': metadata.get('file_type', 'unknown'),
                                'upload_time': metadata.get('upload_time', ''),
                                'node_count': chunk_count,
                                'status': 'active',
                                'source': metadata.get('source', ''),
                                'processing_method': metadata.get('processing_method', 'standard')
                            })
                    
                    logger.info(f"ğŸ“‚ ç²å–åˆ° {len(files)} å€‹å·²ç´¢å¼•æ–‡ä»¶")
                    return files
                    
                except Exception as es_error:
                    logger.warning(f"âš ï¸ ElasticsearchæŸ¥è©¢å¤±æ•—: {es_error}")
                    # é™ç´šåˆ°åŸºæœ¬å¯¦ç¾
                    pass
            
            # åŸºæœ¬å¯¦ç¾ï¼šå˜—è©¦å¾ç´¢å¼•ä¸­ç²å–åŸºæœ¬ä¿¡æ¯
            try:
                # ä½¿ç”¨VectorStoreIndexçš„æŸ¥è©¢åŠŸèƒ½
                if self.vector_store_index:
                    # é€šéæŸ¥è©¢ç²å–æ–‡æª”
                    retriever = self.vector_store_index.as_retriever(similarity_top_k=100)
                    docs = retriever.retrieve("test query for listing files")
                    
                    # æŒ‰æ–‡ä»¶ååˆ†çµ„
                    files_dict = {}
                    for doc in docs:
                        if hasattr(doc, 'metadata'):
                            metadata = doc.metadata
                            file_name = metadata.get('file_name', 'unknown')
                            if file_name not in files_dict:
                                files_dict[file_name] = {
                                    'id': metadata.get('file_path', file_name),
                                    'name': file_name,
                                    'size_mb': 0.0,
                                    'type': metadata.get('file_type', 'unknown'),
                                    'upload_time': metadata.get('upload_time', ''),
                                    'node_count': 0,
                                    'status': 'active'
                                }
                            
                            # ç´¯è¨ˆçµ±è¨ˆ
                            files_dict[file_name]['node_count'] += 1
                            # å˜—è©¦å¾æ–‡æœ¬é•·åº¦ä¼°ç®—å¤§å°
                            if hasattr(doc, 'text'):
                                text_size_mb = len(doc.text.encode('utf-8')) / (1024 * 1024)
                                files_dict[file_name]['size_mb'] += text_size_mb
                    
                    # è½‰æ›ç‚ºåˆ—è¡¨
                    files = list(files_dict.values())
                    for file_info in files:
                        file_info['size_mb'] = round(file_info['size_mb'], 2)
                    
                    return files
            
            except Exception as fallback_error:
                logger.warning(f"âš ï¸ åŸºæœ¬æŸ¥è©¢ä¹Ÿå¤±æ•—: {fallback_error}")
            
            return []
            
        except Exception as e:
            logger.error(f"âŒ ç²å–å·²ç´¢å¼•æ–‡ä»¶åˆ—è¡¨å¤±æ•—: {e}")
            return []
    
    def get_document_statistics(self) -> Dict[str, Any]:
        """
        ç²å–æ–‡æª”çµ±è¨ˆä¿¡æ¯
        Returns:
            Dict: åŒ…å«çµ±è¨ˆä¿¡æ¯çš„å­—å…¸
        """
        try:
            files = self.get_indexed_files()
            total_files = len(files)
            total_chunks = sum(file_info.get('node_count', 0) for file_info in files)
            total_size_mb = sum(file_info.get('size_mb', 0.0) for file_info in files)
            
            return {
                'total_files': total_files,
                'total_documents': total_chunks,
                'total_size_mb': round(total_size_mb, 2)
            }
            
        except Exception as e:
            logger.error(f"âŒ ç²å–æ–‡æª”çµ±è¨ˆä¿¡æ¯å¤±æ•—: {e}")
            return {
                'total_files': 0,
                'total_documents': 0,
                'total_size_mb': 0.0
            }