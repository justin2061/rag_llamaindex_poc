import os
from typing import List, Optional
from llama_index.core import VectorStoreIndex, Document, Settings, load_index_from_storage

# æ¢ä»¶æ€§å°å…¥ streamlitï¼Œå¦‚æœä¸å¯ç”¨å‰‡ä½¿ç”¨ mock
try:
    import streamlit as st
    HAS_STREAMLIT = True
except ImportError:
    class MockStreamlit:
        def info(self, msg): print(f"INFO: {msg}")
        def success(self, msg): print(f"SUCCESS: {msg}")
        def warning(self, msg): print(f"WARNING: {msg}")
        def error(self, msg): print(f"ERROR: {msg}")
    st = MockStreamlit()
    HAS_STREAMLIT = False
from llama_index.core.storage.storage_context import StorageContext
from llama_index.llms.groq import Groq
import requests
import numpy as np
from typing import List
from llama_index.core.embeddings import BaseEmbedding
from llama_index.core.node_parser import SimpleNodeParser

# å°å…¥æ–°çš„åŠŸèƒ½æ¨¡çµ„
from ..storage.conversation_memory import ConversationMemory
from ..processors.user_file_manager import UserFileManager
from ..processors.gemini_ocr import GeminiOCRProcessor

# å˜—è©¦å°å…¥ä¸åŒçš„PDFè™•ç†åº«
PDF_READER = None
PDF_READER_TYPE = None

try:
    from llama_index.readers.file import PyMuPDFReader
    PDF_READER = PyMuPDFReader()
    PDF_READER_TYPE = "PyMuPDF"
except ImportError:
    try:
        import PyPDF2
        PDF_READER_TYPE = "PyPDF2"
    except ImportError:
        try:
            import pdfplumber
            PDF_READER_TYPE = "pdfplumber"
        except ImportError:
            st.error("æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„PDFè™•ç†åº«ï¼Œè«‹å®‰è£ PyMuPDFã€PyPDF2 æˆ– pdfplumber")

from config.config import (
    GROQ_API_KEY, LLM_MODEL, INDEX_DIR, JINA_API_KEY, ELASTICSEARCH_VECTOR_DIMENSION
)

class JinaEmbeddingAPI(BaseEmbedding):
    """è‡ªå®šç¾© Jina Embedding API é¡åˆ¥"""
    
    def __init__(self, api_key: str, model: str = "jina-embeddings-v3", task: str = "text-matching"):
        super().__init__()
        self.api_key = api_key
        self.model = model
        self.task = task
        self.url = 'https://api.jina.ai/v1/embeddings'
        # èˆ‡ Elasticsearch å‘é‡ç¶­åº¦ä¿æŒä¸€è‡´ï¼ˆé¿å…ç¶­åº¦ä¸åŒ¹é…ï¼‰
        self.embed_dim = ELASTICSEARCH_VECTOR_DIMENSION
        
    def _get_text_embedding(self, text: str) -> List[float]:
        """ç²å–å–®å€‹æ–‡æœ¬çš„åµŒå…¥å‘é‡"""
        return self._get_text_embeddings([text])[0]
    
    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        """ç²å–å¤šå€‹æ–‡æœ¬çš„åµŒå…¥å‘é‡"""
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.api_key}'
        }
        data = {
            "model": self.model,
            "task": self.task,
            "truncate": True,
            "input": texts
        }
        
        try:
            response = requests.post(self.url, headers=headers, json=data)
            response.raise_for_status()
            result = response.json()
            
            # æå–åµŒå…¥å‘é‡
            embeddings = []
            for item in result.get('data', []):
                embeddings.append(item.get('embedding', []))
            
            return embeddings
        except Exception as e:
            st.error(f"Jina API èª¿ç”¨å¤±æ•—: {str(e)}")
            st.warning("âš ï¸ å¾Œå‚™æ–¹æ¡ˆï¼šä½¿ç”¨ç°¡å–®çš„æ–‡æœ¬ç‰¹å¾µå‘é‡ï¼ˆåŠŸèƒ½æœ‰é™ï¼‰")
            # å¦‚æœ API å¤±æ•—ï¼Œä½¿ç”¨ç°¡å–®çš„æ–‡æœ¬ç‰¹å¾µä½œç‚ºå¾Œå‚™
            return [self._simple_text_embedding(text) for text in texts]
    
    def _simple_text_embedding(self, text: str) -> List[float]:
        """ç°¡å–®çš„æ–‡æœ¬ç‰¹å¾µå‘é‡ï¼ˆå¾Œå‚™æ–¹æ¡ˆï¼‰ - èˆ‡è¨­å®šç¶­åº¦å°é½Š"""
        import hashlib
        
        # ç¢ºä¿æ–‡æœ¬ä¸ç‚ºç©º
        if not text.strip():
            text = "empty"
        
        # åŸºæ–¼æ–‡æœ¬å…§å®¹ç”Ÿæˆä¸€è‡´çš„ç‰¹å¾µå‘é‡ï¼ˆä½¿ç”¨ sha256ï¼Œé•·åº¦64çš„åå…­é€²åˆ¶ï¼‰
        text_hash = hashlib.sha256(text.encode()).hexdigest()
        
        dim = getattr(self, "embed_dim", ELASTICSEARCH_VECTOR_DIMENSION)
        embedding: List[float] = []
        
        # ä»¥8ç‚ºæ­¥é•·ç”Ÿæˆå€¼ï¼Œç›´åˆ°é”åˆ°æŒ‡å®šç¶­åº¦
        for i in range(dim // 8):  # æ¯8å€‹å€¼ç‚ºä¸€çµ„
            hex_start = (i * 2) % 64
            hex_val = int(text_hash[hex_start:hex_start+2], 16)
            for j in range(8):
                value = (hex_val + j * 31) % 256
                normalized = (value / 255.0) * 2 - 1  # æ­¸ä¸€åŒ–åˆ° [-1, 1]
                embedding.append(normalized)
        
        # è£œé½Šæˆ–æˆªæ–·åˆ°æŒ‡å®šç¶­åº¦
        while len(embedding) < dim:
            embedding.append(0.0)
        
        return embedding[:dim]
    
    async def _aget_text_embedding(self, text: str) -> List[float]:
        """ç•°æ­¥ç²å–æ–‡æœ¬åµŒå…¥ï¼ˆå›é€€åˆ°åŒæ­¥æ–¹æ³•ï¼‰"""
        return self._get_text_embedding(text)
    
    def _get_query_embedding(self, query: str) -> List[float]:
        """ç²å–æŸ¥è©¢åµŒå…¥å‘é‡"""
        return self._get_text_embedding(query)
    
    async def _aget_query_embedding(self, query: str) -> List[float]:
        """ç•°æ­¥ç²å–æŸ¥è©¢åµŒå…¥å‘é‡"""
        return self._get_query_embedding(query)

def load_pdf_with_pypdf2(pdf_path: str) -> List[Document]:
    """ä½¿ç”¨PyPDF2è¼‰å…¥PDF"""
    import PyPDF2
    documents = []
    
    with open(pdf_path, 'rb') as file:
        pdf_reader = PyPDF2.PdfReader(file)
        text = ""
        for page_num, page in enumerate(pdf_reader.pages):
            text += page.extract_text() + "\n"
        
        doc = Document(
            text=text,
            metadata={"source": os.path.basename(pdf_path), "pages": len(pdf_reader.pages)}
        )
        documents.append(doc)
    
    return documents

def load_pdf_with_pdfplumber(pdf_path: str) -> List[Document]:
    """ä½¿ç”¨pdfplumberè¼‰å…¥PDF"""
    import pdfplumber
    documents = []
    
    with pdfplumber.open(pdf_path) as pdf:
        text = ""
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
        
        doc = Document(
            text=text,
            metadata={"source": os.path.basename(pdf_path), "pages": len(pdf.pages)}
        )
        documents.append(doc)
    
    return documents

class RAGSystem:
    def __init__(self):
        self.index = None
        self.query_engine = None
        self.models_initialized = False
        # å»¶é²åˆå§‹åŒ–æ¨¡å‹ï¼Œé¿å…åœ¨é é¢è¼‰å…¥æ™‚å°±åˆå§‹åŒ–
        
    def _setup_models(self):
        """è¨­å®šæ¨¡å‹"""
        # è¨­å®šLLM
        if GROQ_API_KEY:
            llm = Groq(model=LLM_MODEL, api_key=GROQ_API_KEY)
        else:
            st.error("è«‹è¨­å®šGROQ_API_KEYç’°å¢ƒè®Šæ•¸")
            return
        
        # è¨­å®š Embedding æ¨¡å‹ - ç¸½æ˜¯ä½¿ç”¨æˆ‘å€‘çš„è‡ªå®šç¾©å¯¦ä½œ
        if JINA_API_KEY and JINA_API_KEY.strip():
            st.info("ğŸš€ ä½¿ç”¨ Jina Embedding API")
            embed_model = JinaEmbeddingAPI(
                api_key=JINA_API_KEY,
                model="jina-embeddings-v3",
                task="text-matching"
            )
        else:
            st.warning("âš ï¸ æœªè¨­å®š JINA_API_KEYï¼Œå°‡ä½¿ç”¨ç°¡å–®ç‰¹å¾µå‘é‡ä½œç‚ºå¾Œå‚™")
            st.info("ğŸ’¡ å»ºè­°è¨­å®š JINA_API_KEY ä»¥ç²å¾—æ›´å¥½çš„ embedding æ•ˆæœ")
            # ä½¿ç”¨è‡ªå®šç¾©çš„ç°¡å–®å¾Œå‚™æ–¹æ¡ˆ
            embed_model = JinaEmbeddingAPI(
                api_key="dummy",  # å°‡æœƒè§¸ç™¼å¾Œå‚™æ–¹æ¡ˆ
                model="jina-embeddings-v3",
                task="text-matching"
            )
        
        # å…ˆè¨­å®š embedding æ¨¡å‹ï¼Œå†è¨­å®šå…¶ä»–
        Settings.embed_model = embed_model
        Settings.llm = llm  
        Settings.node_parser = SimpleNodeParser.from_defaults(chunk_size=1024)
        
        # è¨­ç½®æ¨™è¨˜é¿å… LlamaIndex å˜—è©¦ä½¿ç”¨é è¨­çš„ OpenAI embedding
        self.models_initialized = True
    
    def load_pdfs(self, pdf_paths: List[str]) -> List[Document]:
        """è¼‰å…¥PDFæª”æ¡ˆ - æ”¯æ´å¤šç¨®PDFè™•ç†åº«"""
        documents = []
        
        # é¡¯ç¤ºä½¿ç”¨çš„PDFè™•ç†åº«
        st.info(f"ä½¿ç”¨ {PDF_READER_TYPE} è™•ç†PDFæª”æ¡ˆ")
        
        with st.spinner("æ­£åœ¨è¼‰å…¥PDFæª”æ¡ˆ..."):
            for pdf_path in pdf_paths:
                try:
                    if PDF_READER_TYPE == "PyMuPDF":
                        docs = PDF_READER.load_data(file_path=pdf_path)
                        # ç‚ºæ¯å€‹æ–‡ä»¶æ·»åŠ å…ƒæ•¸æ“š
                        for doc in docs:
                            doc.metadata["source"] = os.path.basename(pdf_path)
                        documents.extend(docs)
                    
                    elif PDF_READER_TYPE == "PyPDF2":
                        docs = load_pdf_with_pypdf2(pdf_path)
                        documents.extend(docs)
                    
                    elif PDF_READER_TYPE == "pdfplumber":
                        docs = load_pdf_with_pdfplumber(pdf_path)
                        documents.extend(docs)
                    
                    else:
                        st.error("æ²’æœ‰å¯ç”¨çš„PDFè™•ç†åº«")
                        return []
                    
                    st.info(f"æˆåŠŸè¼‰å…¥: {os.path.basename(pdf_path)}")
                except Exception as e:
                    st.error(f"è¼‰å…¥ {os.path.basename(pdf_path)} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        
        return documents
    
    def load_existing_index(self) -> bool:
        """è¼‰å…¥ç¾æœ‰çš„å‘é‡ç´¢å¼•"""
        try:
            if os.path.exists(INDEX_DIR) and os.listdir(INDEX_DIR):
                with st.spinner("æ­£åœ¨è¼‰å…¥ç¾æœ‰ç´¢å¼•..."):
                    storage_context = StorageContext.from_defaults(persist_dir=INDEX_DIR)
                    self.index = load_index_from_storage(storage_context)
                    self.setup_query_engine()
                    st.success("âœ… æˆåŠŸè¼‰å…¥ç¾æœ‰ç´¢å¼•")
                    return True
            else:
                return False
        except Exception as e:
            st.error(f"è¼‰å…¥ç´¢å¼•æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return False
    
    def create_index(self, documents: List[Document]) -> VectorStoreIndex:
        """å»ºç«‹æ–°çš„å‘é‡ç´¢å¼•"""
        with st.spinner("æ­£åœ¨å»ºç«‹å‘é‡ç´¢å¼•..."):
            try:
                # ç¢ºä¿æ¨¡å‹å·²æ­£ç¢ºåˆå§‹åŒ–
                if not self.models_initialized:
                    st.info("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
                    self._setup_models()
                    self.models_initialized = True
                
                # å»ºç«‹æ–°ç´¢å¼•
                index = VectorStoreIndex.from_documents(documents)
                # å„²å­˜ç´¢å¼•
                index.storage_context.persist(persist_dir=INDEX_DIR)
                st.success("âœ… æˆåŠŸå»ºç«‹æ–°ç´¢å¼•")
                
                self.index = index
                return index
            except Exception as e:
                st.error(f"å»ºç«‹ç´¢å¼•æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                return None
    
    def setup_query_engine(self):
        """è¨­å®šæŸ¥è©¢å¼•æ“"""
        if self.index:
            self.query_engine = self.index.as_query_engine(
                similarity_top_k=3,
                response_mode="compact"
            )
    
    def query(self, question: str) -> str:
        """åŸ·è¡ŒæŸ¥è©¢"""
        if not self.query_engine:
            return "ç³»çµ±å°šæœªåˆå§‹åŒ–ï¼Œè«‹å…ˆè¼‰å…¥æ–‡ä»¶ã€‚"
        
        try:
            with st.spinner("æ­£åœ¨æ€è€ƒæ‚¨çš„å•é¡Œ..."):
                response = self.query_engine.query(question)
                return str(response)
        except Exception as e:
            st.error(f"æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return "æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"
    
    def get_source_info(self) -> List[str]:
        """å–å¾—è³‡æ–™ä¾†æºè³‡è¨Š"""
        if not self.index:
            return []
        
        sources = set()
        for node in self.index.docstore.docs.values():
            if "source" in node.metadata:
                sources.add(node.metadata["source"])
        
        return list(sources)
    
    def get_knowledge_base_summary(self) -> dict:
        """å–å¾—çŸ¥è­˜åº«å®Œæ•´æ‘˜è¦"""
        if not self.index:
            return {}
        
        try:
            # åŸºç¤çµ±è¨ˆ
            stats = self.get_document_statistics()
            
            # å…§å®¹ä¸»é¡Œåˆ†æ
            topics = self.analyze_content_topics()
            
            # å»ºè­°å•é¡Œ
            suggested_questions = self.generate_suggested_questions()
            
            return {
                "statistics": stats,
                "topics": topics,
                "suggested_questions": suggested_questions,
                "last_updated": os.path.getmtime(INDEX_DIR) if os.path.exists(INDEX_DIR) else None
            }
        except Exception as e:
            st.error(f"å–å¾—çŸ¥è­˜åº«æ‘˜è¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return {}
    
    def get_document_statistics(self) -> dict:
        """å–å¾—æ–‡ä»¶çµ±è¨ˆè³‡è¨Š"""
        if not self.index:
            return {}
        
        stats = {
            "total_documents": 0,
            "total_nodes": 0,
            "document_details": [],
            "total_pages": 0
        }
        
        # çµ±è¨ˆæ–‡ä»¶è³‡è¨Š
        doc_info = {}
        for node in self.index.docstore.docs.values():
            source = node.metadata.get("source", "æœªçŸ¥")
            pages = node.metadata.get("pages", 1)
            
            if source not in doc_info:
                doc_info[source] = {
                    "name": source,
                    "pages": pages,
                    "node_count": 0
                }
            doc_info[source]["node_count"] += 1
        
        stats["total_documents"] = len(doc_info)
        stats["total_nodes"] = len(self.index.docstore.docs)
        stats["document_details"] = list(doc_info.values())
        stats["total_pages"] = sum(doc["pages"] for doc in doc_info.values())
        
        return stats
    
    def analyze_content_topics(self) -> List[dict]:
        """åˆ†æå…§å®¹ä¸»é¡Œ"""
        if not self.index:
            return []
        
        # è¿”å›é è¨­ä¸»é¡Œåˆ†é¡
        return [
            {
                "name": "èŒ¶æ¨¹å“ç¨®",
                "description": "å°ç£èŒ¶æ¨¹å“ç¨®ä»‹ç´¹èˆ‡ç‰¹æ€§åˆ†æ",
                "keywords": ["å“ç¨®", "èŒ¶æ¨¹", "ç‰¹æ€§", "é©æ‡‰æ€§"],
                "icon": "ğŸŒ±"
            },
            {
                "name": "æ ½åŸ¹æŠ€è¡“", 
                "description": "èŒ¶åœ’ç®¡ç†èˆ‡æ ½åŸ¹æŠ€è¡“è¦é»",
                "keywords": ["æ ½åŸ¹", "ç®¡ç†", "æ–½è‚¥", "ç—…èŸ²å®³"],
                "icon": "ğŸŒ¿"
            },
            {
                "name": "è£½èŒ¶å·¥è—",
                "description": "å„é¡èŒ¶è‘‰è£½ä½œå·¥è—èˆ‡æµç¨‹", 
                "keywords": ["è£½èŒ¶", "ç™¼é…µ", "çƒ˜ç„™", "å·¥è—"],
                "icon": "ğŸƒ"
            },
            {
                "name": "å“è³ªè©•é‘‘",
                "description": "èŒ¶è‘‰å“è³ªæª¢é©—èˆ‡æ„Ÿå®˜è©•é‘‘",
                "keywords": ["å“è³ª", "è©•é‘‘", "æª¢é©—", "æ¨™æº–"],
                "icon": "ğŸ¯"
            }
        ]
    
    def generate_suggested_questions(self) -> List[str]:
        """ç”Ÿæˆå»ºè­°å•é¡Œ"""
        return [
            "å°ç£çš„èŒ¶æ¨¹å“ç¨®ï¼Ÿ",
            "è£½èŒ¶çš„æµç¨‹åŒ…å«å“ªäº›æ­¥é©Ÿï¼Ÿ",
            "èŒ¶è‘‰çš„æ„Ÿå®˜å“è³ªè©•é‘‘æ€éº¼åšï¼Ÿ",
            "èŒ¶åœ’æ ½åŸ¹ç®¡ç†æœ‰å“ªäº›é‡é»ï¼Ÿ",
            "ä¸åŒèŒ¶é¡çš„è£½ä½œå·¥è—ï¼Ÿ",
            "èŒ¶è‘‰å“è³ªæª¢é©—çš„æ¨™æº–æ˜¯ä»€éº¼ï¼Ÿ",
            "å°ç£èŒ¶æ¥­çš„ç™¼å±•æ­·å²å¦‚ä½•ï¼Ÿ",
            "èŒ¶æ¨¹ç—…èŸ²å®³é˜²æ²»æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ"
        ]
