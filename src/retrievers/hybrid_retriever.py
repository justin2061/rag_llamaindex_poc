"""
Hybrid Retriever System
Ê∑∑ÂêàÊ™¢Á¥¢Á≥ªÁµ±ÔºåÁµêÂêàÂêëÈáèÊêúÁ¥¢„ÄÅÈóúÈçµÂ≠óÊêúÁ¥¢ÂíåË™ûÁæ©ÊêúÁ¥¢
"""

import json
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from elasticsearch import Elasticsearch
import numpy as np
from llama_index.core.schema import NodeWithScore, QueryBundle
from llama_index.core.retrievers import BaseRetriever
from llama_index.core import Settings

# ÈÖçÁΩÆlogging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    """ÊêúÁ¥¢ÁµêÊûú"""
    content: str
    score: float
    source: str
    metadata: Dict[str, Any]
    search_type: str  # vector, keyword, semantic, structural
    
@dataclass 
class HybridSearchConfig:
    """Ê∑∑ÂêàÊêúÁ¥¢ÈÖçÁΩÆ"""
    vector_weight: float = 0.6
    keyword_weight: float = 0.3
    semantic_weight: float = 0.1
    structural_weight: float = 0.0
    top_k: int = 10
    rerank_top_k: int = 20

class QueryRewriter:
    """Êü•Ë©¢ÊîπÂØ´Âô®"""
    
    def __init__(self, llm_model=None):
        self.llm_model = llm_model or Settings.llm
    
    def rewrite_query(self, original_query: str, conversation_history: List[str] = None) -> List[str]:
        """
        Êü•Ë©¢ÊîπÂØ´ÔºåÁîüÊàêÂ§öÂÄãËÆäÈ´îÊü•Ë©¢
        """
        rewritten_queries = [original_query]
        
        try:
            # 1. ÂêåÁæ©Ë©ûÊì¥Â±ï
            synonym_query = self._expand_synonyms(original_query)
            if synonym_query != original_query:
                rewritten_queries.append(synonym_query)
            
            # 2. ÂØ¶È´îÊèêÂèñÊü•Ë©¢
            entity_query = self._extract_key_entities(original_query)
            if entity_query:
                rewritten_queries.append(entity_query)
            
            # 3. ‰∏ä‰∏ãÊñáÁõ∏ÈóúÊü•Ë©¢ÔºàÂ¶ÇÊûúÊúâÂ∞çË©±Ê≠∑Âè≤Ôºâ
            if conversation_history:
                context_query = self._generate_contextual_query(original_query, conversation_history)
                if context_query:
                    rewritten_queries.append(context_query)
            
            # 4. Ë™ûÁæ©Áõ∏ÈóúÂïèÈ°å
            related_queries = self._generate_related_queries(original_query)
            rewritten_queries.extend(related_queries)
            
            logger.info(f"üîÑ Êü•Ë©¢ÊîπÂØ´ÂÆåÊàêÔºåÂéüÂßãÊü•Ë©¢: {original_query}")
            logger.info(f"   ÁîüÊàê {len(rewritten_queries)} ÂÄãËÆäÈ´îÊü•Ë©¢")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Êü•Ë©¢ÊîπÂØ´Â§±Êïó: {e}")
        
        return list(set(rewritten_queries))  # ÂéªÈáç
    
    def _expand_synonyms(self, query: str) -> str:
        """ÂêåÁæ©Ë©ûÊì¥Â±ï"""
        synonym_map = {
            "Ê©üÂô®Â≠∏Áøí": ["ML", "machine learning", "‰∫∫Â∑•Êô∫ÊÖßÂ≠∏Áøí"],
            "‰∫∫Â∑•Êô∫ËÉΩ": ["AI", "artificial intelligence", "‰∫∫Â∑•Êô∫ÊÖß"],
            "Ê∑±Â∫¶Â≠∏Áøí": ["DL", "deep learning", "Á•ûÁ∂ìÁ∂≤Ë∑ØÂ≠∏Áøí"],
            "ÊºîÁÆóÊ≥ï": ["algorithm", "ÁÆóÊ≥ï", "ÈÅãÁÆóÊ≥ï"],
            "Ë≥áÊñô": ["Êï∏Êìö", "data", "Êï∏ÊìöË≥áÊñô"],
            "Ê®°Âûã": ["model", "Ê®°ÂûãÁ≥ªÁµ±", "ÊºîÁÆóÊ®°Âûã"]
        }
        
        expanded_query = query
        for term, synonyms in synonym_map.items():
            if term in query:
                expanded_query += " " + " ".join(synonyms[:2])  # Âè™ÂèñÂâç2ÂÄãÂêåÁæ©Ë©û
        
        return expanded_query
    
    def _extract_key_entities(self, query: str) -> str:
        """ÊèêÂèñÈóúÈçµÂØ¶È´î"""
        # Á∞°ÂåñÁâàÂØ¶È´îÊèêÂèñÔºåÂØ¶ÈöõÊáâÁî®‰∏≠ÂèØÁî®NERÊ®°Âûã
        entities = []
        
        # Ê™¢Ê∏¨ÊäÄË°ìË°ìË™û
        tech_terms = ["Ê©üÂô®Â≠∏Áøí", "Ê∑±Â∫¶Â≠∏Áøí", "‰∫∫Â∑•Êô∫ËÉΩ", "ÊºîÁÆóÊ≥ï", "Ê®°Âûã", "Ë®ìÁ∑¥", "È†êÊ∏¨"]
        for term in tech_terms:
            if term in query:
                entities.append(term)
        
        # Ê™¢Ê∏¨Êï∏Â≠óÂíåÊôÇÈñì
        import re
        numbers = re.findall(r'\d+', query)
        entities.extend(numbers)
        
        return " ".join(entities) if entities else ""
    
    def _generate_contextual_query(self, query: str, history: List[str]) -> str:
        """Âü∫ÊñºÂ∞çË©±Ê≠∑Âè≤ÁîüÊàê‰∏ä‰∏ãÊñáÊü•Ë©¢"""
        if not history:
            return ""
        
        # Á∞°ÂåñÁâàÔºöÂ∞áÊúÄËøëÁöÑÂ∞çË©±ÂÖßÂÆπÂä†ÂÖ•Êü•Ë©¢
        recent_context = " ".join(history[-2:])  # ÂèñÊúÄËøë2Ëº™Â∞çË©±
        return f"{query} {recent_context}"
    
    def _generate_related_queries(self, query: str) -> List[str]:
        """ÁîüÊàêÁõ∏ÈóúÊü•Ë©¢"""
        related = []
        
        # Âü∫ÊñºÊü•Ë©¢ÂÖßÂÆπÁîüÊàêÁõ∏ÈóúÂïèÈ°å
        if "‰ªÄÈ∫ºÊòØ" in query or "ÊòØ‰ªÄÈ∫º" in query:
            # Â¶ÇÊûúÂïèÁöÑÊòØÂÆöÁæ©ÔºåÂèØËÉΩÈÇÑÊÉ≥Áü•ÈÅìÊáâÁî®
            topic = query.replace("‰ªÄÈ∫ºÊòØ", "").replace("ÊòØ‰ªÄÈ∫º", "").strip()
            related.append(f"{topic}ÁöÑÊáâÁî®")
            related.append(f"Â¶Ç‰Ωï‰ΩøÁî®{topic}")
        
        if "Â¶Ç‰Ωï" in query or "ÊÄéÈ∫º" in query:
            # Â¶ÇÊûúÂïèÁöÑÊòØÊñπÊ≥ïÔºåÂèØËÉΩÈÇÑÊÉ≥Áü•ÈÅìÂéüÁêÜ
            topic = query.replace("Â¶Ç‰Ωï", "").replace("ÊÄéÈ∫º", "").strip()
            related.append(f"{topic}ÁöÑÂéüÁêÜ")
            related.append(f"{topic}ÁöÑÂÑ™Áº∫Èªû")
        
        return related[:2]  # ÈôêÂà∂Êï∏Èáè

class HybridRetriever(BaseRetriever):
    """Ê∑∑ÂêàÊ™¢Á¥¢Âô®"""
    
    def __init__(
        self, 
        elasticsearch_client: Elasticsearch,
        index_name: str,
        config: HybridSearchConfig = None,
        embedding_model = None
    ):
        super().__init__()
        self.es_client = elasticsearch_client
        self.index_name = index_name
        self.config = config or HybridSearchConfig()
        self.embedding_model = embedding_model or Settings.embed_model
        self.query_rewriter = QueryRewriter()
        
        logger.info(f"üîß HybridRetriever ÂàùÂßãÂåñÂÆåÊàê")
        logger.info(f"   - Á¥¢ÂºïÂêçÁ®±: {index_name}")
        logger.info(f"   - ÂêëÈáèÊ¨äÈáç: {self.config.vector_weight}")
        logger.info(f"   - ÈóúÈçµÂ≠óÊ¨äÈáç: {self.config.keyword_weight}")
        logger.info(f"   - Ë™ûÁæ©Ê¨äÈáç: {self.config.semantic_weight}")
    
    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
        """Âü∑Ë°åÊ∑∑ÂêàÊ™¢Á¥¢"""
        query = query_bundle.query_str
        
        logger.info(f"üîç ÈñãÂßãÊ∑∑ÂêàÊ™¢Á¥¢: {query}")
        
        # 1. Êü•Ë©¢ÊîπÂØ´
        rewritten_queries = self.query_rewriter.rewrite_query(query)
        
        # 2. Â§öÁ®ÆÊ™¢Á¥¢Á≠ñÁï•
        all_results = []
        
        for search_query in rewritten_queries:
            # ÂêëÈáèÊêúÁ¥¢
            vector_results = self._vector_search(search_query)
            all_results.extend([(r, "vector") for r in vector_results])
            
            # ÈóúÈçµÂ≠óÊêúÁ¥¢ 
            keyword_results = self._keyword_search(search_query)
            all_results.extend([(r, "keyword") for r in keyword_results])
            
            # Ë™ûÁæ©ÊêúÁ¥¢
            semantic_results = self._semantic_search(search_query)
            all_results.extend([(r, "semantic") for r in semantic_results])
            
            # ÁµêÊßãÂåñÊêúÁ¥¢
            structural_results = self._structural_search(search_query)
            all_results.extend([(r, "structural") for r in structural_results])
        
        # 3. ÁµêÊûúËûçÂêàÂíåÈáçÊéíÂ∫è
        fused_results = self._fusion_ranking(all_results, query)
        
        # 4. ËΩâÊèõÁÇ∫NodeWithScoreÊ†ºÂºè
        nodes_with_scores = self._convert_to_nodes(fused_results)
        
        logger.info(f"‚úÖ Ê∑∑ÂêàÊ™¢Á¥¢ÂÆåÊàêÔºåËøîÂõû {len(nodes_with_scores)} ÂÄãÁµêÊûú")
        return nodes_with_scores[:self.config.top_k]
    
    def _vector_search(self, query: str) -> List[SearchResult]:
        """ÂêëÈáèÁõ∏‰ººÂ∫¶ÊêúÁ¥¢"""
        try:
            # ÁîüÊàêÊü•Ë©¢ÂêëÈáè
            query_embedding = self.embedding_model.get_text_embedding(query)
            
            search_body = {
                "knn": {
                    "field": "embedding",
                    "query_vector": query_embedding,
                    "k": self.config.rerank_top_k,
                    "num_candidates": self.config.rerank_top_k * 2
                }
            }
            
            response = self.es_client.search(
                index=self.index_name,
                body=search_body,
                size=self.config.rerank_top_k
            )
            
            results = []
            for hit in response['hits']['hits']:
                result = SearchResult(
                    content=hit['_source']['content'],
                    score=hit['_score'],
                    source=hit['_source'].get('metadata', {}).get('source', 'unknown'),
                    metadata=hit['_source'].get('metadata', {}),
                    search_type="vector"
                )
                results.append(result)
            
            logger.info(f"üéØ ÂêëÈáèÊêúÁ¥¢ÊâæÂà∞ {len(results)} ÂÄãÁµêÊûú")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå ÂêëÈáèÊêúÁ¥¢Â§±Êïó: {e}")
            return []
    
    def _keyword_search(self, query: str) -> List[SearchResult]:
        """BM25ÈóúÈçµÂ≠óÊêúÁ¥¢"""
        try:
            search_body = {
                "query": {
                    "bool": {
                        "should": [
                            {
                                "match": {
                                    "content": {
                                        "query": query,
                                        "boost": 2.0
                                    }
                                }
                            },
                            {
                                "match": {
                                    "bm25_content": {
                                        "query": query,
                                        "boost": 1.5
                                    }
                                }
                            },
                            {
                                "match_phrase": {
                                    "content": {
                                        "query": query,
                                        "boost": 3.0
                                    }
                                }
                            }
                        ]
                    }
                }
            }
            
            response = self.es_client.search(
                index=self.index_name,
                body=search_body,
                size=self.config.rerank_top_k
            )
            
            results = []
            for hit in response['hits']['hits']:
                result = SearchResult(
                    content=hit['_source']['content'],
                    score=hit['_score'],
                    source=hit['_source'].get('metadata', {}).get('source', 'unknown'),
                    metadata=hit['_source'].get('metadata', {}),
                    search_type="keyword"
                )
                results.append(result)
            
            logger.info(f"üîë ÈóúÈçµÂ≠óÊêúÁ¥¢ÊâæÂà∞ {len(results)} ÂÄãÁµêÊûú")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå ÈóúÈçµÂ≠óÊêúÁ¥¢Â§±Êïó: {e}")
            return []
    
    def _semantic_search(self, query: str) -> List[SearchResult]:
        """Ë™ûÁæ©ÊêúÁ¥¢ÔºàÂü∫ÊñºÂÖßÂÆπË™ûÁæ©Ôºâ"""
        try:
            search_body = {
                "query": {
                    "bool": {
                        "should": [
                            {
                                "match": {
                                    "content.ngram": {
                                        "query": query,
                                        "boost": 1.5
                                    }
                                }
                            },
                            {
                                "terms": {
                                    "metadata.semantic_info.keywords": query.split(),
                                    "boost": 2.0
                                }
                            },
                            {
                                "terms": {
                                    "metadata.semantic_info.entities": query.split(),
                                    "boost": 2.5
                                }
                            }
                        ]
                    }
                }
            }
            
            response = self.es_client.search(
                index=self.index_name,
                body=search_body,
                size=self.config.rerank_top_k
            )
            
            results = []
            for hit in response['hits']['hits']:
                result = SearchResult(
                    content=hit['_source']['content'],
                    score=hit['_score'],
                    source=hit['_source'].get('metadata', {}).get('source', 'unknown'),
                    metadata=hit['_source'].get('metadata', {}),
                    search_type="semantic"
                )
                results.append(result)
            
            logger.info(f"üß† Ë™ûÁæ©ÊêúÁ¥¢ÊâæÂà∞ {len(results)} ÂÄãÁµêÊûú")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Ë™ûÁæ©ÊêúÁ¥¢Â§±Êïó: {e}")
            return []
    
    def _structural_search(self, query: str) -> List[SearchResult]:
        """ÁµêÊßãÂåñÊêúÁ¥¢ÔºàÂü∫ÊñºÊñáÊ™îÁµêÊßãÔºâ"""
        try:
            search_body = {
                "query": {
                    "bool": {
                        "should": [
                            # Âú®Ê®ôÈ°å‰∏≠ÊêúÁ¥¢
                            {
                                "match": {
                                    "title": {
                                        "query": query,
                                        "boost": 3.0
                                    }
                                }
                            },
                            # Âú®Á´†ÁØÄ‰∏≠ÊêúÁ¥¢
                            {
                                "match": {
                                    "metadata.document_structure.chapter": {
                                        "query": query,
                                        "boost": 2.5
                                    }
                                }
                            },
                            # Âú®ÁØÄ‰∏≠ÊêúÁ¥¢
                            {
                                "match": {
                                    "metadata.document_structure.section": {
                                        "query": query,
                                        "boost": 2.0
                                    }
                                }
                            },
                            # ÊåâÂÖßÂÆπÈ°ûÂûãÈÅéÊøæ
                            {
                                "bool": {
                                    "must": [
                                        {"match": {"content": query}},
                                        {"term": {"metadata.document_structure.content_type": "title"}}
                                    ],
                                    "boost": 2.0
                                }
                            }
                        ]
                    }
                }
            }
            
            response = self.es_client.search(
                index=self.index_name,
                body=search_body,
                size=self.config.rerank_top_k
            )
            
            results = []
            for hit in response['hits']['hits']:
                result = SearchResult(
                    content=hit['_source']['content'],
                    score=hit['_score'],
                    source=hit['_source'].get('metadata', {}).get('source', 'unknown'),
                    metadata=hit['_source'].get('metadata', {}),
                    search_type="structural"
                )
                results.append(result)
            
            logger.info(f"üèóÔ∏è ÁµêÊßãÂåñÊêúÁ¥¢ÊâæÂà∞ {len(results)} ÂÄãÁµêÊûú")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå ÁµêÊßãÂåñÊêúÁ¥¢Â§±Êïó: {e}")
            return []
    
    def _fusion_ranking(self, results: List[Tuple[SearchResult, str]], query: str) -> List[SearchResult]:
        """ËûçÂêàÊéíÂ∫è"""
        
        if not results:
            return []
        
        # ÊåâÂÖßÂÆπÂéªÈáç
        seen_content = set()
        unique_results = []
        for result, search_type in results:
            content_hash = hash(result.content[:100])  # ‰ΩøÁî®Ââç100Â≠óÁ¨¶ÂÅöÂéªÈáç
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                result.search_type = search_type  # Êõ¥Êñ∞ÊêúÁ¥¢È°ûÂûã
                unique_results.append(result)
        
        # Ë®àÁÆóËûçÂêàÂàÜÊï∏
        for result in unique_results:
            fusion_score = self._calculate_fusion_score(result, query)
            result.score = fusion_score
        
        # ÊåâËûçÂêàÂàÜÊï∏ÊéíÂ∫è
        ranked_results = sorted(unique_results, key=lambda x: x.score, reverse=True)
        
        logger.info(f"üîÄ ËûçÂêàÊéíÂ∫èÂÆåÊàêÔºåÂéªÈáçÂæåÂâ©È§ò {len(ranked_results)} ÂÄãÁµêÊûú")
        return ranked_results
    
    def _calculate_fusion_score(self, result: SearchResult, query: str) -> float:
        """Ë®àÁÆóËûçÂêàÂàÜÊï∏"""
        base_score = result.score
        
        # Ê†πÊìöÊêúÁ¥¢È°ûÂûãÂä†Ê¨ä
        weight = {
            "vector": self.config.vector_weight,
            "keyword": self.config.keyword_weight,
            "semantic": self.config.semantic_weight,
            "structural": self.config.structural_weight
        }.get(result.search_type, 0.1)
        
        # È°çÂ§ñÁöÑ‰ø°Ëôü
        bonus = 0.0
        
        # ÂÖßÂÆπÈï∑Â∫¶ÁçéÂãµÔºàÈÅ©‰∏≠Èï∑Â∫¶Êõ¥Â•ΩÔºâ
        content_length = len(result.content)
        if 200 <= content_length <= 1500:
            bonus += 0.1
        
        # ÁµêÊßãÂåñÂÖßÂÆπÁçéÂãµ
        if result.metadata.get('document_structure', {}).get('content_type') == 'title':
            bonus += 0.2
        
        # Êü•Ë©¢Ë©ûÂÆåÂÖ®ÂåπÈÖçÁçéÂãµ
        if query.lower() in result.content.lower():
            bonus += 0.15
        
        final_score = base_score * weight + bonus
        return final_score
    
    def _convert_to_nodes(self, results: List[SearchResult]) -> List[NodeWithScore]:
        """ËΩâÊèõÁÇ∫NodeWithScoreÊ†ºÂºè"""
        from llama_index.core.schema import TextNode
        
        nodes_with_scores = []
        for result in results:
            node = TextNode(
                text=result.content,
                metadata={
                    **result.metadata,
                    "search_type": result.search_type,
                    "fusion_score": result.score
                }
            )
            
            node_with_score = NodeWithScore(
                node=node,
                score=result.score
            )
            nodes_with_scores.append(node_with_score)
        
        return nodes_with_scores