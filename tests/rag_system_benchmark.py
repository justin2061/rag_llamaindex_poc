import os
import time
import psutil
from typing import Dict, List, Any
import streamlit as st
from datetime import datetime

# å¯é¸çš„ç¹ªåœ–ä¾è³´
try:
    import matplotlib.pyplot as plt
    import pandas as pd
    PLOTTING_AVAILABLE = True
except ImportError:
    plt = None
    pd = None
    PLOTTING_AVAILABLE = False

# RAG ç³»çµ±å°å…¥
from enhanced_rag_system import EnhancedRAGSystem

# Graph RAG ç³»çµ± - å¯é¸å°å…¥
try:
    from graph_rag_system import GraphRAGSystem
    GRAPH_RAG_AVAILABLE = True
except ImportError:
    GraphRAGSystem = None
    GRAPH_RAG_AVAILABLE = False

try:
    from elasticsearch_rag_system import ElasticsearchRAGSystem
    ELASTICSEARCH_AVAILABLE = True
except ImportError:
    ELASTICSEARCH_AVAILABLE = False

class RAGSystemBenchmark:
    """RAG ç³»çµ±æ•ˆèƒ½åŸºæº–æ¸¬è©¦"""
    
    def __init__(self):
        self.results = {}
        self.process = psutil.Process(os.getpid())
        
    def get_memory_usage(self) -> Dict[str, float]:
        """ç²å–è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
        memory_info = self.process.memory_info()
        return {
            'rss_mb': memory_info.rss / 1024 / 1024,  # å¯¦éš›è¨˜æ†¶é«”ä½¿ç”¨
            'vms_mb': memory_info.vms / 1024 / 1024,  # è™›æ“¬è¨˜æ†¶é«”ä½¿ç”¨
            'cpu_percent': self.process.cpu_percent(),
            'memory_percent': self.process.memory_percent()
        }
    
    def benchmark_system_initialization(self, system_class, system_name: str) -> Dict[str, Any]:
        """æ¸¬è©¦ç³»çµ±åˆå§‹åŒ–æ•ˆèƒ½"""
        st.info(f"ğŸ”„ æ¸¬è©¦ {system_name} åˆå§‹åŒ–...")
        
        # è¨˜éŒ„é–‹å§‹ç‹€æ…‹
        start_memory = self.get_memory_usage()
        start_time = time.time()
        
        try:
            # åˆå§‹åŒ–ç³»çµ±
            if system_class == ElasticsearchRAGSystem and not ELASTICSEARCH_AVAILABLE:
                return {
                    'success': False,
                    'error': 'Elasticsearch dependencies not available'
                }
            
            system = system_class()
            
            # è¨˜éŒ„çµæŸç‹€æ…‹
            end_time = time.time()
            end_memory = self.get_memory_usage()
            
            result = {
                'success': True,
                'initialization_time': end_time - start_time,
                'memory_before': start_memory,
                'memory_after': end_memory,
                'memory_increase': end_memory['rss_mb'] - start_memory['rss_mb'],
                'system': system
            }
            
            st.success(f"âœ… {system_name} åˆå§‹åŒ–å®Œæˆ - {result['initialization_time']:.2f}ç§’")
            return result
            
        except Exception as e:
            st.error(f"âŒ {system_name} åˆå§‹åŒ–å¤±æ•—: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'initialization_time': time.time() - start_time
            }
    
    def benchmark_document_processing(self, system, documents: List, system_name: str) -> Dict[str, Any]:
        """æ¸¬è©¦æ–‡æª”è™•ç†æ•ˆèƒ½"""
        st.info(f"ğŸ“„ æ¸¬è©¦ {system_name} æ–‡æª”è™•ç†...")
        
        start_memory = self.get_memory_usage()
        start_time = time.time()
        
        try:
            # è™•ç†æ–‡æª”
            with st.spinner(f"æ­£åœ¨è™•ç† {len(documents)} å€‹æ–‡æª”..."):
                index = system.create_index(documents)
                system.setup_query_engine()
            
            end_time = time.time()
            end_memory = self.get_memory_usage()
            
            result = {
                'success': index is not None,
                'processing_time': end_time - start_time,
                'documents_count': len(documents),
                'processing_speed': len(documents) / (end_time - start_time),
                'memory_before': start_memory,
                'memory_after': end_memory,
                'memory_increase': end_memory['rss_mb'] - start_memory['rss_mb'],
                'peak_memory': end_memory['rss_mb']
            }
            
            if result['success']:
                st.success(f"âœ… {system_name} æ–‡æª”è™•ç†å®Œæˆ")
                st.metric("è™•ç†é€Ÿåº¦", f"{result['processing_speed']:.2f} æ–‡æª”/ç§’")
                st.metric("è¨˜æ†¶é«”å¢åŠ ", f"{result['memory_increase']:.1f} MB")
            else:
                st.error(f"âŒ {system_name} æ–‡æª”è™•ç†å¤±æ•—")
            
            return result
            
        except Exception as e:
            st.error(f"âŒ {system_name} æ–‡æª”è™•ç†å¤±æ•—: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time,
                'memory_increase': self.get_memory_usage()['rss_mb'] - start_memory['rss_mb']
            }
    
    def benchmark_query_performance(self, system, test_queries: List[str], system_name: str) -> Dict[str, Any]:
        """æ¸¬è©¦æŸ¥è©¢æ•ˆèƒ½"""
        st.info(f"ğŸ” æ¸¬è©¦ {system_name} æŸ¥è©¢æ•ˆèƒ½...")
        
        query_results = []
        total_start_time = time.time()
        
        for i, query in enumerate(test_queries):
            st.write(f"æŸ¥è©¢ {i+1}/{len(test_queries)}: {query[:50]}...")
            
            start_memory = self.get_memory_usage()
            start_time = time.time()
            
            try:
                # åŸ·è¡ŒæŸ¥è©¢
                if hasattr(system, 'query_with_graph_context'):
                    response = system.query_with_graph_context(query)
                elif hasattr(system, 'query_with_context'):
                    response = system.query_with_context(query)
                else:
                    response = system.query_engine.query(query)
                
                end_time = time.time()
                end_memory = self.get_memory_usage()
                
                query_result = {
                    'query': query,
                    'response_time': end_time - start_time,
                    'response_length': len(str(response)),
                    'memory_before': start_memory['rss_mb'],
                    'memory_after': end_memory['rss_mb'],
                    'success': True
                }
                
            except Exception as e:
                query_result = {
                    'query': query,
                    'response_time': time.time() - start_time,
                    'error': str(e),
                    'success': False
                }
            
            query_results.append(query_result)
        
        total_end_time = time.time()
        
        # è¨ˆç®—çµ±è¨ˆ
        successful_queries = [r for r in query_results if r['success']]
        
        if successful_queries:
            avg_response_time = sum(r['response_time'] for r in successful_queries) / len(successful_queries)
            max_response_time = max(r['response_time'] for r in successful_queries)
            min_response_time = min(r['response_time'] for r in successful_queries)
        else:
            avg_response_time = max_response_time = min_response_time = 0
        
        result = {
            'total_queries': len(test_queries),
            'successful_queries': len(successful_queries),
            'success_rate': len(successful_queries) / len(test_queries) * 100,
            'total_time': total_end_time - total_start_time,
            'avg_response_time': avg_response_time,
            'max_response_time': max_response_time,
            'min_response_time': min_response_time,
            'queries_per_second': len(successful_queries) / (total_end_time - total_start_time),
            'query_details': query_results
        }
        
        st.success(f"âœ… {system_name} æŸ¥è©¢æ¸¬è©¦å®Œæˆ")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("æˆåŠŸç‡", f"{result['success_rate']:.1f}%")
        with col2:
            st.metric("å¹³å‡å›æ‡‰æ™‚é–“", f"{result['avg_response_time']:.2f}ç§’")
        with col3:
            st.metric("QPS", f"{result['queries_per_second']:.2f}")
        
        return result
    
    def run_full_benchmark(self, documents: List, test_queries: List[str]) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´åŸºæº–æ¸¬è©¦"""
        st.header("ğŸš€ RAG ç³»çµ±æ•ˆèƒ½åŸºæº–æ¸¬è©¦")
        
        systems_to_test = [
            (EnhancedRAGSystem, "Enhanced RAG"),
        ]
        
        if GRAPH_RAG_AVAILABLE:
            systems_to_test.append((GraphRAGSystem, "Graph RAG"))
        else:
            st.warning("âš ï¸ Graph RAG ç³»çµ±ä¸å¯ç”¨ï¼ˆä¾è³´å·²ç¦ç”¨ï¼‰ï¼Œè·³é Graph RAG æ¸¬è©¦")
        
        if ELASTICSEARCH_AVAILABLE:
            systems_to_test.append((ElasticsearchRAGSystem, "Elasticsearch RAG"))
        
        benchmark_results = {}
        
        for system_class, system_name in systems_to_test:
            st.subheader(f"ğŸ“Š æ¸¬è©¦ {system_name}")
            
            # ç³»çµ±åˆå§‹åŒ–æ¸¬è©¦
            init_result = self.benchmark_system_initialization(system_class, system_name)
            
            if not init_result['success']:
                benchmark_results[system_name] = {
                    'initialization': init_result,
                    'document_processing': {'success': False, 'skipped': True},
                    'query_performance': {'success': False, 'skipped': True}
                }
                continue
            
            system = init_result['system']
            
            # æ–‡æª”è™•ç†æ¸¬è©¦
            doc_result = self.benchmark_document_processing(system, documents, system_name)
            
            # æŸ¥è©¢æ•ˆèƒ½æ¸¬è©¦
            if doc_result['success']:
                query_result = self.benchmark_query_performance(system, test_queries, system_name)
            else:
                query_result = {'success': False, 'skipped': True}
            
            benchmark_results[system_name] = {
                'initialization': init_result,
                'document_processing': doc_result,
                'query_performance': query_result
            }
            
            # æ¸…ç†è¨˜æ†¶é«”
            del system
            import gc
            gc.collect()
            
            st.markdown("---")
        
        # ç”Ÿæˆæ¯”è¼ƒå ±å‘Š
        self.generate_comparison_report(benchmark_results)
        
        return benchmark_results
    
    def generate_comparison_report(self, results: Dict[str, Any]):
        """ç”Ÿæˆæ¯”è¼ƒå ±å‘Š"""
        st.header("ğŸ“ˆ æ•ˆèƒ½æ¯”è¼ƒå ±å‘Š")
        
        # æº–å‚™æ•¸æ“š
        systems = []
        init_times = []
        processing_times = []
        memory_usage = []
        avg_query_times = []
        success_rates = []
        
        for system_name, result in results.items():
            if result['initialization']['success']:
                systems.append(system_name)
                init_times.append(result['initialization']['initialization_time'])
                
                if result['document_processing']['success']:
                    processing_times.append(result['document_processing']['processing_time'])
                    memory_usage.append(result['document_processing']['memory_increase'])
                else:
                    processing_times.append(0)
                    memory_usage.append(0)
                
                if result['query_performance'].get('success', False):
                    avg_query_times.append(result['query_performance']['avg_response_time'])
                    success_rates.append(result['query_performance']['success_rate'])
                else:
                    avg_query_times.append(0)
                    success_rates.append(0)
        
        if not systems:
            st.warning("æ²’æœ‰æˆåŠŸçš„æ¸¬è©¦çµæœå¯ä»¥æ¯”è¼ƒ")
            return
        
        # å‰µå»ºæ¯”è¼ƒè¡¨æ ¼
        if PLOTTING_AVAILABLE:
            comparison_df = pd.DataFrame({
                'ç³»çµ±': systems,
                'åˆå§‹åŒ–æ™‚é–“(ç§’)': [f"{t:.2f}" for t in init_times],
                'æ–‡æª”è™•ç†æ™‚é–“(ç§’)': [f"{t:.2f}" for t in processing_times],
                'è¨˜æ†¶é«”å¢åŠ (MB)': [f"{m:.1f}" for m in memory_usage],
                'å¹³å‡æŸ¥è©¢æ™‚é–“(ç§’)': [f"{t:.3f}" for t in avg_query_times],
                'æŸ¥è©¢æˆåŠŸç‡(%)': [f"{r:.1f}" for r in success_rates]
            })
            
            st.subheader("ğŸ“Š æ•ˆèƒ½å°æ¯”è¡¨")
            st.dataframe(comparison_df)
        else:
            # ç°¡å–®çš„è¡¨æ ¼é¡¯ç¤ºï¼Œä¸ä¾è³´ pandas
            st.subheader("ğŸ“Š æ•ˆèƒ½å°æ¯”è¡¨")
            data = []
            for i, system in enumerate(systems):
                data.append({
                    'ç³»çµ±': system,
                    'åˆå§‹åŒ–æ™‚é–“(ç§’)': f"{init_times[i]:.2f}",
                    'æ–‡æª”è™•ç†æ™‚é–“(ç§’)': f"{processing_times[i]:.2f}",
                    'è¨˜æ†¶é«”å¢åŠ (MB)': f"{memory_usage[i]:.1f}",
                    'å¹³å‡æŸ¥è©¢æ™‚é–“(ç§’)': f"{avg_query_times[i]:.3f}",
                    'æŸ¥è©¢æˆåŠŸç‡(%)': f"{success_rates[i]:.1f}"
                })
            st.table(data)
        
        # è¦–è¦ºåŒ–æ¯”è¼ƒ
        if PLOTTING_AVAILABLE and len(systems) > 1:
            fig, axes = plt.subplots(2, 2, figsize=(12, 10))
            
            # åˆå§‹åŒ–æ™‚é–“æ¯”è¼ƒ
            axes[0, 0].bar(systems, init_times)
            axes[0, 0].set_title('åˆå§‹åŒ–æ™‚é–“æ¯”è¼ƒ')
            axes[0, 0].set_ylabel('æ™‚é–“ (ç§’)')
            axes[0, 0].tick_params(axis='x', rotation=45)
            
            # è¨˜æ†¶é«”ä½¿ç”¨æ¯”è¼ƒ
            axes[0, 1].bar(systems, memory_usage)
            axes[0, 1].set_title('è¨˜æ†¶é«”ä½¿ç”¨æ¯”è¼ƒ')
            axes[0, 1].set_ylabel('è¨˜æ†¶é«”å¢åŠ  (MB)')
            axes[0, 1].tick_params(axis='x', rotation=45)
            
            # æ–‡æª”è™•ç†æ™‚é–“æ¯”è¼ƒ
            axes[1, 0].bar(systems, processing_times)
            axes[1, 0].set_title('æ–‡æª”è™•ç†æ™‚é–“æ¯”è¼ƒ')
            axes[1, 0].set_ylabel('æ™‚é–“ (ç§’)')
            axes[1, 0].tick_params(axis='x', rotation=45)
            
            # æŸ¥è©¢éŸ¿æ‡‰æ™‚é–“æ¯”è¼ƒ
            axes[1, 1].bar(systems, avg_query_times)
            axes[1, 1].set_title('å¹³å‡æŸ¥è©¢æ™‚é–“æ¯”è¼ƒ')
            axes[1, 1].set_ylabel('æ™‚é–“ (ç§’)')
            axes[1, 1].tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            st.pyplot(fig)
        elif not PLOTTING_AVAILABLE:
            st.info("ğŸ“Š è¦–è¦ºåŒ–åœ–è¡¨ä¸å¯ç”¨ï¼ˆmatplotlib æœªå®‰è£ï¼‰ï¼Œä½¿ç”¨æ–‡å­—ç‰ˆæ¯”è¼ƒ")
            
            # ä½¿ç”¨ Streamlit å…§å»ºåœ–è¡¨åŠŸèƒ½
            if len(systems) > 1:
                st.subheader("ğŸ“ˆ æ•ˆèƒ½æŒ‡æ¨™æ¯”è¼ƒ")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.bar_chart({system: time for system, time in zip(systems, init_times)})
                    st.caption("åˆå§‹åŒ–æ™‚é–“æ¯”è¼ƒ (ç§’)")
                
                with col2:
                    st.bar_chart({system: mem for system, mem in zip(systems, memory_usage)})
                    st.caption("è¨˜æ†¶é«”ä½¿ç”¨æ¯”è¼ƒ (MB)")
        
        # æ¨è–¦å»ºè­°
        st.subheader("ğŸ’¡ å»ºè­°")
        
        if memory_usage:
            min_memory_idx = memory_usage.index(min(memory_usage))
            max_memory_idx = memory_usage.index(max(memory_usage))
            
            st.success(f"ğŸ† è¨˜æ†¶é«”æ•ˆç‡æœ€ä½³: {systems[min_memory_idx]} ({memory_usage[min_memory_idx]:.1f} MB)")
            if max_memory_idx != min_memory_idx:
                st.warning(f"âš ï¸ è¨˜æ†¶é«”æ¶ˆè€—æœ€å¤§: {systems[max_memory_idx]} ({memory_usage[max_memory_idx]:.1f} MB)")
        
        if avg_query_times:
            fastest_idx = avg_query_times.index(min([t for t in avg_query_times if t > 0]))
            st.success(f"âš¡ æŸ¥è©¢é€Ÿåº¦æœ€å¿«: {systems[fastest_idx]} ({avg_query_times[fastest_idx]:.3f}ç§’)")
        
        # ä½¿ç”¨å ´æ™¯æ¨è–¦
        st.markdown("""
        ### ğŸ¯ ä½¿ç”¨å ´æ™¯æ¨è–¦
        
        - **Enhanced RAG**: é©åˆä¸­å°å‹æ–‡æª”é›†åˆï¼Œè¨˜æ†¶é«”ä½¿ç”¨é©ä¸­ï¼ŒéŸ¿æ‡‰é€Ÿåº¦å¿«
        - **Graph RAG**: é©åˆéœ€è¦è¤‡é›œé—œä¿‚åˆ†æçš„å ´æ™¯ï¼Œä½†è¨˜æ†¶é«”æ¶ˆè€—è¼ƒå¤§
        - **Elasticsearch RAG**: é©åˆå¤§è¦æ¨¡æ–‡æª”é›†åˆï¼Œé«˜ä¸¦ç™¼æŸ¥è©¢ï¼Œå¯æ°´å¹³æ“´å±•
        """)

def run_benchmark_app():
    """é‹è¡ŒåŸºæº–æ¸¬è©¦æ‡‰ç”¨"""
    st.set_page_config(
        page_title="RAG ç³»çµ±æ•ˆèƒ½æ¸¬è©¦",
        page_icon="ğŸš€",
        layout="wide"
    )
    
    st.title("ğŸš€ RAG ç³»çµ±æ•ˆèƒ½åŸºæº–æ¸¬è©¦")
    st.markdown("æ¯”è¼ƒä¸åŒ RAG ç³»çµ±çš„æ•ˆèƒ½è¡¨ç¾")
    
    benchmark = RAGSystemBenchmark()
    
    # æ¸¬è©¦æ–‡æª”æº–å‚™
    if st.button("é–‹å§‹åŸºæº–æ¸¬è©¦"):
        # å‰µå»ºæ¸¬è©¦æ–‡æª”
        test_documents = [
            "é€™æ˜¯ä¸€å€‹é—œæ–¼èŒ¶è‘‰çš„æ¸¬è©¦æ–‡æª”ã€‚",
            "å°ç£èŒ¶è‘‰ç¨®é¡ç¹å¤šï¼ŒåŒ…æ‹¬çƒé¾èŒ¶ã€åŒ…ç¨®èŒ¶ç­‰ã€‚",
            "è£½èŒ¶å·¥è—å°èŒ¶è‘‰å“è³ªæœ‰é‡è¦å½±éŸ¿ã€‚"
        ]
        
        # å‰µå»º Document å°è±¡
        from llama_index.core import Document
        documents = [Document(text=doc) for doc in test_documents]
        
        # æ¸¬è©¦æŸ¥è©¢
        test_queries = [
            "ä»€éº¼æ˜¯å°ç£èŒ¶è‘‰ï¼Ÿ",
            "è£½èŒ¶å·¥è—æœ‰å“ªäº›ï¼Ÿ",
            "çƒé¾èŒ¶çš„ç‰¹è‰²æ˜¯ä»€éº¼ï¼Ÿ"
        ]
        
        # åŸ·è¡Œæ¸¬è©¦
        results = benchmark.run_full_benchmark(documents, test_queries)

if __name__ == "__main__":
    run_benchmark_app()