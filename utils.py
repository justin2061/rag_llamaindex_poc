import os
import requests
from bs4 import BeautifulSoup
import re
from typing import List, Dict
import streamlit as st

# RAG System Imports - for factory function
from config import RAG_SYSTEM_TYPE
from enhanced_rag_system import EnhancedRAGSystem
from elasticsearch_rag_system import ElasticsearchRAGSystem

# å˜—è©¦å°å…¥ Graph RAG ç³»çµ±ï¼Œå¦‚æœå¤±æ•—å‰‡è¨­ç‚º None
try:
    from graph_rag_system import GraphRAGSystem
    GRAPH_RAG_AVAILABLE = True
except ImportError as e:
    GraphRAGSystem = None
    GRAPH_RAG_AVAILABLE = False
    # Graph RAG ä¾è³´ä¸å¯ç”¨ï¼Œé€™æ˜¯æ­£å¸¸çš„ï¼ˆå·²ç¦ç”¨ Graph RAG åŠŸèƒ½ï¼‰

def get_rag_system():
    """
    å·¥å» å‡½å¼ï¼šæ ¹æ“šè¨­å®šå›å‚³å°æ‡‰çš„ RAG ç³»çµ±å¯¦ä¾‹ã€‚
    """
    st.info(f"ğŸš€ æ­£åœ¨æ ¹æ“šè¨­å®š '{RAG_SYSTEM_TYPE}' åˆå§‹åŒ– RAG ç³»çµ±...")
    
    if RAG_SYSTEM_TYPE == "graph":
        if GRAPH_RAG_AVAILABLE:
            st.session_state.rag_system_type = "Graph RAG"
            return GraphRAGSystem()
        else:
            st.warning("âš ï¸ Graph RAG ç³»çµ±ä¸å¯ç”¨ï¼ˆç¼ºå°‘ pyvis ä¾è³´ï¼‰ï¼Œå›é€€åˆ° Enhanced RAG")
            st.session_state.rag_system_type = "Enhanced RAG (Graph RAG Fallback)"
            return EnhancedRAGSystem()
    elif RAG_SYSTEM_TYPE == "elasticsearch":
        st.session_state.rag_system_type = "Elasticsearch RAG"
        return ElasticsearchRAGSystem()
    elif RAG_SYSTEM_TYPE == "enhanced":
        st.session_state.rag_system_type = "Enhanced RAG"
        return EnhancedRAGSystem()
    else:
        # é è¨­æˆ–éŒ¯èª¤æƒ…æ³
        st.warning(f"âš ï¸ è¨­å®šçš„ RAG_SYSTEM_TYPE ('{RAG_SYSTEM_TYPE}') ç„¡æ•ˆï¼Œå°‡ä½¿ç”¨é è¨­çš„ 'enhanced' ç³»çµ±ã€‚")
        st.session_state.rag_system_type = "Enhanced RAG (Default)"
        return EnhancedRAGSystem()

def extract_pdf_links_from_page(url: str) -> List[str]:
    """å¾ç¶²é ä¸­æå–PDFé€£çµ"""
    try:
        # æ·»åŠ è«‹æ±‚æ¨™é ­é¿å…è¢«é˜»æ“‹
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # å°‹æ‰¾æ‰€æœ‰PDFé€£çµ - ä½¿ç”¨å¤šç¨®æ–¹å¼
        pdf_links = set()  # ä½¿ç”¨seté¿å…é‡è¤‡
        
        # æ–¹æ³•1: æª¢æŸ¥hrefå±¬æ€§ä¸­åŒ…å«.pdfçš„é€£çµ
        for link in soup.find_all('a', href=True):
            href = link['href'].lower()
            if '.pdf' in href:
                full_url = _make_absolute_url(link['href'], url)
                if full_url:
                    pdf_links.add(full_url)
        
        # æ–¹æ³•2: æª¢æŸ¥é€£çµæ–‡å­—ä¸­åŒ…å«PDFçš„é€£çµ
        for link in soup.find_all('a', href=True):
            link_text = link.get_text().lower()
            if 'pdf' in link_text or 'æª”æ¡ˆ' in link_text or 'ä¸‹è¼‰' in link_text:
                href = link['href']
                # æª¢æŸ¥æ˜¯å¦å¯èƒ½æ˜¯PDFæ–‡ä»¶é€£çµ
                if any(keyword in href.lower() for keyword in ['.pdf', 'download', 'file', 'doc']):
                    full_url = _make_absolute_url(href, url)
                    if full_url:
                        pdf_links.add(full_url)
        
        # æ–¹æ³•3: æŸ¥æ‰¾ç‰¹å®šçš„å°èŒ¶æ”¹å ´PDFé€£çµæ¨¡å¼
        for link in soup.find_all('a', href=True):
            href = link['href']
            # å°èŒ¶æ”¹å ´çš„æ–‡ä»¶é€šå¸¸åœ¨ /files/ è·¯å¾‘ä¸‹
            if '/files/' in href and any(ext in href.lower() for ext in ['.pdf', 'a01_']):
                full_url = _make_absolute_url(href, url)
                if full_url:
                    pdf_links.add(full_url)
        
        pdf_list = list(pdf_links)
        
        # é™¤éŒ¯è³‡è¨Š
        st.write(f"ğŸ” åœ¨ {url} æ‰¾åˆ° {len(pdf_list)} å€‹PDFé€£çµ")
        if pdf_list:
            st.write("ğŸ“„ ç™¼ç¾çš„PDFé€£çµ:")
            for i, link in enumerate(pdf_list[:5]):  # åªé¡¯ç¤ºå‰5å€‹
                st.write(f"  {i+1}. {link}")
            if len(pdf_list) > 5:
                st.write(f"  ... é‚„æœ‰ {len(pdf_list)-5} å€‹é€£çµ")
        
        return pdf_list
        
    except Exception as e:
        st.error(f"âŒ å¾ {url} æå–PDFé€£çµæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return []

def _make_absolute_url(href: str, base_url: str) -> str:
    """å°‡ç›¸å°è·¯å¾‘è½‰æ›ç‚ºçµ•å°è·¯å¾‘"""
    try:
        if href.startswith('http'):
            return href
        elif href.startswith('/'):
            # å–å¾—ç¶²ç«™æ ¹åŸŸå
            base_domain = "https://www.tbrs.gov.tw"
            return base_domain + href
        elif href.startswith('../'):
            # è™•ç†ç›¸å°è·¯å¾‘
            base_parts = base_url.split('/')
            href_parts = href.split('/')
            
            # ç§»é™¤ '../' ä¸¦èª¿æ•´è·¯å¾‘
            for part in href_parts:
                if part == '..':
                    if len(base_parts) > 3:  # ä¿ç•™å”è­°å’ŒåŸŸå
                        base_parts.pop()
                elif part:
                    base_parts.append(part)
            
            return '/'.join(base_parts)
        else:
            # å…¶ä»–ç›¸å°è·¯å¾‘
            base_path = base_url.rsplit('/', 1)[0]
            return f"{base_path}/{href}"
    except:
        return ""

def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬å…§å®¹"""
    # ç§»é™¤å¤šé¤˜çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text)
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)
    return text.strip()

def validate_groq_api_key(api_key: str) -> bool:
    """é©—è­‰Groq API Key"""
    if not api_key or api_key == "your_groq_api_key_here":
        return False
    
    # åŸºæœ¬æ ¼å¼æª¢æŸ¥
    if len(api_key) < 20:
        return False
    
    return True

def format_response(response: str) -> str:
    """æ ¼å¼åŒ–å›æ‡‰æ–‡æœ¬"""
    # ç¢ºä¿å›æ‡‰ä»¥é©ç•¶çš„æ¨™é»ç¬¦è™Ÿçµå°¾
    if response and not response.endswith(('.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ')):
        response += 'ã€‚'
    
    return response

def get_file_size(filepath: str) -> str:
    """å–å¾—æª”æ¡ˆå¤§å°çš„å‹å–„é¡¯ç¤º"""
    try:
        size = os.path.getsize(filepath)
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size < 1024.0:
                return f"{size:.1f} {unit}"
            size /= 1024.0
        return f"{size:.1f} TB"
    except:
        return "æœªçŸ¥"

def create_progress_callback():
    """å‰µå»ºé€²åº¦å›èª¿å‡½æ•¸"""
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    def update_progress(current: int, total: int, message: str = ""):
        progress = current / total if total > 0 else 0
        progress_bar.progress(progress)
        status_text.text(f"{message} ({current}/{total})")
    
    return update_progress
 